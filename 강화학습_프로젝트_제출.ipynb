{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UdUBhx3hFgtP",
        "yFwbKGtWMOw-",
        "4fnYFSOuFb2n",
        "xXYWm78_UYPa",
        "sWkTvajZbYbD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4DlMuPKFB_X",
        "outputId": "dfa358b7-3e1b-415d-fcce-9b7a74680836"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Gradient"
      ],
      "metadata": {
        "id": "UdUBhx3hFgtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gym\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "###  policyGradient  ###\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "seed_num = 2022\n",
        "torch.manual_seed(seed_num)\n",
        "torch.cuda.manual_seed(seed_num)\n",
        "torch.cuda.manual_seed_all(seed_num)\n",
        "np.random.seed(seed_num)\n",
        "random.seed(seed_num)\n",
        "\n",
        "class PolicyNet(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_layer_size=64):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(500, input_size)\n",
        "        # self.fc1 = torch.nn.Linear(input_size, hidden_layer_size)   # MLP\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, hidden_layer_size),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "        # self.fc2 = torch.nn.Linear(hidden_layer_size, output_size)  # MLP\n",
        "\n",
        "        self.final = torch.nn.Linear(hidden_layer_size, output_size)\n",
        "        self.softmax = torch.nn.Softmax(dim=0)                      # 소프트맥스 필요, 출력 값이 각 action을 실행할 '확률'이라서\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x)\n",
        "        # x = torch.from_numpy(x).float()\n",
        "        x= torch.LongTensor([x])\n",
        "        x = self.embedding(x)\n",
        "        x= x.squeeze(0)\n",
        "\n",
        "        return self.softmax(self.final(self.layer1(x)))\n",
        "\n",
        "    def get_action_and_logp(self, x):\n",
        "        action_prob = self.forward(x)  # action_prob : 각 action을 실행할 '확률'값들에 대한 tensor ,  ex) [0.1, 0.5, 0.2, 0.2]\n",
        "        m = torch.distributions.Categorical(action_prob)  # Categorical한 대상에 대해 확률분포를 생성함. ex) action_prob=[0.1, 0.5, 0.2, 0.2]라면, 0번째 action을 뽑을 확률 0.1, 1번째 action을 뽑을 확률 0.5\n",
        "\n",
        "        action = m.sample()                               # 출력값은 tensor(3) or tensor(1), or tensor(0) 등 index로 출력함\n",
        "        # print(m.sample())                                 # 출력값은 tensor(3) or tensor(1), or tensor(0) 등 index로 출력함\n",
        "        logp = m.log_prob(action)                         # log_prob은 확률값을 log로 변환시키는 함수 , log(tensor(0))과 동일 즉, log(0번째 action의 확률)과 동일\n",
        "        # print(logp)                                     # 출력 : tensor(-2.3026)   // np.log(0.1)  = -2.3025850929940455\n",
        "        return action.item(), logp                        # action_index , log 취한 action 선택 확률\n",
        "\n",
        "    def act(self, x):\n",
        "        action, _ = self.get_action_and_logp(x)\n",
        "        return action\n",
        "\n",
        "class ValueNet(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_layer_size=64):\n",
        "        super(ValueNet, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(500, input_size)\n",
        "        self.fc1 = torch.nn.Linear(input_size, hidden_layer_size)\n",
        "        self.fc2 = torch.nn.Linear(hidden_layer_size, 6)\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, hidden_layer_size),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "        self.final = torch.nn.Linear(hidden_layer_size, 6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= torch.LongTensor([x])\n",
        "        x = self.embedding(x)\n",
        "        x= x.squeeze(0)\n",
        "        return self.final(self.layer1(x))\n",
        "        # return self.fc2(torch.nn.functional.relu(self.fc1(x)))\n",
        "\n",
        "def policyGradient(env, max_num_steps=500, gamma=0.98, lr=0.002,\n",
        "                   num_traj=10, num_iter=200):\n",
        "    input_size = env.observation_space.n  ## STATE SPACE (STATE의 개수)\n",
        "    output_size = env.action_space.n             ## ACTION의 개수\n",
        "    Trajectory = namedtuple('Trajectory', 'states actions rewards dones logp')    # 'Trajectory'에 states actions rewards dones logp 튜플을 할당함\n",
        "\n",
        "\n",
        "    def collect_trajectory():  # Trajectory 모으는 함수 (1 episode 돌리는 함수)\n",
        "        state_list = []\n",
        "        action_list = []\n",
        "        reward_list = []\n",
        "        dones_list = []\n",
        "        logp_list = []\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        while not done and steps <= max_num_steps:   # done(에피소드 종료) 될때까지 루프 \n",
        "            action, logp = policy.get_action_and_logp(state)  # policy = PolicyNet(input_size, output_size)\n",
        "            newstate, reward, done, _, _ = env.step(action)    # s_, r, terminated, truncated, _\n",
        "            state_list.append(state)\n",
        "            action_list.append(action)\n",
        "            reward_list.append(reward)\n",
        "            dones_list.append(done)\n",
        "            logp_list.append(logp)\n",
        "            steps += 1\n",
        "            state = newstate\n",
        "\n",
        "        traj = Trajectory(states=state_list, actions=action_list,\n",
        "                          rewards=reward_list, logp=logp_list, dones=dones_list)\n",
        "      # print(traj)  # Trajectory(states=[array([ 9.5363184e-06,  9.3068667e-03,  3.0773466e-02, -4.3019545e-03],\n",
        "      # dtype=float32), array([ 1.9567365e-04, -1.8624260e-01,  3.0687427e-02,  2.9792932e-01],\n",
        "      # dtype=float32), array([-0.00352918,  0.00842878,  0.03664601,  0.01508044], dtype=float32), array([-0.0033606 ,  0.20300654,  0.03694762, -0.26581872], dtype=float32), array([ 0.00069953,  0.3975822 ,  0.03163125, -0.54662293], dtype=float32), array([ 0.00865117,  0.5922458 ,  0.02069879, -0.82917416], dtype=float32), array([ 0.02049609,  0.3968471 ,  0.00411531, -0.5300539 ], dtype=float32), array([ 0.02843303,  0.5919109 , -0.00648577, -0.82143724], dtype=float32), array([ 0.04027125,  0.3968783 , -0.02291452, -0.5308013 ], dtype=float32), array([ 0.04820881,  0.59231496, -0.03353054, -0.8306156 ], dtype=float32), array([ 0.06005511,  0.39766696, -0.05014285, -0.54866385], dtype=float32), array([ 0.06800845,  0.20328394, -0.06111613, -0.27219164], dtype=float32), array([ 0.07207413,  0.0090849 , -0.06655996,  0.00060612], dtype=float32), array([ 0.07225583, -0.18502247, -0.06654784,  0.27156827], dtype=float32), array([ 0.06855538, -0.37913483, -0.06111648,  0.5425417 ], dtype=float32), array([ 0.06097268, -0.18320957, -0.05026564,  0.23124543], dtype=float32), array([ 0.05730849,  0.01259326, -0.04564074, -0.07685973], dtype=float32), array([ 0.05756036, -0.1818457 , -0.04717793,  0.20108126], dtype=float32), array([ 0.05392344, -0.37626228, -0.0431563 ,  0.4785165 ], dtype=float32), array([ 0.0463982 , -0.5707492 , -0.03358598,  0.75729126], dtype=float32), array([ 0.03498321, -0.37518087, -0.01844015,  0.45423177], dtype=float32), array([ 0.02747959, -0.5700373 , -0.00935551,  0.7410456 ], dtype=float32), array([ 0.01607885, -0.37478745,  0.0054654 ,  0.44543317], dtype=float32), array([ 0.0085831 , -0.5699863 ,  0.01437406,  0.7398339 ], dtype=float32), array([-0.00281663, -0.76530373,  0.02917074,  1.0370055 ], dtype=float32), array([-0.0181227 , -0.57058144,  0.04991085,  0.75362134], dtype=float32), array([-0.02953433, -0.7663547 ,  0.06498328,  1.0615833 ], dtype=float32), array([-0.04486142, -0.5721506 ,  0.08621494,  0.78998363], dtype=float32), array([-0.05630443, -0.768344  ,  0.10201462,  1.1084964 ], dtype=float32), array([-0.07167131, -0.5746998 ,  0.12418454,  0.8494806 ], dtype=float32), array([-0.08316531, -0.38147032,  0.14117415,  0.59828496], dtype=float32), array([-0.09079471, -0.18857652,  0.15313986,  0.35319027], dtype=float32), array([-0.09456625,  0.00407392,  0.16020367,  0.11244383], dtype=float32), array([-0.09448477,  0.19658096,  0.16245253, -0.12572043], dtype=float32), array([-0.09055315,  0.38904798,  0.15993813, -0.36306855], dtype=float32), array([-0.08277219,  0.58157825,  0.15267676, -0.60135657], dtype=float32), array([-0.07114062,  0.774272  ,  0.14064963, -0.8423221 ], dtype=float32), array([-0.05565519,  0.5775394 ,  0.12380318, -0.50892246], dtype=float32), array([-0.0441044 ,  0.3809106 ,  0.11362474, -0.17993148], dtype=float32), array([-0.03648619,  0.18436155,  0.11002611,  0.14632481], dtype=float32), array([-0.03279895, -0.01214998,  0.1129526 ,  0.47159216], dtype=float32), array([-0.03304195, -0.20867096,  0.12238444,  0.79763263], dtype=float32), array([-0.03721537, -0.01542167,  0.1383371 ,  0.54581815], dtype=float32), array([-0.03752381,  0.17751318,  0.14925346,  0.29972214], dtype=float32), array([-0.03397354, -0.01938604,  0.15524791,  0.6355052 ], dtype=float32), array([-0.03436126,  0.17326893,  0.167958  ,  0.395458  ], dtype=float32), array([-0.03089589,  0.36565927,  0.17586717,  0.16008124], dtype=float32), array([-0.0235827 ,  0.5578845 ,  0.17906879, -0.07237025], dtype=float32), array([-0.01242501,  0.75004774,  0.1776214 , -0.30364075], dtype=float32), array([0.00257595, 0.55289793, 0.17154858, 0.0393778 ], dtype=float32), array([ 0.0136339 ,  0.74519783,  0.17233613, -0.19464979], dtype=float32), array([0.02853786, 0.5480834 , 0.16844313, 0.14705835], dtype=float32), array([ 0.03949953,  0.74044305,  0.1713843 , -0.08810893], dtype=float32), array([0.05430839, 0.54333186, 0.16962212, 0.25336695], dtype=float32), array([0.06517503, 0.73567706, 0.17468946, 0.01861984], dtype=float32), array([ 0.07988857,  0.9279195 ,  0.17506185, -0.21425721], dtype=float32), array([0.09844696, 0.7307833 , 0.17077671, 0.12813324], dtype=float32), array([0.11306263, 0.53367877, 0.17333938, 0.46945378], dtype=float32), array([0.1237362 , 0.33658645, 0.18272845, 0.8113689 ], dtype=float32), array([0.13046794, 0.13949473, 0.19895583, 1.1555083 ], dtype=float32)], actions=[0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1], rewards=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], dones=[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True], logp=[tensor(-0.7011, grad_fn=<SqueezeBackward1>), tensor(-0.4942, grad_fn=<SqueezeBackward1>), tensor(-0.6693, grad_fn=<SqueezeBackward1>), tensor(-0.8719, grad_fn=<SqueezeBackward1>), tensor(-1.0927, grad_fn=<SqueezeBackward1>), tensor(-0.3000, grad_fn=<SqueezeBackward1>), tensor(-1.0943, grad_fn=<SqueezeBackward1>), tensor(-0.2976, grad_fn=<SqueezeBackward1>), tensor(-1.1101, grad_fn=<SqueezeBackward1>), tensor(-0.2912, grad_fn=<SqueezeBackward1>), tensor(-0.3850, grad_fn=<SqueezeBackward1>), tensor(-0.5034, grad_fn=<SqueezeBackward1>), tensor(-0.6458, grad_fn=<SqueezeBackward1>), tensor(-0.8513, grad_fn=<SqueezeBackward1>), tensor(-0.3972, grad_fn=<SqueezeBackward1>), tensor(-0.5714, grad_fn=<SqueezeBackward1>), tensor(-0.6144, grad_fn=<SqueezeBackward1>), tensor(-0.8118, grad_fn=<SqueezeBackward1>), tensor(-1.0638, grad_fn=<SqueezeBackward1>), tensor(-0.2649, grad_fn=<SqueezeBackward1>), tensor(-1.0635, grad_fn=<SqueezeBackward1>), tensor(-0.2634, grad_fn=<SqueezeBackward1>), tensor(-1.0795, grad_fn=<SqueezeBackward1>), tensor(-1.4881, grad_fn=<SqueezeBackward1>), tensor(-0.1490, grad_fn=<SqueezeBackward1>), tensor(-1.5465, grad_fn=<SqueezeBackward1>), tensor(-0.1374, grad_fn=<SqueezeBackward1>), tensor(-1.6341, grad_fn=<SqueezeBackward1>), tensor(-0.1220, grad_fn=<SqueezeBackward1>), tensor(-0.1897, grad_fn=<SqueezeBackward1>), tensor(-0.2872, grad_fn=<SqueezeBackward1>), tensor(-0.4043, grad_fn=<SqueezeBackward1>), tensor(-0.5435, grad_fn=<SqueezeBackward1>), tensor(-0.7004, grad_fn=<SqueezeBackward1>), tensor(-0.9015, grad_fn=<SqueezeBackward1>), tensor(-1.0939, grad_fn=<SqueezeBackward1>), tensor(-0.3111, grad_fn=<SqueezeBackward1>), tensor(-0.4323, grad_fn=<SqueezeBackward1>), tensor(-0.5986, grad_fn=<SqueezeBackward1>), tensor(-0.8267, grad_fn=<SqueezeBackward1>), tensor(-1.1433, grad_fn=<SqueezeBackward1>), tensor(-0.2386, grad_fn=<SqueezeBackward1>), tensor(-0.3441, grad_fn=<SqueezeBackward1>), tensor(-0.9646, grad_fn=<SqueezeBackward1>), tensor(-0.3061, grad_fn=<SqueezeBackward1>), tensor(-0.4233, grad_fn=<SqueezeBackward1>), tensor(-0.5728, grad_fn=<SqueezeBackward1>), tensor(-0.7365, grad_fn=<SqueezeBackward1>), tensor(-0.5085, grad_fn=<SqueezeBackward1>), tensor(-0.6748, grad_fn=<SqueezeBackward1>), tensor(-0.5544, grad_fn=<SqueezeBackward1>), tensor(-0.6252, grad_fn=<SqueezeBackward1>), tensor(-0.6050, grad_fn=<SqueezeBackward1>), tensor(-0.5652, grad_fn=<SqueezeBackward1>), tensor(-0.7268, grad_fn=<SqueezeBackward1>), tensor(-0.5218, grad_fn=<SqueezeBackward1>), tensor(-0.7160, grad_fn=<SqueezeBackward1>), tensor(-1.0044, grad_fn=<SqueezeBackward1>), tensor(-1.3989, grad_fn=<SqueezeBackward1>), tensor(-0.1645, grad_fn=<SqueezeBackward1>)])\n",
        "\n",
        "        return traj\n",
        "\n",
        "    def calc_returns(rewards):\n",
        "        dis_rewards = [gamma**i * r for i, r in enumerate(rewards)]  ## [ gamma**0 * r, gamma**1 * r, gamma**2 * r, gamma**3 * r, ...]\n",
        "        return [sum(dis_rewards[i:]) for i in range(len(dis_rewards))]  ## [R(0~end), R(1~end), R(2~end), ...]\n",
        "\n",
        "    policy = PolicyNet(input_size, output_size)\n",
        "    policy_optimizer = torch.optim.Adam(policy.parameters(), lr=lr)\n",
        "\n",
        "    value = ValueNet(input_size)\n",
        "    value_optimizer = torch.optim.Adam(value.parameters(), lr=lr)\n",
        "\n",
        "    mean_return_list = []\n",
        "    for it in range(num_iter):\n",
        "        traj_list = [collect_trajectory() for _ in range(num_traj)]    #\n",
        "        returns = [calc_returns(traj.rewards) for traj in traj_list]   # traj.rewards는 traj(Trajectory에서 tuple 요소중 rewards를 뽑아냄)\n",
        "                                                                       # returns는 2차원 : [[Return(0~end), Return(1~end), Return(2~end), ...],  [Return(0~end), Return(1~end), Return(2~end), ...], ... , [Return(0~end), Return(1~end), Return(2~end), ...] ]\n",
        "\n",
        "        # ====================================#\n",
        "        # policy gradient                    #\n",
        "        # ====================================#\n",
        "        # policy_loss_terms = [-1. * traj.logp[j] * (torch.Tensor([returns[i][0]]))                   # returns[i][0] :   Return(0~end)로만 참고함 ([Return(0~end), Return(0~end), Return(0~end), ..., ...])   Expectation of Return이 목적함수(로스 펑션으로 만들기 위해 -1 추가)\n",
        "        #                     for i, traj in enumerate(traj_list) for j in range(len(traj.actions))]  # policy gradient, policyNet에서의 로스 펑션, 값이 작을수록 Loss가 적은 형태로 식을 만들기 위해 앞에 -1 붙임\n",
        "\n",
        "        # ====================================#\n",
        "        # policy gradient with reward-to-go  #\n",
        "        # ====================================#\n",
        "        # policy_loss_terms = [-1. * traj.logp[j] * (torch.Tensor([returns[i][j]]))                   # returns[i][j]] :   Return(j~end)로 참고함 => [Return(0~end), Return(1~end), Return(2~end), ~, Return(end-1~end) , ..., ... , ]    Expectation of Return이 목적함수\n",
        "        #                     for i, traj in enumerate(traj_list) for j in range(len(traj.actions))]  # policy gradient, policyNet에서의 로스 펑션, 값이 작을수록 Loss가 적은 형태로 식을 만들기 위해 앞에 -1 붙임\n",
        "\n",
        "        #====================================#\n",
        "        # policy gradient with base function #\n",
        "        #====================================#\n",
        "        policy_loss_terms = [-1. * traj.logp[j] * (returns[i][j] - value(traj.states[j]))             # returns[i][j] - value(traj.states[j]  :   리턴값에 대해 표준화 적용 (평균값을 빼줌), 평균값은 곧 기대값이고 V(s)를 통해 구함. V(s)는 ValueNet에서 구함.\n",
        "                             for i, traj in enumerate(traj_list) for j in range(len(traj.actions))]   # policy gradient, policyNet에서의 로스 펑션, 값이 작을수록 Loss가 적은 형태로 식을 만들기 위해 앞에 -1 붙임\n",
        "\n",
        "\n",
        "        policy_loss = 1. / num_traj * torch.cat(policy_loss_terms).sum()                              # 각  returns의 원소들을 합산하고 나눔  =>  Expectation of Return 계산\n",
        "        policy_optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        policy_optimizer.step()\n",
        "\n",
        "        value_loss_terms = [1. / len(traj.actions) * (value(traj.states[j]) - returns[i][j])**2.      # ValueNet에서의 로스 펑션, MSE 적용\n",
        "                            for i, traj in enumerate(traj_list) for j in range(len(traj.actions))]\n",
        "        value_loss = 1. / num_traj * torch.cat(value_loss_terms).sum()\n",
        "        value_optimizer.zero_grad()\n",
        "        value_loss.backward()\n",
        "        value_optimizer.step()\n",
        "\n",
        "        mean_return = 1. / num_traj * \\\n",
        "            sum([traj_returns[0] for traj_returns in returns])\n",
        "        mean_return_list.append(mean_return)\n",
        "        pd.DataFrame(mean_return_list).to_csv('/content/drive/MyDrive/강화학습/policy_gradient_model_record2.csv')\n",
        "\n",
        "        if it % 10 == 0:\n",
        "            print('Iteration {}: Mean Return = {}'.format(it, mean_return))\n",
        "            torch.save(policy.state_dict(), '/content/drive/MyDrive/강화학습/policy_gradient_model2_temp.pth')\n",
        "\n",
        "    torch.save(policy.state_dict(), '/content/drive/MyDrive/강화학습/policy_gradient_model2_last.pth')\n",
        "\n",
        "    return policy, mean_return_list\n",
        "\n",
        "env = gym.make('Taxi-v3').unwrapped\n",
        "env._max_episode_steps=500\n",
        "agent, mean_return_list = policyGradient(env, num_iter=5000, max_num_steps=500, gamma=1.0, num_traj=5)\n",
        "\n",
        "\n",
        "plt.plot(mean_return_list)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Mean Return')\n",
        "plt.savefig('pg1_returns2.png', format='png', dpi=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZwBsyZ0GnmxF",
        "outputId": "068b03cc-c70c-4cb4-af6d-a8028521403d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Mean Return = -1966.2\n",
            "Iteration 10: Mean Return = -1914.0\n",
            "Iteration 20: Mean Return = -1946.4\n",
            "Iteration 30: Mean Return = -1865.4\n",
            "Iteration 40: Mean Return = -1818.6000000000001\n",
            "Iteration 50: Mean Return = -1206.4\n",
            "Iteration 60: Mean Return = -1590.0\n",
            "Iteration 70: Mean Return = -1362.0\n",
            "Iteration 80: Mean Return = -1950.0\n",
            "Iteration 90: Mean Return = -2257.8\n",
            "Iteration 100: Mean Return = -1217.4\n",
            "Iteration 110: Mean Return = -1611.8000000000002\n",
            "Iteration 120: Mean Return = -1597.2\n",
            "Iteration 130: Mean Return = -1296.0\n",
            "Iteration 140: Mean Return = -1274.4\n",
            "Iteration 150: Mean Return = -1687.2\n",
            "Iteration 160: Mean Return = -668.0\n",
            "Iteration 170: Mean Return = -876.4000000000001\n",
            "Iteration 180: Mean Return = -583.0\n",
            "Iteration 190: Mean Return = -932.2\n",
            "Iteration 200: Mean Return = -875.8000000000001\n",
            "Iteration 210: Mean Return = -618.4000000000001\n",
            "Iteration 220: Mean Return = -454.40000000000003\n",
            "Iteration 230: Mean Return = -574.8000000000001\n",
            "Iteration 240: Mean Return = -481.40000000000003\n",
            "Iteration 250: Mean Return = -443.20000000000005\n",
            "Iteration 260: Mean Return = -502.40000000000003\n",
            "Iteration 270: Mean Return = -339.40000000000003\n",
            "Iteration 280: Mean Return = -406.8\n",
            "Iteration 290: Mean Return = -180.8\n",
            "Iteration 300: Mean Return = -338.20000000000005\n",
            "Iteration 310: Mean Return = -474.0\n",
            "Iteration 320: Mean Return = -357.20000000000005\n",
            "Iteration 330: Mean Return = -200.8\n",
            "Iteration 340: Mean Return = -321.20000000000005\n",
            "Iteration 350: Mean Return = -156.4\n",
            "Iteration 360: Mean Return = -245.8\n",
            "Iteration 370: Mean Return = -361.6\n",
            "Iteration 380: Mean Return = -232.20000000000002\n",
            "Iteration 390: Mean Return = -190.4\n",
            "Iteration 400: Mean Return = -316.20000000000005\n",
            "Iteration 410: Mean Return = -96.80000000000001\n",
            "Iteration 420: Mean Return = -286.0\n",
            "Iteration 430: Mean Return = -264.0\n",
            "Iteration 440: Mean Return = -188.4\n",
            "Iteration 450: Mean Return = -77.4\n",
            "Iteration 460: Mean Return = -64.60000000000001\n",
            "Iteration 470: Mean Return = -103.60000000000001\n",
            "Iteration 480: Mean Return = -164.0\n",
            "Iteration 490: Mean Return = -67.8\n",
            "Iteration 500: Mean Return = -257.0\n",
            "Iteration 510: Mean Return = -131.8\n",
            "Iteration 520: Mean Return = -134.20000000000002\n",
            "Iteration 530: Mean Return = -170.8\n",
            "Iteration 540: Mean Return = -250.0\n",
            "Iteration 550: Mean Return = -330.0\n",
            "Iteration 560: Mean Return = -191.4\n",
            "Iteration 570: Mean Return = -154.8\n",
            "Iteration 580: Mean Return = -192.8\n",
            "Iteration 590: Mean Return = -33.0\n",
            "Iteration 600: Mean Return = -57.6\n",
            "Iteration 610: Mean Return = -105.4\n",
            "Iteration 620: Mean Return = -33.0\n",
            "Iteration 630: Mean Return = -98.0\n",
            "Iteration 640: Mean Return = -71.8\n",
            "Iteration 650: Mean Return = -40.6\n",
            "Iteration 660: Mean Return = -38.2\n",
            "Iteration 670: Mean Return = -123.4\n",
            "Iteration 680: Mean Return = -59.6\n",
            "Iteration 690: Mean Return = -113.0\n",
            "Iteration 700: Mean Return = -156.20000000000002\n",
            "Iteration 710: Mean Return = -34.6\n",
            "Iteration 720: Mean Return = -31.0\n",
            "Iteration 730: Mean Return = -148.0\n",
            "Iteration 740: Mean Return = -73.4\n",
            "Iteration 750: Mean Return = -47.400000000000006\n",
            "Iteration 760: Mean Return = -23.0\n",
            "Iteration 770: Mean Return = -38.6\n",
            "Iteration 780: Mean Return = -36.4\n",
            "Iteration 790: Mean Return = -133.6\n",
            "Iteration 800: Mean Return = -57.800000000000004\n",
            "Iteration 810: Mean Return = -67.8\n",
            "Iteration 820: Mean Return = -29.400000000000002\n",
            "Iteration 830: Mean Return = -59.0\n",
            "Iteration 840: Mean Return = -17.0\n",
            "Iteration 850: Mean Return = -19.0\n",
            "Iteration 860: Mean Return = -9.8\n",
            "Iteration 870: Mean Return = -33.2\n",
            "Iteration 880: Mean Return = -20.0\n",
            "Iteration 890: Mean Return = -18.6\n",
            "Iteration 900: Mean Return = -106.4\n",
            "Iteration 910: Mean Return = -124.2\n",
            "Iteration 920: Mean Return = -129.20000000000002\n",
            "Iteration 930: Mean Return = -43.2\n",
            "Iteration 940: Mean Return = -23.6\n",
            "Iteration 950: Mean Return = -108.0\n",
            "Iteration 960: Mean Return = -43.2\n",
            "Iteration 970: Mean Return = -36.0\n",
            "Iteration 980: Mean Return = -37.6\n",
            "Iteration 990: Mean Return = -40.400000000000006\n",
            "Iteration 1000: Mean Return = -218.4\n",
            "Iteration 1010: Mean Return = -15.600000000000001\n",
            "Iteration 1020: Mean Return = -122.2\n",
            "Iteration 1030: Mean Return = -35.800000000000004\n",
            "Iteration 1040: Mean Return = -14.200000000000001\n",
            "Iteration 1050: Mean Return = -109.2\n",
            "Iteration 1060: Mean Return = -14.200000000000001\n",
            "Iteration 1070: Mean Return = -9.8\n",
            "Iteration 1080: Mean Return = -204.20000000000002\n",
            "Iteration 1090: Mean Return = -10.600000000000001\n",
            "Iteration 1100: Mean Return = -114.0\n",
            "Iteration 1110: Mean Return = -21.6\n",
            "Iteration 1120: Mean Return = -2.6\n",
            "Iteration 1130: Mean Return = -19.200000000000003\n",
            "Iteration 1140: Mean Return = -4.4\n",
            "Iteration 1150: Mean Return = -5.4\n",
            "Iteration 1160: Mean Return = -19.200000000000003\n",
            "Iteration 1170: Mean Return = -15.200000000000001\n",
            "Iteration 1180: Mean Return = -35.2\n",
            "Iteration 1190: Mean Return = -14.4\n",
            "Iteration 1200: Mean Return = -17.400000000000002\n",
            "Iteration 1210: Mean Return = -235.4\n",
            "Iteration 1220: Mean Return = -19.8\n",
            "Iteration 1230: Mean Return = -14.0\n",
            "Iteration 1240: Mean Return = 3.0\n",
            "Iteration 1250: Mean Return = -16.0\n",
            "Iteration 1260: Mean Return = -11.600000000000001\n",
            "Iteration 1270: Mean Return = -110.80000000000001\n",
            "Iteration 1280: Mean Return = -112.4\n",
            "Iteration 1290: Mean Return = -3.0\n",
            "Iteration 1300: Mean Return = -10.8\n",
            "Iteration 1310: Mean Return = -12.200000000000001\n",
            "Iteration 1320: Mean Return = -4.6000000000000005\n",
            "Iteration 1330: Mean Return = -102.4\n",
            "Iteration 1340: Mean Return = -1.2000000000000002\n",
            "Iteration 1350: Mean Return = -121.4\n",
            "Iteration 1360: Mean Return = -7.0\n",
            "Iteration 1370: Mean Return = -21.200000000000003\n",
            "Iteration 1380: Mean Return = -0.2\n",
            "Iteration 1390: Mean Return = -13.600000000000001\n",
            "Iteration 1400: Mean Return = -2.4000000000000004\n",
            "Iteration 1410: Mean Return = -2.2\n",
            "Iteration 1420: Mean Return = -6.0\n",
            "Iteration 1430: Mean Return = -4.800000000000001\n",
            "Iteration 1440: Mean Return = -2.6\n",
            "Iteration 1450: Mean Return = -113.60000000000001\n",
            "Iteration 1460: Mean Return = -99.2\n",
            "Iteration 1470: Mean Return = -109.80000000000001\n",
            "Iteration 1480: Mean Return = -25.6\n",
            "Iteration 1490: Mean Return = -8.6\n",
            "Iteration 1500: Mean Return = -5.6000000000000005\n",
            "Iteration 1510: Mean Return = -4.0\n",
            "Iteration 1520: Mean Return = -4.6000000000000005\n",
            "Iteration 1530: Mean Return = -13.4\n",
            "Iteration 1540: Mean Return = -121.60000000000001\n",
            "Iteration 1550: Mean Return = -6.4\n",
            "Iteration 1560: Mean Return = -14.600000000000001\n",
            "Iteration 1570: Mean Return = -4.4\n",
            "Iteration 1580: Mean Return = -18.2\n",
            "Iteration 1590: Mean Return = -110.0\n",
            "Iteration 1600: Mean Return = -8.4\n",
            "Iteration 1610: Mean Return = -102.4\n",
            "Iteration 1620: Mean Return = -1.2000000000000002\n",
            "Iteration 1630: Mean Return = -5.2\n",
            "Iteration 1640: Mean Return = -112.0\n",
            "Iteration 1650: Mean Return = -11.600000000000001\n",
            "Iteration 1660: Mean Return = -3.8000000000000003\n",
            "Iteration 1670: Mean Return = -15.4\n",
            "Iteration 1680: Mean Return = -14.600000000000001\n",
            "Iteration 1690: Mean Return = 3.8000000000000003\n",
            "Iteration 1700: Mean Return = -0.6000000000000001\n",
            "Iteration 1710: Mean Return = -0.8\n",
            "Iteration 1720: Mean Return = 4.0\n",
            "Iteration 1730: Mean Return = -95.80000000000001\n",
            "Iteration 1740: Mean Return = -6.0\n",
            "Iteration 1750: Mean Return = 1.2000000000000002\n",
            "Iteration 1760: Mean Return = -202.0\n",
            "Iteration 1770: Mean Return = -2.6\n",
            "Iteration 1780: Mean Return = -7.4\n",
            "Iteration 1790: Mean Return = 5.800000000000001\n",
            "Iteration 1800: Mean Return = -1.8\n",
            "Iteration 1810: Mean Return = 6.2\n",
            "Iteration 1820: Mean Return = -5.6000000000000005\n",
            "Iteration 1830: Mean Return = -96.80000000000001\n",
            "Iteration 1840: Mean Return = -1.8\n",
            "Iteration 1850: Mean Return = -197.4\n",
            "Iteration 1860: Mean Return = -1.4000000000000001\n",
            "Iteration 1870: Mean Return = -102.2\n",
            "Iteration 1880: Mean Return = -98.0\n",
            "Iteration 1890: Mean Return = -97.60000000000001\n",
            "Iteration 1900: Mean Return = -1.6\n",
            "Iteration 1910: Mean Return = 0.4\n",
            "Iteration 1920: Mean Return = 2.4000000000000004\n",
            "Iteration 1930: Mean Return = -0.8\n",
            "Iteration 1940: Mean Return = -7.4\n",
            "Iteration 1950: Mean Return = 2.8000000000000003\n",
            "Iteration 1960: Mean Return = -99.0\n",
            "Iteration 1970: Mean Return = -2.4000000000000004\n",
            "Iteration 1980: Mean Return = -45.2\n",
            "Iteration 1990: Mean Return = 6.4\n",
            "Iteration 2000: Mean Return = -102.60000000000001\n",
            "Iteration 2010: Mean Return = 1.0\n",
            "Iteration 2020: Mean Return = 3.4000000000000004\n",
            "Iteration 2030: Mean Return = -4.0\n",
            "Iteration 2040: Mean Return = -3.0\n",
            "Iteration 2050: Mean Return = -0.4\n",
            "Iteration 2060: Mean Return = -97.80000000000001\n",
            "Iteration 2070: Mean Return = 7.6000000000000005\n",
            "Iteration 2080: Mean Return = 1.0\n",
            "Iteration 2090: Mean Return = 3.6\n",
            "Iteration 2100: Mean Return = 4.6000000000000005\n",
            "Iteration 2110: Mean Return = -101.60000000000001\n",
            "Iteration 2120: Mean Return = -95.0\n",
            "Iteration 2130: Mean Return = -8.8\n",
            "Iteration 2140: Mean Return = 3.4000000000000004\n",
            "Iteration 2150: Mean Return = -101.60000000000001\n",
            "Iteration 2160: Mean Return = -2.4000000000000004\n",
            "Iteration 2170: Mean Return = -99.4\n",
            "Iteration 2180: Mean Return = 6.6000000000000005\n",
            "Iteration 2190: Mean Return = 4.2\n",
            "Iteration 2200: Mean Return = 4.4\n",
            "Iteration 2210: Mean Return = 5.4\n",
            "Iteration 2220: Mean Return = 3.2\n",
            "Iteration 2230: Mean Return = 7.6000000000000005\n",
            "Iteration 2240: Mean Return = -5.800000000000001\n",
            "Iteration 2250: Mean Return = 6.2\n",
            "Iteration 2260: Mean Return = -0.4\n",
            "Iteration 2270: Mean Return = 1.6\n",
            "Iteration 2280: Mean Return = 0.0\n",
            "Iteration 2290: Mean Return = -96.80000000000001\n",
            "Iteration 2300: Mean Return = 2.6\n",
            "Iteration 2310: Mean Return = -1.2000000000000002\n",
            "Iteration 2320: Mean Return = 2.4000000000000004\n",
            "Iteration 2330: Mean Return = 0.6000000000000001\n",
            "Iteration 2340: Mean Return = 2.0\n",
            "Iteration 2350: Mean Return = 3.2\n",
            "Iteration 2360: Mean Return = 7.800000000000001\n",
            "Iteration 2370: Mean Return = 5.4\n",
            "Iteration 2380: Mean Return = 6.800000000000001\n",
            "Iteration 2390: Mean Return = -3.2\n",
            "Iteration 2400: Mean Return = 4.800000000000001\n",
            "Iteration 2410: Mean Return = 6.2\n",
            "Iteration 2420: Mean Return = 4.6000000000000005\n",
            "Iteration 2430: Mean Return = 2.2\n",
            "Iteration 2440: Mean Return = 3.8000000000000003\n",
            "Iteration 2450: Mean Return = 3.2\n",
            "Iteration 2460: Mean Return = 3.6\n",
            "Iteration 2470: Mean Return = -1.0\n",
            "Iteration 2480: Mean Return = 8.200000000000001\n",
            "Iteration 2490: Mean Return = 1.2000000000000002\n",
            "Iteration 2500: Mean Return = 2.4000000000000004\n",
            "Iteration 2510: Mean Return = 0.2\n",
            "Iteration 2520: Mean Return = 3.4000000000000004\n",
            "Iteration 2530: Mean Return = -8.0\n",
            "Iteration 2540: Mean Return = 1.4000000000000001\n",
            "Iteration 2550: Mean Return = 4.2\n",
            "Iteration 2560: Mean Return = 1.0\n",
            "Iteration 2570: Mean Return = 0.0\n",
            "Iteration 2580: Mean Return = -7.0\n",
            "Iteration 2590: Mean Return = -2.8000000000000003\n",
            "Iteration 2600: Mean Return = 3.8000000000000003\n",
            "Iteration 2610: Mean Return = 4.2\n",
            "Iteration 2620: Mean Return = 5.2\n",
            "Iteration 2630: Mean Return = -0.2\n",
            "Iteration 2640: Mean Return = 5.6000000000000005\n",
            "Iteration 2650: Mean Return = -96.80000000000001\n",
            "Iteration 2660: Mean Return = 7.0\n",
            "Iteration 2670: Mean Return = 4.2\n",
            "Iteration 2680: Mean Return = 5.6000000000000005\n",
            "Iteration 2690: Mean Return = -97.0\n",
            "Iteration 2700: Mean Return = 3.0\n",
            "Iteration 2710: Mean Return = 7.4\n",
            "Iteration 2720: Mean Return = 3.0\n",
            "Iteration 2730: Mean Return = -8.8\n",
            "Iteration 2740: Mean Return = 4.2\n",
            "Iteration 2750: Mean Return = 4.6000000000000005\n",
            "Iteration 2760: Mean Return = 4.6000000000000005\n",
            "Iteration 2770: Mean Return = 7.0\n",
            "Iteration 2780: Mean Return = -98.80000000000001\n",
            "Iteration 2790: Mean Return = 1.4000000000000001\n",
            "Iteration 2800: Mean Return = 6.6000000000000005\n",
            "Iteration 2810: Mean Return = -0.6000000000000001\n",
            "Iteration 2820: Mean Return = -8.8\n",
            "Iteration 2830: Mean Return = -97.60000000000001\n",
            "Iteration 2840: Mean Return = -200.0\n",
            "Iteration 2850: Mean Return = 0.6000000000000001\n",
            "Iteration 2860: Mean Return = 7.4\n",
            "Iteration 2870: Mean Return = -1.6\n",
            "Iteration 2880: Mean Return = -1.8\n",
            "Iteration 2890: Mean Return = 2.0\n",
            "Iteration 2900: Mean Return = 6.6000000000000005\n",
            "Iteration 2910: Mean Return = 3.2\n",
            "Iteration 2920: Mean Return = 5.800000000000001\n",
            "Iteration 2930: Mean Return = 1.6\n",
            "Iteration 2940: Mean Return = 3.6\n",
            "Iteration 2950: Mean Return = 3.6\n",
            "Iteration 2960: Mean Return = 2.6\n",
            "Iteration 2970: Mean Return = 2.4000000000000004\n",
            "Iteration 2980: Mean Return = -2.6\n",
            "Iteration 2990: Mean Return = 7.4\n",
            "Iteration 3000: Mean Return = 4.0\n",
            "Iteration 3010: Mean Return = -100.0\n",
            "Iteration 3020: Mean Return = 2.6\n",
            "Iteration 3030: Mean Return = 6.800000000000001\n",
            "Iteration 3040: Mean Return = 3.0\n",
            "Iteration 3050: Mean Return = 0.6000000000000001\n",
            "Iteration 3060: Mean Return = 0.6000000000000001\n",
            "Iteration 3070: Mean Return = 0.4\n",
            "Iteration 3080: Mean Return = 3.4000000000000004\n",
            "Iteration 3090: Mean Return = 4.4\n",
            "Iteration 3100: Mean Return = 2.0\n",
            "Iteration 3110: Mean Return = 5.4\n",
            "Iteration 3120: Mean Return = -1.2000000000000002\n",
            "Iteration 3130: Mean Return = 5.4\n",
            "Iteration 3140: Mean Return = 9.4\n",
            "Iteration 3150: Mean Return = 4.4\n",
            "Iteration 3160: Mean Return = 5.2\n",
            "Iteration 3170: Mean Return = 6.2\n",
            "Iteration 3180: Mean Return = 4.0\n",
            "Iteration 3190: Mean Return = 5.6000000000000005\n",
            "Iteration 3200: Mean Return = 6.800000000000001\n",
            "Iteration 3210: Mean Return = 8.0\n",
            "Iteration 3220: Mean Return = 3.0\n",
            "Iteration 3230: Mean Return = 7.6000000000000005\n",
            "Iteration 3240: Mean Return = 4.2\n",
            "Iteration 3250: Mean Return = 6.800000000000001\n",
            "Iteration 3260: Mean Return = 6.6000000000000005\n",
            "Iteration 3270: Mean Return = 5.0\n",
            "Iteration 3280: Mean Return = 5.800000000000001\n",
            "Iteration 3290: Mean Return = 3.4000000000000004\n",
            "Iteration 3300: Mean Return = 4.2\n",
            "Iteration 3310: Mean Return = -0.8\n",
            "Iteration 3320: Mean Return = 5.4\n",
            "Iteration 3330: Mean Return = 6.2\n",
            "Iteration 3340: Mean Return = 4.4\n",
            "Iteration 3350: Mean Return = 5.800000000000001\n",
            "Iteration 3360: Mean Return = 5.800000000000001\n",
            "Iteration 3370: Mean Return = 7.0\n",
            "Iteration 3380: Mean Return = 5.800000000000001\n",
            "Iteration 3390: Mean Return = 5.0\n",
            "Iteration 3400: Mean Return = 6.6000000000000005\n",
            "Iteration 3410: Mean Return = 3.4000000000000004\n",
            "Iteration 3420: Mean Return = 0.4\n",
            "Iteration 3430: Mean Return = 5.0\n",
            "Iteration 3440: Mean Return = -97.4\n",
            "Iteration 3450: Mean Return = 4.4\n",
            "Iteration 3460: Mean Return = 3.2\n",
            "Iteration 3470: Mean Return = 7.2\n",
            "Iteration 3480: Mean Return = 6.2\n",
            "Iteration 3490: Mean Return = 8.200000000000001\n",
            "Iteration 3500: Mean Return = 4.6000000000000005\n",
            "Iteration 3510: Mean Return = -93.60000000000001\n",
            "Iteration 3520: Mean Return = 7.2\n",
            "Iteration 3530: Mean Return = -100.80000000000001\n",
            "Iteration 3540: Mean Return = 5.4\n",
            "Iteration 3550: Mean Return = 3.0\n",
            "Iteration 3560: Mean Return = -93.0\n",
            "Iteration 3570: Mean Return = 4.800000000000001\n",
            "Iteration 3580: Mean Return = 9.600000000000001\n",
            "Iteration 3590: Mean Return = 5.0\n",
            "Iteration 3600: Mean Return = 3.6\n",
            "Iteration 3610: Mean Return = 5.800000000000001\n",
            "Iteration 3620: Mean Return = 5.800000000000001\n",
            "Iteration 3630: Mean Return = 7.4\n",
            "Iteration 3640: Mean Return = 4.6000000000000005\n",
            "Iteration 3650: Mean Return = 4.0\n",
            "Iteration 3660: Mean Return = 6.0\n",
            "Iteration 3670: Mean Return = 3.0\n",
            "Iteration 3680: Mean Return = 6.800000000000001\n",
            "Iteration 3690: Mean Return = 3.2\n",
            "Iteration 3700: Mean Return = 3.0\n",
            "Iteration 3710: Mean Return = 1.6\n",
            "Iteration 3720: Mean Return = 5.0\n",
            "Iteration 3730: Mean Return = 5.4\n",
            "Iteration 3740: Mean Return = 4.2\n",
            "Iteration 3750: Mean Return = 5.2\n",
            "Iteration 3760: Mean Return = 7.2\n",
            "Iteration 3770: Mean Return = 7.800000000000001\n",
            "Iteration 3780: Mean Return = 7.0\n",
            "Iteration 3790: Mean Return = 4.2\n",
            "Iteration 3800: Mean Return = 2.0\n",
            "Iteration 3810: Mean Return = 5.800000000000001\n",
            "Iteration 3820: Mean Return = 2.6\n",
            "Iteration 3830: Mean Return = 8.8\n",
            "Iteration 3840: Mean Return = 1.4000000000000001\n",
            "Iteration 3850: Mean Return = 6.4\n",
            "Iteration 3860: Mean Return = 6.0\n",
            "Iteration 3870: Mean Return = 5.6000000000000005\n",
            "Iteration 3880: Mean Return = 6.6000000000000005\n",
            "Iteration 3890: Mean Return = 5.800000000000001\n",
            "Iteration 3900: Mean Return = 6.4\n",
            "Iteration 3910: Mean Return = 8.0\n",
            "Iteration 3920: Mean Return = 4.800000000000001\n",
            "Iteration 3930: Mean Return = 6.4\n",
            "Iteration 3940: Mean Return = 6.0\n",
            "Iteration 3950: Mean Return = 6.6000000000000005\n",
            "Iteration 3960: Mean Return = 7.800000000000001\n",
            "Iteration 3970: Mean Return = 6.6000000000000005\n",
            "Iteration 3980: Mean Return = 5.800000000000001\n",
            "Iteration 3990: Mean Return = 8.4\n",
            "Iteration 4000: Mean Return = 4.0\n",
            "Iteration 4010: Mean Return = 4.0\n",
            "Iteration 4020: Mean Return = 7.4\n",
            "Iteration 4030: Mean Return = 9.0\n",
            "Iteration 4040: Mean Return = 8.200000000000001\n",
            "Iteration 4050: Mean Return = 3.0\n",
            "Iteration 4060: Mean Return = 8.200000000000001\n",
            "Iteration 4070: Mean Return = 5.2\n",
            "Iteration 4080: Mean Return = 5.6000000000000005\n",
            "Iteration 4090: Mean Return = 7.4\n",
            "Iteration 4100: Mean Return = 5.4\n",
            "Iteration 4110: Mean Return = 8.0\n",
            "Iteration 4120: Mean Return = 5.0\n",
            "Iteration 4130: Mean Return = 5.6000000000000005\n",
            "Iteration 4140: Mean Return = 5.800000000000001\n",
            "Iteration 4150: Mean Return = 7.2\n",
            "Iteration 4160: Mean Return = 7.6000000000000005\n",
            "Iteration 4170: Mean Return = 7.800000000000001\n",
            "Iteration 4180: Mean Return = 8.0\n",
            "Iteration 4190: Mean Return = 8.0\n",
            "Iteration 4200: Mean Return = 6.6000000000000005\n",
            "Iteration 4210: Mean Return = 5.4\n",
            "Iteration 4220: Mean Return = 7.4\n",
            "Iteration 4230: Mean Return = 6.2\n",
            "Iteration 4240: Mean Return = 4.6000000000000005\n",
            "Iteration 4250: Mean Return = 8.8\n",
            "Iteration 4260: Mean Return = 6.6000000000000005\n",
            "Iteration 4270: Mean Return = 6.0\n",
            "Iteration 4280: Mean Return = 6.4\n",
            "Iteration 4290: Mean Return = 6.2\n",
            "Iteration 4300: Mean Return = 5.2\n",
            "Iteration 4310: Mean Return = 7.800000000000001\n",
            "Iteration 4320: Mean Return = 8.4\n",
            "Iteration 4330: Mean Return = 5.800000000000001\n",
            "Iteration 4340: Mean Return = 5.6000000000000005\n",
            "Iteration 4350: Mean Return = 5.2\n",
            "Iteration 4360: Mean Return = 7.800000000000001\n",
            "Iteration 4370: Mean Return = 7.2\n",
            "Iteration 4380: Mean Return = 5.4\n",
            "Iteration 4390: Mean Return = 6.800000000000001\n",
            "Iteration 4400: Mean Return = 6.2\n",
            "Iteration 4410: Mean Return = 3.2\n",
            "Iteration 4420: Mean Return = 6.2\n",
            "Iteration 4430: Mean Return = 5.4\n",
            "Iteration 4440: Mean Return = 4.800000000000001\n",
            "Iteration 4450: Mean Return = 8.200000000000001\n",
            "Iteration 4460: Mean Return = 7.0\n",
            "Iteration 4470: Mean Return = 6.800000000000001\n",
            "Iteration 4480: Mean Return = 7.2\n",
            "Iteration 4490: Mean Return = 6.0\n",
            "Iteration 4500: Mean Return = 3.6\n",
            "Iteration 4510: Mean Return = 5.800000000000001\n",
            "Iteration 4520: Mean Return = 5.0\n",
            "Iteration 4530: Mean Return = 7.6000000000000005\n",
            "Iteration 4540: Mean Return = 5.6000000000000005\n",
            "Iteration 4550: Mean Return = -94.4\n",
            "Iteration 4560: Mean Return = 8.0\n",
            "Iteration 4570: Mean Return = 6.800000000000001\n",
            "Iteration 4580: Mean Return = 7.6000000000000005\n",
            "Iteration 4590: Mean Return = 6.2\n",
            "Iteration 4600: Mean Return = 6.4\n",
            "Iteration 4610: Mean Return = 9.200000000000001\n",
            "Iteration 4620: Mean Return = 6.2\n",
            "Iteration 4630: Mean Return = 6.4\n",
            "Iteration 4640: Mean Return = 4.800000000000001\n",
            "Iteration 4650: Mean Return = 9.0\n",
            "Iteration 4660: Mean Return = 7.2\n",
            "Iteration 4670: Mean Return = -96.4\n",
            "Iteration 4680: Mean Return = 5.6000000000000005\n",
            "Iteration 4690: Mean Return = 6.0\n",
            "Iteration 4700: Mean Return = 8.6\n",
            "Iteration 4710: Mean Return = 7.0\n",
            "Iteration 4720: Mean Return = 4.2\n",
            "Iteration 4730: Mean Return = 6.4\n",
            "Iteration 4740: Mean Return = 6.4\n",
            "Iteration 4750: Mean Return = 8.200000000000001\n",
            "Iteration 4760: Mean Return = 7.2\n",
            "Iteration 4770: Mean Return = 9.200000000000001\n",
            "Iteration 4780: Mean Return = 7.6000000000000005\n",
            "Iteration 4790: Mean Return = 7.2\n",
            "Iteration 4800: Mean Return = 9.4\n",
            "Iteration 4810: Mean Return = 4.4\n",
            "Iteration 4820: Mean Return = 9.200000000000001\n",
            "Iteration 4830: Mean Return = 7.2\n",
            "Iteration 4840: Mean Return = 5.800000000000001\n",
            "Iteration 4850: Mean Return = 6.4\n",
            "Iteration 4860: Mean Return = 6.6000000000000005\n",
            "Iteration 4870: Mean Return = 4.800000000000001\n",
            "Iteration 4880: Mean Return = 6.0\n",
            "Iteration 4890: Mean Return = 6.0\n",
            "Iteration 4900: Mean Return = 6.800000000000001\n",
            "Iteration 4910: Mean Return = 7.4\n",
            "Iteration 4920: Mean Return = -95.60000000000001\n",
            "Iteration 4930: Mean Return = 4.2\n",
            "Iteration 4940: Mean Return = 6.4\n",
            "Iteration 4950: Mean Return = 8.4\n",
            "Iteration 4960: Mean Return = 6.4\n",
            "Iteration 4970: Mean Return = 7.4\n",
            "Iteration 4980: Mean Return = 7.2\n",
            "Iteration 4990: Mean Return = 6.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEGCAYAAACgt3iRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxdVb338c8vJ2PbtGmTdJ5pamkLFAxlqkwWKEUtTo8FUSZFrqKojxeK6L14EeV6r3qvV67KVRQeecQBkT7IYFFmpLRACy2lkE50ntOkTTP/nj/OTnrSJjknOWNOvu/XK6+cs/Y+e//WaXp+Z62119rm7oiIiMQjJ90BiIhI36dkIiIicVMyERGRuCmZiIhI3JRMREQkbrnpDiBdysrKfOLEiekOQ0SkT3nllVf2uHv50eX9NplMnDiR5cuXpzsMEZE+xcw2dVaubi4REYmbkomIiMRNyUREROKmZCIiInFTMhERkbgpmYiISNyyJpmY2TwzW2tmVWa2KN3xiIj0J1kxz8TMQsBdwAXAFmCZmS129zfTG5lkum3Vh8nNMZpaHQNaWp0RgwvJz+34PauppZWWVic3xzAztlUfZlBBLvm5OQwsyMXdaWl1auqbGVyYS4s77lCYF8KDxxv3HqKusYXJ5QMpygthZrS2OrUNzVTXNeIO5cUFDMgPAdDY0srB+mbycnMYkBei+nATtfXNTCwdwNbqw7y7t45N++o4deIwGptbGZAfYtuBw0wfNRiArdWHmVw2iNyQ0dzirNhczbSRxTS1trK7toGB+bm8u6+OOVPKONjYzI4D9YRyjO3V9WzZX8e57xnOrtp6Rg4uZGBBLjtq6tld28DxowazYnM1DU0tlBUXUJQXYtjAfIYOyOfPb2wjL5TDee8Zzr5D4Tq98u4+hhcXclz5IPJCRtWug7xnZDFvbq9hd20DDU2trNlRQ9mgAsYNG8CZx5XS2NzKvkON1De18ELVXmaNL2F3bQMnjR3C3kON1NY3M2pIIS2tTnlxAdsP1PPs27vZWn2YaSOLWbO9lrrGZuZUlLHkzZ2cdVwZRfkhpo0s5q0dtTy+agezxpUwpCiPkgF5rNt9iJIBeVROGEpxYR5v7ajh3b11TB1ZzJvbahg7tIgVm6uZXD6IbdWHmTOljP3Bv9njq3dwoK6JM6eUcrixhQ17DnHi2CHUN7VSU9/EedOGU5QXYseBerZWH2ZAfogTxgxh7c5aNuw+RKvDso37GDesiAumj2Dl5gNcNGMkT6/dxYOvbuWqMyewfNN+zjyulOer9jJtZDGzJw5j8/46Rg0pYvnGfTy2ageXzR7HkKI8tlQf5qX1+7j+7MlsP1CPGazZXsPIIUVMG1nM6JIijh9VTEFuKKH/lywb7mdiZmcAt7n7RcHzWwDc/btdvaaystI1afGIPQcbMKB0UAEA1XWNNLa0Mry4kF019ZQXF2Bmx7yuvqmFzfvq+M6ja7h2zmSu+MVSAN66fR5rd9Sy4K4XALjvmtn88+LVzBpXwg8/MYuXN+xj6fq9TB89mDe31bB2Zy2PvL69/bjnTxvO1+cfz9wfPMOJY4cQyjF21TRQOiifgtwclm3cn/w3RSRLPXbj+zg++NLRU2b2irtXHlOeJcnkY8A8d/9M8PxTwGnufsNR+10HXAcwfvz4927a1OlEzqz01zU7WbphH4vmTSMnx2hqaeV7j79FbX0zdY0tLF65DYAnvnw226oPc/WvlnV6nPknjOSdnQd5Z9fBVIYvIgn00OfP5OTxQ3v12q6SSVZ0c8XK3e8G7oZwyyTN4SRMfVMLX35gBV+7aCrjhw1k6jceA2D2pGH87Ir3snpbDdfeG26F3f3s+m6PddF/PNvt9kff2JGYoCWj/NdlJ/PF37zW7T5PfvUcPvGzv7P3UGOX+2y885Lw7z2HOPffn24vm7joz12+5o3bLuSE2/4SU5xtx3f3Y1rKv35pE9/40yoA5kwp49efOQ2gw7nbXv/u3jrO/renYjpnm9dvu5ATgzg3fHc+k255NOprzppSygtVe7vcPvf4ETy5ZmfU4/z805V85r7w/+G2Olzzq2X87a1dUV975nGlLJw9ni9F/PvmdNLLEK9sSSZbgXERz8cGZVmnpdX5+h/fYMrwQVw4YwTLN+7nf/9+JRDuu4308oZ9nHz7knSEKX3MB08aHTWZDCrIZeSQwm6TSZuJZQNjPndvPtg663KN7GXJyen+mL35LI18SWfn70xra5RjxhhHZ/u1xtir1Nlrk5BLsiaZLAMqzGwS4SSyELg8vSHF5+2dtWzaW8dn71vOty+dyRtbDvDb5Zs77HPHo2vSFJ30dQtmjebhFdt69JpQjiXlQyhR35JbIz5bc6Mkk9707vcmTqf7E0UJs9tzx1qHzl5rqGXSKXdvNrMbgCeAEHCPu69Oc1i91tzSyoU/PNLd1NZ0F0mU8cMG9Pg1oVg/+XooUQkqsmUSLdZYv9VH6k0yaY1ymljf0/haJp0kE7VMuubujwLROzEz2N/X7eUHS9bqSiXJSCGzpHyjTUvLpBfH71WYUU4U6/sZTyJPzleAY2VNMskGl/3PS+kOQaRLOTnJ+UabqAZP5Od2Mlomval7tPPEeszOEm6sdUhSg/LY86TmNCLS18U66NxTiWqZ9KSbqzdTInrTKoueTOLo5ooyuN8mx5LRnuzkPCk4h0RRXddIQ3NLusOQfmRcL8ZMCnPDM9sTLVE5atrII5PwKicOi3bWLreUDMjr/BXBS04aOyTmmE6NEsels0ZTXBC9g6ikKP+YsooRg2KKYcbowZw0tqRD2YjBhTG9tieyYtJib2TSDPiJi/7MnCllPF+1J92h9Gt/uP4MPvbTv3coO/O4Ul5cd2SewOyJw3h5474O+0woHUBhboh9dY3srm1oL/9E5bgOV+B98fwpzJ40jHtf3BTT3ILjRw3mPxfOIj+Uw8ot1dz4wIoO2685axJDivJ4cd0e7v/MaXx/ydv85Ol1Hfb585fmcMmPngfC39bPPK6UCaUDuH3BTH6w5G3+629VfPvSmVxx+oT2+RjP/uN5DB2Yx2OrdnDxzJGs3HyAORVlANQ1NjP9n54AwhPfCnJD3PTgSr55yXTycnM4JWIi3F/X7KQgN8ScijLW7qhly/46Rg4ppGJ4MT95eh2nTx7GoMJcZowewtbqw/ziuQ1MKhvAvJmjKMzLIZRjLF2/j5r6Jg43tnDa5FImRbnkePO+OuoaW5g6YlD7t/79hxp5fesBTho7hJIB4Q9ld+cHS95m/LABXDRzJE+s2sGL6/ZyqKGZOz58Aq3uNLWEv/oX5YVYta2Gc6aWs6umnuLCPIryQ7y1o4bn3t7DwIJcJpUN5KRxQ1i1tYYxQ4s4686/AVB1x8Ws3VlLQW4OhxtbKRmQx0vr9/LBk0ZTmBdezuRgQzPNLa3U1jfz2uZqjisfyOIV2xg+uJBzppaxcU8dc6ePYM/BBppbnJFDwomgobmFVzbtZ0xJEcOLC2lxZ8eBwxTkhijIy6G2vpk3t9Uw/4RR7S21A4ebeHtnbdQk152sngHfG5mWTKRrP7rs5PYJVwtPHcfw4gION7Vw87xpTLn1yATNlzcc+ZD/0Emj22f1v/rNCygpyuPBV7cwYnAhn77nZQDOfU85v7p6Nq++u5+ygQWMLz3ybb3t32TjnZdwwm1PUFvfzMp/vpAhRXnUN7VQ19jClfe8zBtbD/Dra09jTkUZBxuamfnP4Q/aX1xZyfuPH9Hh3/at2+e1f4DsPdjA81V7mDqiuH1Zi6aWViqC+qz61kUMiuEba2eeWruLq3+5jHOmlnPvNbM71KU7+w81UpCXw4D8rs9b39TCtG8+HtPx+qu29yjHYP13s+896iqZqJsrxTbvq6NqVy2trc7jq7b3qu+2r8kL9awf4/xpw/npFacAkJ+bw4dOGt2+7aZ50/jqhe/h1kumkxs68ud7z1Wn8sgX57Q//9FlJ7c/HjYwn5wc4+OV45hcfuSbbVtUp4wf2iGRRFMYLGzY9mHf1v3R1k0fyjHef/yIY14XOTZQOqiABbPGdFgfKS+iPr1NJEDvLlUChg7M7zaRSGySNLSU8fSXk2Lv+154CYfbL53JN9M8f2T1ty5iRvBNujNTRwzi7Z2xrcFVkJtDQ3OMI4JRjC4pZMbocL90ebDw5HM3nceTa3YybOCxfccXzxzJoIJcZo6J3pcdmbsTNaDcdpS2ZNHVF4RUf8gk43z99YOyJ1Iz3J15lEzSZMeBw+kOgYFRvv1+9JSxvPZu9THLtHSmZEAeO2saOt3W08ZXZ/8Zxw0bwNVnTTqmfP135vf6A67HL/Ojn/asYslYD6k7/aDRm5H6a8JVN1cKPL5qOys3V6c7jB5ZdPE0rp0zKWrCiUU8E8SidQPmBPcXiVXkpZoxr4vURXnbodrO394y6WL/VF3vn8wvxv31W7dEp5ZJClz/61eBjgOWdz21rqvdM8JHTh7TYUwilcyOfEAn+st1x9wU3wfjkWQS/p1jHcuPlqx5GqnUVoUsqErS9Ne3Ri0TSbqeXmRgHPkPmeiumsjDJeoDse0w2ZAsorGjfsux+sPfQWeUTPqBf//4SWk9f1zdXAlumwyNmJB2YgwD9gCTy8OTw0JHXZU2a3x4IlhZcfgigc66sQbmJ/bWqLEYGsyliDYnozfa/jX66wdmLNremU+cOq7b/bKNurlSqK6xOS3nHdrFjN7uDCoM/2l89cKpPPjqlqj7X3HaBL6/5O1Ot33q9Anc9/fwXS07m/R3tLxQDqUDCxiYH+Lr84/vYeRH/OUrZx8zVlUyIJ/l35jLjgP17fdK78ySr5xNUZAI7rnqVFZurj7mct2bLnoPHz55DMcFycbMGJgf4uaLp7Xv88xN57Fl/+GYk8pzN513zP3ne2rWuBJ+dfWpnHFcKRC+RWtBnMdsk5tjnDO1nKvOnJiQ42WjnBzjzX+5KOH3WM90SiYp1DZ2kmrnTzt2CYw7Pjyzw/NPnjae+5e+2/68bb7BmJIiRgwuYFBBeKZy20TAL8+toGrXQR55fTszxwzmhvOn8L9OHcdp3/nrMee64bwp3PbBGdTUN1EyIL99Et09V1VSXdfEV3+3kqe/di75uTnc8/wGbpxbQX5uDqv/ZV5c9Z46opipI4qPKS8bVEBZcMlxVyoiXjdsYD7ndfIe5oZyjrmP9tExx3KuSL1Z5qQz50Yse9Lbe313xsy495rZCTtetuqP83X6X43T6MU0LZfSWZfEJ0+bAIRbLfvrmrj+nON4Z9dBPv7esZw+ubTDvku/Prf98ZVnTmRC6QDKBhXww6Alcv60EZgZ5YMKWHjqOD5w4miu+MXSiADC39balrK475rZTCgdwITScDfMR04Z277rNz4wPTGVFpGU0phJCjVHu1NOL3zu7Mlxvb4liKm4MJfffe4MPl45rttvx++dMPSYb9rtk/ZyjDs/eiJzKsq46/JTIrZ3TGZnTy1vTyQikh2UTPq4c6aW875gEb4Zo490Z0QuLdKdT58xEaB9fKA3OkuRl5w4qr1ffWBB/+o7FumPlEz6uOmjB3P3pyp55h/P7bAS6NFLi9zaxUD2/75wKuu/M79Xg4XRLuj55gems+KfLuiX/cci/Y3+l/dh186Z1D4OEa3bqLiw839qM0vaBLRQxDiJiGQ3tUz6sKMn9PWHFYhFJDMpmWSo0ycPS2iLIRmtj7FDBwS/ixJ/8F4aU1LU4yXvRSR+6ubKUF96fwWf/PnSY8q7m/QX68Vi1845dvXd3vjoKWMYObiQs6aURt85RZ696Ty10ETSQC2TDPQvC2ZwxuRS7ouYHPbGbRfyxm0Xdvu6AcEVWZ2Nj7yvohwI32r1mwmay2FmzKkoy6ilNUI5lrYFKkX6M7VMkqi2volfvrCxR68pzMtpv1z3fRXlLJg1mheq9lBc2HFJlFFDCvni+VM6lLUtmdFZy2N0SZFusyoiSaNkkkTfefQtfvPyu9F3jHDGUbPP/3PhyR13CBoBP/zELIYeddfB9kX4tKariKSY+gOS6FBDzxd2nFQ2qNvtbetsjSk5dtD7AyeG75U+/4SR7WWnBCvbiogkk/XXwcrKykpfvnx5Us9x6V0vsKKHd1h86/Z5FOZ1PYHQ3dl3qJHSGBYP3LT3EGWDChJyt0QREQAze8XdK48u16dMEvU0kZw/bXi3iQTCg96xJBKIPpFRRCRR1M2VQTTSISJ9lZKJiIjETckkg2TQdA0RkR5RMskgl582Pt0hiIj0igbgM4QmFIpIX6aWiYiIxE3JJMHcnfqmlnSHISKSUhmXTMzsNjPbamYrgp/5EdtuMbMqM1trZhdFlM8LyqrMbFF6Ig+7f+m7TPvm42zaeyidYYiIpFSmjpn80N3/PbLAzKYDC4EZwGjgSTObGmy+C7gA2AIsM7PF7v5mKgNu8+gb2wH402vb0nF6EZG0yNRk0pkFwAPu3gBsMLMqoG2N9ip3Xw9gZg8E+6YlmbTRZb4i0p9kXDdX4AYze93M7jGzoUHZGGBzxD5bgrKuyo9hZteZ2XIzW7579+5kxH3kXD3Y91sfmpG0OEREUiEtycTMnjSzVZ38LAB+AhwHzAK2A99P1Hnd/W53r3T3yvLy8kQdtlPRWiZtN7ICuGD6iKTGIiKSbGnp5nL3ubHsZ2b/AzwSPN0KjIvYPDYoo5vylGtbhHnzvsPd7rfkq+dQOjCfQw3NMS/cKCKSqTKum8vMRkU8/TCwKni8GFhoZgVmNgmoAF4GlgEVZjbJzPIJD9IvTmXMnfnt8s3dbs/LMQrzQkokIpIVMnEA/ntmNovwjQM3Ap8DcPfVZvY7wgPrzcAX3L0FwMxuAJ4AQsA97r46HYHH6vpzjqO8WElERLJHxiUTd/9UN9vuAO7opPxR4NFkxhWrWK7iWnTxtOQHIiKSQhnXzdXX9dMbV4pIP6dkIiIicVMyERGRuCmZiIhI3JRMUmzOlLJ0hyAiknBKJilWkKu3XESyjz7ZUkwLQIpINlIyERGRuCmZiIhI3JRMEsyJNmtR/Vwikn2UTEREJG5KJiIiEjclkwSzKN1YuppLRLKRkkmCRR8zERHJPkomIiISNyWTFJswbEC6QxARSbiMuzlWNvvlVadyltbmEpEspGSSQudNG57uEEREkkLdXCIiEreoLRMzKwc+C0yM3N/dr0leWCIi0pfE0s31MPAc8CTQktxwstd4DbyLSBaLJZkMcPebkx5Jlvv6/GnpDkFEJGliGTN5xMzmJz2SLOGasygi/VAsyeRGwgnlsJnVmFmtmdUkO7Dso3VURCR7ddvNZWY5wDx3fyFF8WQtrcklItms25aJu7cCP05RLFmhq6ShXCIi2SyWbq6/mtlHzfTdOh56+0Qkm8WSTD4H/B5o0JhJdBqAF5H+KOqlwe5enIpAst2EUs0zEZHsFcsM+LM7K3f3ZxMfTvaaOkI5WUSyVyyTFv8x4nEhMBt4BTg/KRH1cRv2HEp3CCIiKRdLN9cHI5+b2TjgP5IWUR+3q7Yh3SGIiKRcb1YN3gIcn+hARESk74plzOS/oP3G5jnALODVZAYlIiJ9Sywtk+WEx0heAf4O3OzuV8RzUjP7uJmtNrNWM6s8atstZlZlZmvN7KKI8nlBWZWZLYoon2RmS4Py35pZfjyxiYhIz8WSTErc/d7g5353f8HMbozzvKuAjwAdrggzs+nAQmAGMA/4bzMLmVkIuAu4GJgOXBbsC/CvwA/dfQqwH7g2zthERKSHYkkmV3ZSdlU8J3X3Ne6+tpNNC4AH3L3B3TcAVYSvHpsNVLn7endvBB4AFgSz8s8H/hC8/l7g0nhiExGRnutyzMTMLgMuByaZ2eKITcXAviTFMwZ4KeL5lqAMYPNR5acBpUC1uzd3sv8xzOw64DqA8ePHJyhkERHpbgD+RWA7UAZ8P6K8Fng92oHN7ElgZCebbnX3h3sSZKK4+93A3QCVlZVa+EREJEG6TCbuvgnYBJxhZhOACnd/0syKgCLCSaVL7j63F/FsBcZFPB8blNFF+V6gxMxyg9ZJ5P4iIpIiUcdMzOyzhMckfhYUjQX+lKR4FgMLzazAzCYBFcDLwDKgIrhyK5/wIP1id3fgKeBjweuvJHzPehERSaFYBuC/AJwF1AC4+zvA8HhOamYfNrMtwBnAn83sieDYq4HfAW8CjwNfcPeWoNVxA/AEsAb4XbAvwM3AV82sivAYyi/iiU1ERHoulrW5Gty9se1+HGaWy5FJjL3i7g8BD3Wx7Q7gjk7KHwUe7aR8PeGrvdJuz0EtpSIi/VMsLZNnzOzrQJGZXUD43ib/L7lh9U2V334y3SGIiKRFLMlkEbAbeIPwjbIedfdbkxpVH/TGlgPpDkFEJG2iJhN3b3X3/3H3j7v7x4BNZrYkBbH1KR/88fPpDkFEJG26TCZmdr6ZvW1mB83s12Z2gpktB74L/CR1IYqISKbrrmXyfcKzxUsJXxr8d+BX7v5ed/9jKoITEZG+oburudzdnw4e/8nMtrr7j1MQU5/S2NzKoj9GXRBARCSrdZdMSszsI5H7Rj5X6yTsxXV7+OOrXU+6Ly7M5fmbdYdjEclu3SWTZ4DIW/Y+G/HcASUTIDen+2sYbp43jSFFeSmKRkQkPbpbm+vqVAbSV0XJJSIi/YI+CuMUClYGEBHpz5RM4hTKUTIREVEyicPW6sO8s+tgt/ucNLYkRdGIiKRPLAs9YmZnAhMj93f3+5IUU59x1p1/i7rPCWOHpCASEZH0ippMzOz/AMcBK4CWoNiBfp9MREQkLJaWSSUwPbgRlYiIyDFiGTNZRef3cpcoHvninHSHICKSErG0TMqAN83sZaD97k/u/qGkRZXhmlpa+drvV0bdb+jA/BREIyKSfrEkk9uSHURfs2zjPh5esS3qfrpoWET6i6jJxN2fSUUg2UjzGUWkv4g6ZmJmp5vZsuC+Jo1m1mJmNakILlNZjG2OWPcTEenrYhmA/zFwGfAOUAR8BrgrmUFlC7VMRKS/iGkGvLtXASF3b3H3XwLzkhuWiIj0JbEMwNeZWT6wwsy+B2xHy7CIiEiEWJLCp4L9bgAOAeOAjyYzqEwXa/eVerlEpL+I5WquTWZWBIxy92+lIKbsoWwiIv1ELFdzfZDwulyPB89nmdniZAeWDYYN0KRFEekfYunmug2YDVQDuPsKYFISY8oauSENLYlI/xDLp12Tux84qqxfL/qo3isRkY5iuZprtZldDoTMrAL4EvBicsMSEZG+JJaWyReBGYQXefwNUAN8OZlBiYhI3xLL1Vx1wK3BjwCmqe0iIh10mUyiXbHVn5egP9TYnO4QREQySnctkzOAzYS7tpaiced29zy/Id0hiIhklO6SyUjgAsKLPF4O/Bn4jbuvTkVgmaxVdzAWEemgywH4YFHHx939SuB0oAp42sxuSFl0GUpLy4uIdNTt1VxmVmBmHwF+DXwB+BHwULwnNbOPm9lqM2s1s8qI8olmdtjMVgQ/P43Y9l4ze8PMqszsRxaMgpvZMDNbYmbvBL+Hxhtf9PiTfQYRkb6ly2RiZvcBfwdOAb7l7qe6++3uvjUB510FfAR4tpNt69x9VvBzfUT5T4DPAhXBT9sy+IuAv7p7BfDX4HlSqZdLRKSj7lomVxD+0L4ReNHMaoKf2njvtOjua9x9baz7m9koYLC7v+TuDtwHXBpsXgDcGzy+N6I8adQyERHpqMsBeHdP18JSk8zsNcKTI7/h7s8BY4AtEftsCcoARrj79uDxDmBEVwc2s+uA6wDGjx+f6LhFRPqtWJZT6RUze5LwFWFHu9XdH+7iZduB8e6+18zeC/zJzGbEek53dzPrshPK3e8G7gaorKxUZ5WISIIkLZm4+9xevKaB8LItuPsrZrYOmApsBcZG7Do2KAPYaWaj3H170B22K77IRUSkpzJqjXQzKzezUPB4MuExm/VBN1aNmZ0eXMX1aaCtdbMYuDJ4fGVEuYiIpEhakomZfdjMthCeZf9nM3si2HQ28LqZrQD+AFzv7vuCbZ8Hfk54vss64LGg/E7gAjN7B5gbPBcRkRRKWjdXd9z9ITqZr+LuDwIPdvGa5cDMTsr3Au9PdIzd0UKPIiIdZVQ3V1+hVCIi0pGSiYiIxE3JRERE4qZk0gsaMhER6UjJRERE4qZkIiIicVMy6QX1comIdKRkIiIicVMy6YVWLREpItKBkkkvPPP27nSHICKSUZRMREQkbkomSbLhu/PTHYKISMoomSSJFoMUkf5EyUREROKmZCIiInFTMkkC9XCJSH+jZCIiInFTMkkCNUxEpL9RMumhnz2zLt0hiIhkHCWTHvrxU1XpDkFEJOMomSRBKEcdXSLSvyiZ9JDHsMhjxfDi5AciIpJBlEx6qCWGJYPvu3Z2CiIREckcSiY9FEsyKRtUkIJIREQyh5JJDzW3tqY7BBGRjKNk0kO6MZaIyLGUTEREJG5KJiIiEjclkwT75dWnpjsEEZGUUzJJoGvOmsR57xme7jBERFJOyUREROKmZCIiInFTMhERkbgpmYiISNzSkkzM7N/M7C0ze93MHjKzkohtt5hZlZmtNbOLIsrnBWVVZrYoonySmS0Nyn9rZvmprs+RWNJ1ZhGR9EpXy2QJMNPdTwTeBm4BMLPpwEJgBjAP+G8zC5lZCLgLuBiYDlwW7Avwr8AP3X0KsB+4NqU1ERGR9CQTd/+LuzcHT18CxgaPFwAPuHuDu28AqoDZwU+Vu69390bgAWCBmRlwPvCH4PX3Apemqh5fmTs1VacSEclomTBmcg3wWPB4DLA5YtuWoKyr8lKgOiIxtZV3ysyuM7PlZrZ89+7dcQd+8Qkj2XjnJXEfR0Skr8tN1oHN7ElgZCebbnX3h4N9bgWagfuTFUckd78buBugsrIy7iUbp47QTbBERCCJycTd53a33cyuAj4AvN+9/f6FW4FxEbuNDcroonwvUGJmuUHrJHJ/ERFJkXRdzTUPuAn4kLvXRWxaDCw0swIzmwRUAC8Dy4CK4MqtfMKD9IuDJPQU8LHg9VcCD6eqHiIiEpa0lkkUPwYKgCXhMXRecvfr3X21mf0OeJNw99cX3L0FwMxuAJ4AQsA97r46ONbNwANm9m3gNfn4aw0AAAeRSURBVOAXqa2KiIikJZkEl/F2te0O4I5Oyh8FHu2kfD3hq73STtNMRKS/yoSruUREpI9TMhERkbgpmfTSYze+L90hiIhkDCWTXjp+1OB0hyAikjGUTEREJG5KJj00dmhRukMQEck46Zpn0mct+co5NLa0pjsMEZGMomTSQ0X5IYoIdbpN9zMRkf5K3VwJ5HEvHSki0jcpmYiISNyUTBJI3Vwi0l8pmYiISNyUTEREJG5KJiIiEjclkwT4ytyp6Q5BRCStlEwSoDBPb6OI9G/6FEyAvFD4bczP1dspIv2TZsAnwOWnjWdnTT2fP7fLG0iKiGQ1JZMEKMwLccv849MdhohI2qhfRkRE4qZkIiIicVMyERGRuCmZiIhI3JRMREQkbkomIiISNyUTERGJm5KJiIjEzbyf3mvWzHYDm3r58jJgTwLD6QtU5/5Bdc5+8dZ3gruXH13Yb5NJPMxsubtXpjuOVFKd+wfVOfslq77q5hIRkbgpmYiISNyUTHrn7nQHkAaqc/+gOme/pNRXYyYiIhI3tUxERCRuSiYiIhI3JZMeMrN5ZrbWzKrMbFG64+ktM7vHzHaZ2aqIsmFmtsTM3gl+Dw3Kzcx+FNT5dTM7JeI1Vwb7v2NmV6ajLrEys3Fm9pSZvWlmq83sxqA8a+ttZoVm9rKZrQzq/K2gfJKZLQ3q9lszyw/KC4LnVcH2iRHHuiUoX2tmF6WnRrEzs5CZvWZmjwTPs7rOZrbRzN4wsxVmtjwoS93ftrvrJ8YfIASsAyYD+cBKYHq64+plXc4GTgFWRZR9D1gUPF4E/GvweD7wGGDA6cDSoHwYsD74PTR4PDTddeumzqOAU4LHxcDbwPRsrncQ+6DgcR6wNKjL74CFQflPgX8IHn8e+GnweCHw2+Dx9ODvvQCYFPw/CKW7flHq/lXg/wKPBM+zus7ARqDsqLKU/W2rZdIzs4Eqd1/v7o3AA8CCNMfUK+7+LLDvqOIFwL3B43uBSyPK7/Owl4ASMxsFXAQscfd97r4fWALMS370vePu29391eBxLbAGGEMW1zuI/WDwNC/4ceB84A9B+dF1bnsv/gC838wsKH/A3RvcfQNQRfj/Q0Yys7HAJcDPg+dGlte5Cyn721Yy6ZkxwOaI51uCsmwxwt23B493ACOCx13Vu8++H0FXxsmEv6lndb2D7p4VwC7CHw7rgGp3bw52iYy/vW7B9gNAKX2szsB/ADcBrcHzUrK/zg78xcxeMbPrgrKU/W3n9jZqyW7u7maWldeNm9kg4EHgy+5eE/4SGpaN9Xb3FmCWmZUADwHT0hxSUpnZB4Bd7v6KmZ2b7nhSaI67bzWz4cASM3srcmOy/7bVMumZrcC4iOdjg7JssTNo6hL83hWUd1XvPvd+mFke4URyv7v/MSjO+noDuHs18BRwBuFujbYvk5Hxt9ct2D4E2EvfqvNZwIfMbCPhrujzgf8ku+uMu28Nfu8i/KVhNin821Yy6ZllQEVwVUg+4cG6xWmOKZEWA21Xb1wJPBxR/ungCpDTgQNB0/kJ4EIzGxpcJXJhUJaRgn7wXwBr3P0HEZuytt5mVh60SDCzIuACwmNFTwEfC3Y7us5t78XHgL95eGR2MbAwuPJpElABvJyaWvSMu9/i7mPdfSLh/6N/c/dPksV1NrOBZlbc9pjw3+QqUvm3ne4rEPraD+GrIN4m3O98a7rjiaMevwG2A02E+0WvJdxP/FfgHeBJYFiwrwF3BXV+A6iMOM41hAcmq4Cr012vKHWeQ7hf+XVgRfAzP5vrDZwIvBbUeRXwT0H5ZMIfjFXA74GCoLwweF4VbJ8ccaxbg/diLXBxuusWY/3P5cjVXFlb56BuK4Of1W2fTan829ZyKiIiEjd1c4mISNyUTEREJG5KJiIiEjclExERiZuSiYiIxE3JRCROZnYw+D3RzC5P8LG/ftTzFxN5fJFEUTIRSZyJQI+SScSM7K50SCbufmYPYxJJCSUTkcS5E3hfcD+JrwQLLP6bmS0L7hnxOQAzO9fMnjOzxcCbQdmfggX6Vrct0mdmdwJFwfHuD8raWkEWHHtVcA+LT0Qc+2kz+4OZvWVm91vk4mMiSaKFHkUSZxHwNXf/AECQFA64+6lmVgC8YGZ/CfY9BZjp4aXNAa5x933BkifLzOxBd19kZje4+6xOzvURYBZwElAWvObZYNvJwAxgG/AC4bWqnk98dUWOUMtEJHkuJLz+0QrCS92XEl7fCeDliEQC8CUzWwm8RHihvQq6Nwf4jbu3uPtO4Bng1Ihjb3H3VsJLxkxMSG1EuqGWiUjyGPBFd++wUF6wLPqho57PBc5w9zoze5rwelG91RDxuAX9P5cUUMtEJHFqCd8OuM0TwD8Ey95jZlODFV2PNgTYHySSaYRvo9qmqe31R3kO+EQwLlNO+DbMGbmirfQP+sYikjivAy1Bd9WvCN9DYyLwajAIvpsjt02N9DhwvZmtIbw67UsR2+4GXjezVz28jHqbhwjfl2Ql4ZWQb3L3HUEyEkk5rRosIiJxUzeXiIjETclERETipmQiIiJxUzIREZG4KZmIiEjclExERCRuSiYiIhK3/w+eaPoGMRM4rgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gym\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "###  policyGradient  ###\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "seed_num = 700\n",
        "torch.manual_seed(seed_num)\n",
        "torch.cuda.manual_seed(seed_num)\n",
        "torch.cuda.manual_seed_all(seed_num)\n",
        "np.random.seed(seed_num)\n",
        "random.seed(seed_num)\n",
        "\n",
        "class PolicyNet(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_layer_size=64):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(500, input_size)\n",
        "        # self.fc1 = torch.nn.Linear(input_size, hidden_layer_size)   # MLP\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, hidden_layer_size),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "        # self.fc2 = torch.nn.Linear(hidden_layer_size, output_size)  # MLP\n",
        "\n",
        "        self.final = torch.nn.Linear(hidden_layer_size, output_size)\n",
        "        self.softmax = torch.nn.Softmax(dim=0)                      # 소프트맥스 필요, 출력 값이 각 action을 실행할 '확률'이라서\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x)\n",
        "        # x = torch.from_numpy(x).float()\n",
        "        x= torch.LongTensor([x])\n",
        "        x = self.embedding(x)\n",
        "        x= x.squeeze(0)\n",
        "\n",
        "        return self.softmax(self.final(self.layer1(x)))\n",
        "\n",
        "    def get_action_and_logp(self, x):\n",
        "        action_prob = self.forward(x)  # action_prob : 각 action을 실행할 '확률'값들에 대한 tensor ,  ex) [0.1, 0.5, 0.2, 0.2]\n",
        "        m = torch.distributions.Categorical(action_prob)  # Categorical한 대상에 대해 확률분포를 생성함. ex) action_prob=[0.1, 0.5, 0.2, 0.2]라면, 0번째 action을 뽑을 확률 0.1, 1번째 action을 뽑을 확률 0.5\n",
        "\n",
        "        action = m.sample()                               # 출력값은 tensor(3) or tensor(1), or tensor(0) 등 index로 출력함\n",
        "        # print(m.sample())                                 # 출력값은 tensor(3) or tensor(1), or tensor(0) 등 index로 출력함\n",
        "        logp = m.log_prob(action)                         # log_prob은 확률값을 log로 변환시키는 함수 , log(tensor(0))과 동일 즉, log(0번째 action의 확률)과 동일\n",
        "        # print(logp)                                     # 출력 : tensor(-2.3026)   // np.log(0.1)  = -2.3025850929940455\n",
        "        return action.item(), logp                        # action_index , log 취한 action 선택 확률\n",
        "\n",
        "    def act(self, x):\n",
        "        action, _ = self.get_action_and_logp(x)\n",
        "        return action\n",
        "\n",
        "class ValueNet(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_layer_size=64):\n",
        "        super(ValueNet, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(500, input_size)\n",
        "        self.fc1 = torch.nn.Linear(input_size, hidden_layer_size)\n",
        "        self.fc2 = torch.nn.Linear(hidden_layer_size, 6)\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, hidden_layer_size),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "        self.final = torch.nn.Linear(hidden_layer_size, 6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= torch.LongTensor([x])\n",
        "        x = self.embedding(x)\n",
        "        x= x.squeeze(0)\n",
        "        return self.final(self.layer1(x))\n",
        "        # return self.fc2(torch.nn.functional.relu(self.fc1(x)))\n",
        "\n",
        "def policyGradient(env, max_num_steps=500, gamma=0.98, lr=0.002,\n",
        "                   num_traj=10, num_iter=200):\n",
        "    input_size = env.observation_space.n  ## STATE SPACE (STATE의 개수)\n",
        "    output_size = env.action_space.n             ## ACTION의 개수\n",
        "    Trajectory = namedtuple('Trajectory', 'states actions rewards dones logp')    # 'Trajectory'에 states actions rewards dones logp 튜플을 할당함\n",
        "\n",
        "\n",
        "    def collect_trajectory():  # Trajectory 모으는 함수 (1 episode 돌리는 함수)\n",
        "        state_list = []\n",
        "        action_list = []\n",
        "        reward_list = []\n",
        "        dones_list = []\n",
        "        logp_list = []\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        while not done and steps <= max_num_steps:   # done(에피소드 종료) 될때까지 루프 \n",
        "            action, logp = policy.get_action_and_logp(state)  # policy = PolicyNet(input_size, output_size)\n",
        "            newstate, reward, done, _, _ = env.step(action)    # s_, r, terminated, truncated, _\n",
        "            state_list.append(state)\n",
        "            action_list.append(action)\n",
        "            reward_list.append(reward)\n",
        "            dones_list.append(done)\n",
        "            logp_list.append(logp)\n",
        "            steps += 1\n",
        "            state = newstate\n",
        "\n",
        "        traj = Trajectory(states=state_list, actions=action_list,\n",
        "                          rewards=reward_list, logp=logp_list, dones=dones_list)\n",
        "      # print(traj)  # Trajectory(states=[array([ 9.5363184e-06,  9.3068667e-03,  3.0773466e-02, -4.3019545e-03],\n",
        "      # dtype=float32), array([ 1.9567365e-04, -1.8624260e-01,  3.0687427e-02,  2.9792932e-01],\n",
        "      # dtype=float32), array([-0.00352918,  0.00842878,  0.03664601,  0.01508044], dtype=float32), array([-0.0033606 ,  0.20300654,  0.03694762, -0.26581872], dtype=float32), array([ 0.00069953,  0.3975822 ,  0.03163125, -0.54662293], dtype=float32), array([ 0.00865117,  0.5922458 ,  0.02069879, -0.82917416], dtype=float32), array([ 0.02049609,  0.3968471 ,  0.00411531, -0.5300539 ], dtype=float32), array([ 0.02843303,  0.5919109 , -0.00648577, -0.82143724], dtype=float32), array([ 0.04027125,  0.3968783 , -0.02291452, -0.5308013 ], dtype=float32), array([ 0.04820881,  0.59231496, -0.03353054, -0.8306156 ], dtype=float32), array([ 0.06005511,  0.39766696, -0.05014285, -0.54866385], dtype=float32), array([ 0.06800845,  0.20328394, -0.06111613, -0.27219164], dtype=float32), array([ 0.07207413,  0.0090849 , -0.06655996,  0.00060612], dtype=float32), array([ 0.07225583, -0.18502247, -0.06654784,  0.27156827], dtype=float32), array([ 0.06855538, -0.37913483, -0.06111648,  0.5425417 ], dtype=float32), array([ 0.06097268, -0.18320957, -0.05026564,  0.23124543], dtype=float32), array([ 0.05730849,  0.01259326, -0.04564074, -0.07685973], dtype=float32), array([ 0.05756036, -0.1818457 , -0.04717793,  0.20108126], dtype=float32), array([ 0.05392344, -0.37626228, -0.0431563 ,  0.4785165 ], dtype=float32), array([ 0.0463982 , -0.5707492 , -0.03358598,  0.75729126], dtype=float32), array([ 0.03498321, -0.37518087, -0.01844015,  0.45423177], dtype=float32), array([ 0.02747959, -0.5700373 , -0.00935551,  0.7410456 ], dtype=float32), array([ 0.01607885, -0.37478745,  0.0054654 ,  0.44543317], dtype=float32), array([ 0.0085831 , -0.5699863 ,  0.01437406,  0.7398339 ], dtype=float32), array([-0.00281663, -0.76530373,  0.02917074,  1.0370055 ], dtype=float32), array([-0.0181227 , -0.57058144,  0.04991085,  0.75362134], dtype=float32), array([-0.02953433, -0.7663547 ,  0.06498328,  1.0615833 ], dtype=float32), array([-0.04486142, -0.5721506 ,  0.08621494,  0.78998363], dtype=float32), array([-0.05630443, -0.768344  ,  0.10201462,  1.1084964 ], dtype=float32), array([-0.07167131, -0.5746998 ,  0.12418454,  0.8494806 ], dtype=float32), array([-0.08316531, -0.38147032,  0.14117415,  0.59828496], dtype=float32), array([-0.09079471, -0.18857652,  0.15313986,  0.35319027], dtype=float32), array([-0.09456625,  0.00407392,  0.16020367,  0.11244383], dtype=float32), array([-0.09448477,  0.19658096,  0.16245253, -0.12572043], dtype=float32), array([-0.09055315,  0.38904798,  0.15993813, -0.36306855], dtype=float32), array([-0.08277219,  0.58157825,  0.15267676, -0.60135657], dtype=float32), array([-0.07114062,  0.774272  ,  0.14064963, -0.8423221 ], dtype=float32), array([-0.05565519,  0.5775394 ,  0.12380318, -0.50892246], dtype=float32), array([-0.0441044 ,  0.3809106 ,  0.11362474, -0.17993148], dtype=float32), array([-0.03648619,  0.18436155,  0.11002611,  0.14632481], dtype=float32), array([-0.03279895, -0.01214998,  0.1129526 ,  0.47159216], dtype=float32), array([-0.03304195, -0.20867096,  0.12238444,  0.79763263], dtype=float32), array([-0.03721537, -0.01542167,  0.1383371 ,  0.54581815], dtype=float32), array([-0.03752381,  0.17751318,  0.14925346,  0.29972214], dtype=float32), array([-0.03397354, -0.01938604,  0.15524791,  0.6355052 ], dtype=float32), array([-0.03436126,  0.17326893,  0.167958  ,  0.395458  ], dtype=float32), array([-0.03089589,  0.36565927,  0.17586717,  0.16008124], dtype=float32), array([-0.0235827 ,  0.5578845 ,  0.17906879, -0.07237025], dtype=float32), array([-0.01242501,  0.75004774,  0.1776214 , -0.30364075], dtype=float32), array([0.00257595, 0.55289793, 0.17154858, 0.0393778 ], dtype=float32), array([ 0.0136339 ,  0.74519783,  0.17233613, -0.19464979], dtype=float32), array([0.02853786, 0.5480834 , 0.16844313, 0.14705835], dtype=float32), array([ 0.03949953,  0.74044305,  0.1713843 , -0.08810893], dtype=float32), array([0.05430839, 0.54333186, 0.16962212, 0.25336695], dtype=float32), array([0.06517503, 0.73567706, 0.17468946, 0.01861984], dtype=float32), array([ 0.07988857,  0.9279195 ,  0.17506185, -0.21425721], dtype=float32), array([0.09844696, 0.7307833 , 0.17077671, 0.12813324], dtype=float32), array([0.11306263, 0.53367877, 0.17333938, 0.46945378], dtype=float32), array([0.1237362 , 0.33658645, 0.18272845, 0.8113689 ], dtype=float32), array([0.13046794, 0.13949473, 0.19895583, 1.1555083 ], dtype=float32)], actions=[0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1], rewards=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], dones=[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True], logp=[tensor(-0.7011, grad_fn=<SqueezeBackward1>), tensor(-0.4942, grad_fn=<SqueezeBackward1>), tensor(-0.6693, grad_fn=<SqueezeBackward1>), tensor(-0.8719, grad_fn=<SqueezeBackward1>), tensor(-1.0927, grad_fn=<SqueezeBackward1>), tensor(-0.3000, grad_fn=<SqueezeBackward1>), tensor(-1.0943, grad_fn=<SqueezeBackward1>), tensor(-0.2976, grad_fn=<SqueezeBackward1>), tensor(-1.1101, grad_fn=<SqueezeBackward1>), tensor(-0.2912, grad_fn=<SqueezeBackward1>), tensor(-0.3850, grad_fn=<SqueezeBackward1>), tensor(-0.5034, grad_fn=<SqueezeBackward1>), tensor(-0.6458, grad_fn=<SqueezeBackward1>), tensor(-0.8513, grad_fn=<SqueezeBackward1>), tensor(-0.3972, grad_fn=<SqueezeBackward1>), tensor(-0.5714, grad_fn=<SqueezeBackward1>), tensor(-0.6144, grad_fn=<SqueezeBackward1>), tensor(-0.8118, grad_fn=<SqueezeBackward1>), tensor(-1.0638, grad_fn=<SqueezeBackward1>), tensor(-0.2649, grad_fn=<SqueezeBackward1>), tensor(-1.0635, grad_fn=<SqueezeBackward1>), tensor(-0.2634, grad_fn=<SqueezeBackward1>), tensor(-1.0795, grad_fn=<SqueezeBackward1>), tensor(-1.4881, grad_fn=<SqueezeBackward1>), tensor(-0.1490, grad_fn=<SqueezeBackward1>), tensor(-1.5465, grad_fn=<SqueezeBackward1>), tensor(-0.1374, grad_fn=<SqueezeBackward1>), tensor(-1.6341, grad_fn=<SqueezeBackward1>), tensor(-0.1220, grad_fn=<SqueezeBackward1>), tensor(-0.1897, grad_fn=<SqueezeBackward1>), tensor(-0.2872, grad_fn=<SqueezeBackward1>), tensor(-0.4043, grad_fn=<SqueezeBackward1>), tensor(-0.5435, grad_fn=<SqueezeBackward1>), tensor(-0.7004, grad_fn=<SqueezeBackward1>), tensor(-0.9015, grad_fn=<SqueezeBackward1>), tensor(-1.0939, grad_fn=<SqueezeBackward1>), tensor(-0.3111, grad_fn=<SqueezeBackward1>), tensor(-0.4323, grad_fn=<SqueezeBackward1>), tensor(-0.5986, grad_fn=<SqueezeBackward1>), tensor(-0.8267, grad_fn=<SqueezeBackward1>), tensor(-1.1433, grad_fn=<SqueezeBackward1>), tensor(-0.2386, grad_fn=<SqueezeBackward1>), tensor(-0.3441, grad_fn=<SqueezeBackward1>), tensor(-0.9646, grad_fn=<SqueezeBackward1>), tensor(-0.3061, grad_fn=<SqueezeBackward1>), tensor(-0.4233, grad_fn=<SqueezeBackward1>), tensor(-0.5728, grad_fn=<SqueezeBackward1>), tensor(-0.7365, grad_fn=<SqueezeBackward1>), tensor(-0.5085, grad_fn=<SqueezeBackward1>), tensor(-0.6748, grad_fn=<SqueezeBackward1>), tensor(-0.5544, grad_fn=<SqueezeBackward1>), tensor(-0.6252, grad_fn=<SqueezeBackward1>), tensor(-0.6050, grad_fn=<SqueezeBackward1>), tensor(-0.5652, grad_fn=<SqueezeBackward1>), tensor(-0.7268, grad_fn=<SqueezeBackward1>), tensor(-0.5218, grad_fn=<SqueezeBackward1>), tensor(-0.7160, grad_fn=<SqueezeBackward1>), tensor(-1.0044, grad_fn=<SqueezeBackward1>), tensor(-1.3989, grad_fn=<SqueezeBackward1>), tensor(-0.1645, grad_fn=<SqueezeBackward1>)])\n",
        "\n",
        "        return traj\n",
        "\n",
        "    def calc_returns(rewards):\n",
        "        dis_rewards = [gamma**i * r for i, r in enumerate(rewards)]  ## [ gamma**0 * r, gamma**1 * r, gamma**2 * r, gamma**3 * r, ...]\n",
        "        return [sum(dis_rewards[i:]) for i in range(len(dis_rewards))]  ## [R(0~end), R(1~end), R(2~end), ...]\n",
        "\n",
        "    policy = PolicyNet(input_size, output_size)\n",
        "    policy_optimizer = torch.optim.Adam(policy.parameters(), lr=lr)\n",
        "\n",
        "    value = ValueNet(input_size)\n",
        "    value_optimizer = torch.optim.Adam(value.parameters(), lr=lr)\n",
        "\n",
        "    mean_return_list = []\n",
        "    for it in range(num_iter):\n",
        "        traj_list = [collect_trajectory() for _ in range(num_traj)]    #\n",
        "        returns = [calc_returns(traj.rewards) for traj in traj_list]   # traj.rewards는 traj(Trajectory에서 tuple 요소중 rewards를 뽑아냄)\n",
        "                                                                       # returns는 2차원 : [[Return(0~end), Return(1~end), Return(2~end), ...],  [Return(0~end), Return(1~end), Return(2~end), ...], ... , [Return(0~end), Return(1~end), Return(2~end), ...] ]\n",
        "\n",
        "        # ====================================#\n",
        "        # policy gradient                    #\n",
        "        # ====================================#\n",
        "        # policy_loss_terms = [-1. * traj.logp[j] * (torch.Tensor([returns[i][0]]))                   # returns[i][0] :   Return(0~end)로만 참고함 ([Return(0~end), Return(0~end), Return(0~end), ..., ...])   Expectation of Return이 목적함수(로스 펑션으로 만들기 위해 -1 추가)\n",
        "        #                     for i, traj in enumerate(traj_list) for j in range(len(traj.actions))]  # policy gradient, policyNet에서의 로스 펑션, 값이 작을수록 Loss가 적은 형태로 식을 만들기 위해 앞에 -1 붙임\n",
        "\n",
        "        # ====================================#\n",
        "        # policy gradient with reward-to-go  #\n",
        "        # ====================================#\n",
        "        # policy_loss_terms = [-1. * traj.logp[j] * (torch.Tensor([returns[i][j]]))                   # returns[i][j]] :   Return(j~end)로 참고함 => [Return(0~end), Return(1~end), Return(2~end), ~, Return(end-1~end) , ..., ... , ]    Expectation of Return이 목적함수\n",
        "        #                     for i, traj in enumerate(traj_list) for j in range(len(traj.actions))]  # policy gradient, policyNet에서의 로스 펑션, 값이 작을수록 Loss가 적은 형태로 식을 만들기 위해 앞에 -1 붙임\n",
        "\n",
        "        #====================================#\n",
        "        # policy gradient with base function #\n",
        "        #====================================#\n",
        "        policy_loss_terms = [-1. * traj.logp[j] * (returns[i][j] - value(traj.states[j]))             # returns[i][j] - value(traj.states[j]  :   리턴값에 대해 표준화 적용 (평균값을 빼줌), 평균값은 곧 기대값이고 V(s)를 통해 구함. V(s)는 ValueNet에서 구함.\n",
        "                             for i, traj in enumerate(traj_list) for j in range(len(traj.actions))]   # policy gradient, policyNet에서의 로스 펑션, 값이 작을수록 Loss가 적은 형태로 식을 만들기 위해 앞에 -1 붙임\n",
        "\n",
        "\n",
        "        policy_loss = 1. / num_traj * torch.cat(policy_loss_terms).sum()                              # 각  returns의 원소들을 합산하고 나눔  =>  Expectation of Return 계산\n",
        "        policy_optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        policy_optimizer.step()\n",
        "\n",
        "        value_loss_terms = [1. / len(traj.actions) * (value(traj.states[j]) - returns[i][j])**2.      # ValueNet에서의 로스 펑션, MSE 적용\n",
        "                            for i, traj in enumerate(traj_list) for j in range(len(traj.actions))]\n",
        "        value_loss = 1. / num_traj * torch.cat(value_loss_terms).sum()\n",
        "        value_optimizer.zero_grad()\n",
        "        value_loss.backward()\n",
        "        value_optimizer.step()\n",
        "\n",
        "        mean_return = 1. / num_traj * \\\n",
        "            sum([traj_returns[0] for traj_returns in returns])\n",
        "        mean_return_list.append(mean_return)\n",
        "        pd.DataFrame(mean_return_list).to_csv('/content/drive/MyDrive/강화학습/policy_gradient_model_record1.csv')\n",
        "\n",
        "        if it % 10 == 0:\n",
        "            print('Iteration {}: Mean Return = {}'.format(it, mean_return))\n",
        "            torch.save(policy.state_dict(), '/content/drive/MyDrive/강화학습/policy_gradient_model1_temp.pth')\n",
        "\n",
        "    torch.save(policy.state_dict(), '/content/drive/MyDrive/강화학습/policy_gradient_model1_last.pth')\n",
        "\n",
        "    return policy, mean_return_list\n",
        "\n",
        "env = gym.make('Taxi-v3').unwrapped\n",
        "env._max_episode_steps=500\n",
        "agent, mean_return_list = policyGradient(env, num_iter=5000, max_num_steps=500, gamma=1.0, num_traj=5)\n",
        "\n",
        "\n",
        "plt.plot(mean_return_list)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Mean Return')\n",
        "plt.savefig('pg1_returns.png', format='png', dpi=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PR5hJjqtET3s",
        "outputId": "76766eea-9b73-4dac-cbef-cba205252b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Mean Return = -1795.0\n",
            "Iteration 10: Mean Return = -1924.8000000000002\n",
            "Iteration 20: Mean Return = -1973.4\n",
            "Iteration 30: Mean Return = -1344.4\n",
            "Iteration 40: Mean Return = -1246.4\n",
            "Iteration 50: Mean Return = -1227.6000000000001\n",
            "Iteration 60: Mean Return = -999.2\n",
            "Iteration 70: Mean Return = -959.8000000000001\n",
            "Iteration 80: Mean Return = -693.0\n",
            "Iteration 90: Mean Return = -663.2\n",
            "Iteration 100: Mean Return = -555.0\n",
            "Iteration 110: Mean Return = -502.20000000000005\n",
            "Iteration 120: Mean Return = -911.4000000000001\n",
            "Iteration 130: Mean Return = -498.40000000000003\n",
            "Iteration 140: Mean Return = -640.0\n",
            "Iteration 150: Mean Return = -439.40000000000003\n",
            "Iteration 160: Mean Return = -485.6\n",
            "Iteration 170: Mean Return = -572.4\n",
            "Iteration 180: Mean Return = -655.8000000000001\n",
            "Iteration 190: Mean Return = -478.8\n",
            "Iteration 200: Mean Return = -447.40000000000003\n",
            "Iteration 210: Mean Return = -358.6\n",
            "Iteration 220: Mean Return = -412.20000000000005\n",
            "Iteration 230: Mean Return = -324.20000000000005\n",
            "Iteration 240: Mean Return = -363.0\n",
            "Iteration 250: Mean Return = -564.0\n",
            "Iteration 260: Mean Return = -274.8\n",
            "Iteration 270: Mean Return = -362.0\n",
            "Iteration 280: Mean Return = -236.60000000000002\n",
            "Iteration 290: Mean Return = -534.2\n",
            "Iteration 300: Mean Return = -449.40000000000003\n",
            "Iteration 310: Mean Return = -262.0\n",
            "Iteration 320: Mean Return = -337.8\n",
            "Iteration 330: Mean Return = -398.6\n",
            "Iteration 340: Mean Return = -467.6\n",
            "Iteration 350: Mean Return = -441.20000000000005\n",
            "Iteration 360: Mean Return = -187.8\n",
            "Iteration 370: Mean Return = -466.40000000000003\n",
            "Iteration 380: Mean Return = -170.8\n",
            "Iteration 390: Mean Return = -155.8\n",
            "Iteration 400: Mean Return = -264.6\n",
            "Iteration 410: Mean Return = -359.8\n",
            "Iteration 420: Mean Return = -252.60000000000002\n",
            "Iteration 430: Mean Return = -159.4\n",
            "Iteration 440: Mean Return = -160.8\n",
            "Iteration 450: Mean Return = -267.40000000000003\n",
            "Iteration 460: Mean Return = -322.8\n",
            "Iteration 470: Mean Return = -267.40000000000003\n",
            "Iteration 480: Mean Return = -186.8\n",
            "Iteration 490: Mean Return = -296.40000000000003\n",
            "Iteration 500: Mean Return = -341.40000000000003\n",
            "Iteration 510: Mean Return = -158.20000000000002\n",
            "Iteration 520: Mean Return = -233.8\n",
            "Iteration 530: Mean Return = -118.60000000000001\n",
            "Iteration 540: Mean Return = -240.20000000000002\n",
            "Iteration 550: Mean Return = -220.60000000000002\n",
            "Iteration 560: Mean Return = -217.20000000000002\n",
            "Iteration 570: Mean Return = -228.4\n",
            "Iteration 580: Mean Return = -214.0\n",
            "Iteration 590: Mean Return = -211.4\n",
            "Iteration 600: Mean Return = -136.6\n",
            "Iteration 610: Mean Return = -318.0\n",
            "Iteration 620: Mean Return = -335.20000000000005\n",
            "Iteration 630: Mean Return = -208.8\n",
            "Iteration 640: Mean Return = -150.8\n",
            "Iteration 650: Mean Return = -185.8\n",
            "Iteration 660: Mean Return = -29.8\n",
            "Iteration 670: Mean Return = -320.8\n",
            "Iteration 680: Mean Return = -39.6\n",
            "Iteration 690: Mean Return = -246.8\n",
            "Iteration 700: Mean Return = -193.4\n",
            "Iteration 710: Mean Return = -134.20000000000002\n",
            "Iteration 720: Mean Return = -413.40000000000003\n",
            "Iteration 730: Mean Return = -135.20000000000002\n",
            "Iteration 740: Mean Return = -406.40000000000003\n",
            "Iteration 750: Mean Return = -16.2\n",
            "Iteration 760: Mean Return = -325.20000000000005\n",
            "Iteration 770: Mean Return = -424.6\n",
            "Iteration 780: Mean Return = -131.6\n",
            "Iteration 790: Mean Return = -149.4\n",
            "Iteration 800: Mean Return = -108.60000000000001\n",
            "Iteration 810: Mean Return = -320.0\n",
            "Iteration 820: Mean Return = -100.0\n",
            "Iteration 830: Mean Return = -184.8\n",
            "Iteration 840: Mean Return = -44.800000000000004\n",
            "Iteration 850: Mean Return = -314.8\n",
            "Iteration 860: Mean Return = -30.200000000000003\n",
            "Iteration 870: Mean Return = -154.60000000000002\n",
            "Iteration 880: Mean Return = -214.60000000000002\n",
            "Iteration 890: Mean Return = -135.6\n",
            "Iteration 900: Mean Return = -214.4\n",
            "Iteration 910: Mean Return = -210.4\n",
            "Iteration 920: Mean Return = -310.20000000000005\n",
            "Iteration 930: Mean Return = -217.8\n",
            "Iteration 940: Mean Return = -130.0\n",
            "Iteration 950: Mean Return = -27.200000000000003\n",
            "Iteration 960: Mean Return = -210.60000000000002\n",
            "Iteration 970: Mean Return = -107.60000000000001\n",
            "Iteration 980: Mean Return = -228.8\n",
            "Iteration 990: Mean Return = -402.6\n",
            "Iteration 1000: Mean Return = -21.0\n",
            "Iteration 1010: Mean Return = -110.80000000000001\n",
            "Iteration 1020: Mean Return = -107.0\n",
            "Iteration 1030: Mean Return = -150.20000000000002\n",
            "Iteration 1040: Mean Return = -305.8\n",
            "Iteration 1050: Mean Return = -125.4\n",
            "Iteration 1060: Mean Return = -106.60000000000001\n",
            "Iteration 1070: Mean Return = -315.40000000000003\n",
            "Iteration 1080: Mean Return = -303.2\n",
            "Iteration 1090: Mean Return = -23.6\n",
            "Iteration 1100: Mean Return = -301.2\n",
            "Iteration 1110: Mean Return = -321.0\n",
            "Iteration 1120: Mean Return = -219.60000000000002\n",
            "Iteration 1130: Mean Return = -213.0\n",
            "Iteration 1140: Mean Return = -120.80000000000001\n",
            "Iteration 1150: Mean Return = -201.60000000000002\n",
            "Iteration 1160: Mean Return = -210.8\n",
            "Iteration 1170: Mean Return = -112.2\n",
            "Iteration 1180: Mean Return = -410.0\n",
            "Iteration 1190: Mean Return = -212.0\n",
            "Iteration 1200: Mean Return = -104.2\n",
            "Iteration 1210: Mean Return = -213.0\n",
            "Iteration 1220: Mean Return = -12.200000000000001\n",
            "Iteration 1230: Mean Return = -109.80000000000001\n",
            "Iteration 1240: Mean Return = -312.8\n",
            "Iteration 1250: Mean Return = -8.8\n",
            "Iteration 1260: Mean Return = -310.40000000000003\n",
            "Iteration 1270: Mean Return = -94.4\n",
            "Iteration 1280: Mean Return = -105.80000000000001\n",
            "Iteration 1290: Mean Return = -6.4\n",
            "Iteration 1300: Mean Return = -1.8\n",
            "Iteration 1310: Mean Return = -111.60000000000001\n",
            "Iteration 1320: Mean Return = -203.60000000000002\n",
            "Iteration 1330: Mean Return = -111.0\n",
            "Iteration 1340: Mean Return = -227.8\n",
            "Iteration 1350: Mean Return = -300.8\n",
            "Iteration 1360: Mean Return = -105.80000000000001\n",
            "Iteration 1370: Mean Return = -101.2\n",
            "Iteration 1380: Mean Return = -151.20000000000002\n",
            "Iteration 1390: Mean Return = -199.20000000000002\n",
            "Iteration 1400: Mean Return = -112.4\n",
            "Iteration 1410: Mean Return = -115.4\n",
            "Iteration 1420: Mean Return = 0.6000000000000001\n",
            "Iteration 1430: Mean Return = -204.0\n",
            "Iteration 1440: Mean Return = -6.2\n",
            "Iteration 1450: Mean Return = -95.80000000000001\n",
            "Iteration 1460: Mean Return = 0.4\n",
            "Iteration 1470: Mean Return = -202.8\n",
            "Iteration 1480: Mean Return = -298.6\n",
            "Iteration 1490: Mean Return = 2.2\n",
            "Iteration 1500: Mean Return = -97.60000000000001\n",
            "Iteration 1510: Mean Return = -96.60000000000001\n",
            "Iteration 1520: Mean Return = -26.6\n",
            "Iteration 1530: Mean Return = -93.0\n",
            "Iteration 1540: Mean Return = -303.0\n",
            "Iteration 1550: Mean Return = -203.4\n",
            "Iteration 1560: Mean Return = -6.0\n",
            "Iteration 1570: Mean Return = -145.0\n",
            "Iteration 1580: Mean Return = -299.8\n",
            "Iteration 1590: Mean Return = 3.6\n",
            "Iteration 1600: Mean Return = -196.60000000000002\n",
            "Iteration 1610: Mean Return = -99.60000000000001\n",
            "Iteration 1620: Mean Return = -101.4\n",
            "Iteration 1630: Mean Return = -303.40000000000003\n",
            "Iteration 1640: Mean Return = -199.0\n",
            "Iteration 1650: Mean Return = -101.80000000000001\n",
            "Iteration 1660: Mean Return = -201.20000000000002\n",
            "Iteration 1670: Mean Return = -202.0\n",
            "Iteration 1680: Mean Return = -103.80000000000001\n",
            "Iteration 1690: Mean Return = -407.0\n",
            "Iteration 1700: Mean Return = -279.0\n",
            "Iteration 1710: Mean Return = -204.20000000000002\n",
            "Iteration 1720: Mean Return = -131.0\n",
            "Iteration 1730: Mean Return = -131.0\n",
            "Iteration 1740: Mean Return = -305.6\n",
            "Iteration 1750: Mean Return = -103.0\n",
            "Iteration 1760: Mean Return = 1.0\n",
            "Iteration 1770: Mean Return = -8.6\n",
            "Iteration 1780: Mean Return = -206.4\n",
            "Iteration 1790: Mean Return = -201.60000000000002\n",
            "Iteration 1800: Mean Return = -200.8\n",
            "Iteration 1810: Mean Return = -203.8\n",
            "Iteration 1820: Mean Return = -211.20000000000002\n",
            "Iteration 1830: Mean Return = -101.80000000000001\n",
            "Iteration 1840: Mean Return = -213.4\n",
            "Iteration 1850: Mean Return = -47.0\n",
            "Iteration 1860: Mean Return = -113.0\n",
            "Iteration 1870: Mean Return = -93.4\n",
            "Iteration 1880: Mean Return = -398.8\n",
            "Iteration 1890: Mean Return = -227.4\n",
            "Iteration 1900: Mean Return = -203.8\n",
            "Iteration 1910: Mean Return = -200.4\n",
            "Iteration 1920: Mean Return = -208.60000000000002\n",
            "Iteration 1930: Mean Return = -102.60000000000001\n",
            "Iteration 1940: Mean Return = 7.6000000000000005\n",
            "Iteration 1950: Mean Return = 5.800000000000001\n",
            "Iteration 1960: Mean Return = 4.800000000000001\n",
            "Iteration 1970: Mean Return = -95.4\n",
            "Iteration 1980: Mean Return = -223.60000000000002\n",
            "Iteration 1990: Mean Return = -298.8\n",
            "Iteration 2000: Mean Return = -299.40000000000003\n",
            "Iteration 2010: Mean Return = -99.80000000000001\n",
            "Iteration 2020: Mean Return = -225.0\n",
            "Iteration 2030: Mean Return = -92.60000000000001\n",
            "Iteration 2040: Mean Return = 6.800000000000001\n",
            "Iteration 2050: Mean Return = -202.20000000000002\n",
            "Iteration 2060: Mean Return = -96.4\n",
            "Iteration 2070: Mean Return = -95.4\n",
            "Iteration 2080: Mean Return = -100.2\n",
            "Iteration 2090: Mean Return = -199.4\n",
            "Iteration 2100: Mean Return = -199.0\n",
            "Iteration 2110: Mean Return = -98.4\n",
            "Iteration 2120: Mean Return = -91.80000000000001\n",
            "Iteration 2130: Mean Return = -99.60000000000001\n",
            "Iteration 2140: Mean Return = -299.40000000000003\n",
            "Iteration 2150: Mean Return = 6.6000000000000005\n",
            "Iteration 2160: Mean Return = 4.2\n",
            "Iteration 2170: Mean Return = -94.80000000000001\n",
            "Iteration 2180: Mean Return = -97.4\n",
            "Iteration 2190: Mean Return = -193.8\n",
            "Iteration 2200: Mean Return = -199.0\n",
            "Iteration 2210: Mean Return = 7.4\n",
            "Iteration 2220: Mean Return = -96.0\n",
            "Iteration 2230: Mean Return = -199.20000000000002\n",
            "Iteration 2240: Mean Return = -301.6\n",
            "Iteration 2250: Mean Return = -96.4\n",
            "Iteration 2260: Mean Return = 1.8\n",
            "Iteration 2270: Mean Return = 2.0\n",
            "Iteration 2280: Mean Return = -206.20000000000002\n",
            "Iteration 2290: Mean Return = -204.60000000000002\n",
            "Iteration 2300: Mean Return = -201.8\n",
            "Iteration 2310: Mean Return = 5.800000000000001\n",
            "Iteration 2320: Mean Return = -196.4\n",
            "Iteration 2330: Mean Return = -197.20000000000002\n",
            "Iteration 2340: Mean Return = -95.4\n",
            "Iteration 2350: Mean Return = 3.4000000000000004\n",
            "Iteration 2360: Mean Return = -196.8\n",
            "Iteration 2370: Mean Return = -298.6\n",
            "Iteration 2380: Mean Return = -100.60000000000001\n",
            "Iteration 2390: Mean Return = -194.8\n",
            "Iteration 2400: Mean Return = -194.4\n",
            "Iteration 2410: Mean Return = -197.0\n",
            "Iteration 2420: Mean Return = -93.60000000000001\n",
            "Iteration 2430: Mean Return = -197.60000000000002\n",
            "Iteration 2440: Mean Return = -196.0\n",
            "Iteration 2450: Mean Return = -94.2\n",
            "Iteration 2460: Mean Return = -296.6\n",
            "Iteration 2470: Mean Return = -398.20000000000005\n",
            "Iteration 2480: Mean Return = 5.0\n",
            "Iteration 2490: Mean Return = 6.2\n",
            "Iteration 2500: Mean Return = 6.2\n",
            "Iteration 2510: Mean Return = -297.6\n",
            "Iteration 2520: Mean Return = 6.800000000000001\n",
            "Iteration 2530: Mean Return = 6.4\n",
            "Iteration 2540: Mean Return = -0.6000000000000001\n",
            "Iteration 2550: Mean Return = 4.6000000000000005\n",
            "Iteration 2560: Mean Return = -98.0\n",
            "Iteration 2570: Mean Return = -410.6\n",
            "Iteration 2580: Mean Return = -301.6\n",
            "Iteration 2590: Mean Return = -194.60000000000002\n",
            "Iteration 2600: Mean Return = -98.80000000000001\n",
            "Iteration 2610: Mean Return = 3.6\n",
            "Iteration 2620: Mean Return = -95.4\n",
            "Iteration 2630: Mean Return = -99.60000000000001\n",
            "Iteration 2640: Mean Return = -199.8\n",
            "Iteration 2650: Mean Return = 5.2\n",
            "Iteration 2660: Mean Return = -100.4\n",
            "Iteration 2670: Mean Return = -96.80000000000001\n",
            "Iteration 2680: Mean Return = -197.20000000000002\n",
            "Iteration 2690: Mean Return = 3.4000000000000004\n",
            "Iteration 2700: Mean Return = -93.80000000000001\n",
            "Iteration 2710: Mean Return = -196.8\n",
            "Iteration 2720: Mean Return = -101.4\n",
            "Iteration 2730: Mean Return = -197.20000000000002\n",
            "Iteration 2740: Mean Return = -94.0\n",
            "Iteration 2750: Mean Return = -95.2\n",
            "Iteration 2760: Mean Return = -94.0\n",
            "Iteration 2770: Mean Return = -96.80000000000001\n",
            "Iteration 2780: Mean Return = 9.200000000000001\n",
            "Iteration 2790: Mean Return = -198.20000000000002\n",
            "Iteration 2800: Mean Return = -200.20000000000002\n",
            "Iteration 2810: Mean Return = -198.8\n",
            "Iteration 2820: Mean Return = -205.8\n",
            "Iteration 2830: Mean Return = -170.8\n",
            "Iteration 2840: Mean Return = -107.4\n",
            "Iteration 2850: Mean Return = -297.2\n",
            "Iteration 2860: Mean Return = -198.0\n",
            "Iteration 2870: Mean Return = -97.80000000000001\n",
            "Iteration 2880: Mean Return = -195.4\n",
            "Iteration 2890: Mean Return = 7.800000000000001\n",
            "Iteration 2900: Mean Return = 5.800000000000001\n",
            "Iteration 2910: Mean Return = -195.60000000000002\n",
            "Iteration 2920: Mean Return = -196.4\n",
            "Iteration 2930: Mean Return = -299.6\n",
            "Iteration 2940: Mean Return = -297.0\n",
            "Iteration 2950: Mean Return = 6.6000000000000005\n",
            "Iteration 2960: Mean Return = -209.4\n",
            "Iteration 2970: Mean Return = 4.800000000000001\n",
            "Iteration 2980: Mean Return = -197.4\n",
            "Iteration 2990: Mean Return = -197.20000000000002\n",
            "Iteration 3000: Mean Return = 0.6000000000000001\n",
            "Iteration 3010: Mean Return = -197.20000000000002\n",
            "Iteration 3020: Mean Return = -95.0\n",
            "Iteration 3030: Mean Return = 9.0\n",
            "Iteration 3040: Mean Return = -197.4\n",
            "Iteration 3050: Mean Return = -95.60000000000001\n",
            "Iteration 3060: Mean Return = -96.4\n",
            "Iteration 3070: Mean Return = -95.80000000000001\n",
            "Iteration 3080: Mean Return = -195.8\n",
            "Iteration 3090: Mean Return = -198.0\n",
            "Iteration 3100: Mean Return = -297.6\n",
            "Iteration 3110: Mean Return = -93.0\n",
            "Iteration 3120: Mean Return = -198.4\n",
            "Iteration 3130: Mean Return = -297.8\n",
            "Iteration 3140: Mean Return = 6.2\n",
            "Iteration 3150: Mean Return = 6.800000000000001\n",
            "Iteration 3160: Mean Return = 5.2\n",
            "Iteration 3170: Mean Return = -93.80000000000001\n",
            "Iteration 3180: Mean Return = -96.4\n",
            "Iteration 3190: Mean Return = 3.8000000000000003\n",
            "Iteration 3200: Mean Return = -95.2\n",
            "Iteration 3210: Mean Return = 6.0\n",
            "Iteration 3220: Mean Return = -196.0\n",
            "Iteration 3230: Mean Return = 7.800000000000001\n",
            "Iteration 3240: Mean Return = -195.20000000000002\n",
            "Iteration 3250: Mean Return = -92.80000000000001\n",
            "Iteration 3260: Mean Return = -195.8\n",
            "Iteration 3270: Mean Return = 5.2\n",
            "Iteration 3280: Mean Return = -97.0\n",
            "Iteration 3290: Mean Return = -93.80000000000001\n",
            "Iteration 3300: Mean Return = -301.2\n",
            "Iteration 3310: Mean Return = 8.6\n",
            "Iteration 3320: Mean Return = -94.2\n",
            "Iteration 3330: Mean Return = -196.4\n",
            "Iteration 3340: Mean Return = -299.8\n",
            "Iteration 3350: Mean Return = 4.0\n",
            "Iteration 3360: Mean Return = 6.800000000000001\n",
            "Iteration 3370: Mean Return = 7.800000000000001\n",
            "Iteration 3380: Mean Return = 7.0\n",
            "Iteration 3390: Mean Return = 5.800000000000001\n",
            "Iteration 3400: Mean Return = 7.6000000000000005\n",
            "Iteration 3410: Mean Return = -193.60000000000002\n",
            "Iteration 3420: Mean Return = -93.60000000000001\n",
            "Iteration 3430: Mean Return = -95.0\n",
            "Iteration 3440: Mean Return = -297.8\n",
            "Iteration 3450: Mean Return = -194.20000000000002\n",
            "Iteration 3460: Mean Return = -96.2\n",
            "Iteration 3470: Mean Return = -95.4\n",
            "Iteration 3480: Mean Return = -96.80000000000001\n",
            "Iteration 3490: Mean Return = 5.2\n",
            "Iteration 3500: Mean Return = -195.20000000000002\n",
            "Iteration 3510: Mean Return = -97.80000000000001\n",
            "Iteration 3520: Mean Return = -198.0\n",
            "Iteration 3530: Mean Return = -195.4\n",
            "Iteration 3540: Mean Return = -96.2\n",
            "Iteration 3550: Mean Return = 4.4\n",
            "Iteration 3560: Mean Return = -95.2\n",
            "Iteration 3570: Mean Return = -95.60000000000001\n",
            "Iteration 3580: Mean Return = 7.6000000000000005\n",
            "Iteration 3590: Mean Return = -198.20000000000002\n",
            "Iteration 3600: Mean Return = -94.80000000000001\n",
            "Iteration 3610: Mean Return = -403.6\n",
            "Iteration 3620: Mean Return = 7.800000000000001\n",
            "Iteration 3630: Mean Return = 7.0\n",
            "Iteration 3640: Mean Return = -93.80000000000001\n",
            "Iteration 3650: Mean Return = -194.20000000000002\n",
            "Iteration 3660: Mean Return = 7.800000000000001\n",
            "Iteration 3670: Mean Return = -93.2\n",
            "Iteration 3680: Mean Return = 2.6\n",
            "Iteration 3690: Mean Return = -96.4\n",
            "Iteration 3700: Mean Return = -94.60000000000001\n",
            "Iteration 3710: Mean Return = -94.60000000000001\n",
            "Iteration 3720: Mean Return = 4.4\n",
            "Iteration 3730: Mean Return = -298.0\n",
            "Iteration 3740: Mean Return = -101.4\n",
            "Iteration 3750: Mean Return = 8.200000000000001\n",
            "Iteration 3760: Mean Return = -196.0\n",
            "Iteration 3770: Mean Return = 8.0\n",
            "Iteration 3780: Mean Return = -100.0\n",
            "Iteration 3790: Mean Return = -196.60000000000002\n",
            "Iteration 3800: Mean Return = 5.800000000000001\n",
            "Iteration 3810: Mean Return = 2.4000000000000004\n",
            "Iteration 3820: Mean Return = -197.20000000000002\n",
            "Iteration 3830: Mean Return = -95.60000000000001\n",
            "Iteration 3840: Mean Return = 3.6\n",
            "Iteration 3850: Mean Return = 8.200000000000001\n",
            "Iteration 3860: Mean Return = -198.4\n",
            "Iteration 3870: Mean Return = 2.6\n",
            "Iteration 3880: Mean Return = -96.80000000000001\n",
            "Iteration 3890: Mean Return = 3.8000000000000003\n",
            "Iteration 3900: Mean Return = -94.4\n",
            "Iteration 3910: Mean Return = -200.4\n",
            "Iteration 3920: Mean Return = -195.4\n",
            "Iteration 3930: Mean Return = -93.2\n",
            "Iteration 3940: Mean Return = -215.8\n",
            "Iteration 3950: Mean Return = -96.0\n",
            "Iteration 3960: Mean Return = -94.0\n",
            "Iteration 3970: Mean Return = -95.2\n",
            "Iteration 3980: Mean Return = 4.800000000000001\n",
            "Iteration 3990: Mean Return = -95.4\n",
            "Iteration 4000: Mean Return = -94.2\n",
            "Iteration 4010: Mean Return = -93.60000000000001\n",
            "Iteration 4020: Mean Return = -93.4\n",
            "Iteration 4030: Mean Return = -92.80000000000001\n",
            "Iteration 4040: Mean Return = 8.6\n",
            "Iteration 4050: Mean Return = 7.800000000000001\n",
            "Iteration 4060: Mean Return = -94.80000000000001\n",
            "Iteration 4070: Mean Return = -93.80000000000001\n",
            "Iteration 4080: Mean Return = -94.2\n",
            "Iteration 4090: Mean Return = -97.0\n",
            "Iteration 4100: Mean Return = 9.600000000000001\n",
            "Iteration 4110: Mean Return = 7.800000000000001\n",
            "Iteration 4120: Mean Return = -92.0\n",
            "Iteration 4130: Mean Return = -196.4\n",
            "Iteration 4140: Mean Return = -93.4\n",
            "Iteration 4150: Mean Return = -93.4\n",
            "Iteration 4160: Mean Return = -93.2\n",
            "Iteration 4170: Mean Return = -93.80000000000001\n",
            "Iteration 4180: Mean Return = 6.0\n",
            "Iteration 4190: Mean Return = -195.20000000000002\n",
            "Iteration 4200: Mean Return = 8.0\n",
            "Iteration 4210: Mean Return = -92.0\n",
            "Iteration 4220: Mean Return = -196.0\n",
            "Iteration 4230: Mean Return = -194.60000000000002\n",
            "Iteration 4240: Mean Return = -195.8\n",
            "Iteration 4250: Mean Return = -92.0\n",
            "Iteration 4260: Mean Return = -92.0\n",
            "Iteration 4270: Mean Return = 6.6000000000000005\n",
            "Iteration 4280: Mean Return = -93.2\n",
            "Iteration 4290: Mean Return = -92.4\n",
            "Iteration 4300: Mean Return = -91.80000000000001\n",
            "Iteration 4310: Mean Return = -94.4\n",
            "Iteration 4320: Mean Return = -94.2\n",
            "Iteration 4330: Mean Return = -93.4\n",
            "Iteration 4340: Mean Return = -193.60000000000002\n",
            "Iteration 4350: Mean Return = -93.4\n",
            "Iteration 4360: Mean Return = -95.0\n",
            "Iteration 4370: Mean Return = -94.0\n",
            "Iteration 4380: Mean Return = 4.6000000000000005\n",
            "Iteration 4390: Mean Return = -298.0\n",
            "Iteration 4400: Mean Return = -195.60000000000002\n",
            "Iteration 4410: Mean Return = -94.4\n",
            "Iteration 4420: Mean Return = -196.4\n",
            "Iteration 4430: Mean Return = 6.800000000000001\n",
            "Iteration 4440: Mean Return = -195.20000000000002\n",
            "Iteration 4450: Mean Return = -93.4\n",
            "Iteration 4460: Mean Return = -315.40000000000003\n",
            "Iteration 4470: Mean Return = -195.0\n",
            "Iteration 4480: Mean Return = -196.8\n",
            "Iteration 4490: Mean Return = 7.2\n",
            "Iteration 4500: Mean Return = 8.8\n",
            "Iteration 4510: Mean Return = -92.4\n",
            "Iteration 4520: Mean Return = -101.0\n",
            "Iteration 4530: Mean Return = -197.0\n",
            "Iteration 4540: Mean Return = 7.2\n",
            "Iteration 4550: Mean Return = 7.4\n",
            "Iteration 4560: Mean Return = 8.4\n",
            "Iteration 4570: Mean Return = -297.6\n",
            "Iteration 4580: Mean Return = -93.4\n",
            "Iteration 4590: Mean Return = -93.60000000000001\n",
            "Iteration 4600: Mean Return = 8.200000000000001\n",
            "Iteration 4610: Mean Return = -300.2\n",
            "Iteration 4620: Mean Return = 7.2\n",
            "Iteration 4630: Mean Return = -94.80000000000001\n",
            "Iteration 4640: Mean Return = -197.4\n",
            "Iteration 4650: Mean Return = -93.60000000000001\n",
            "Iteration 4660: Mean Return = -198.20000000000002\n",
            "Iteration 4670: Mean Return = -194.8\n",
            "Iteration 4680: Mean Return = -91.4\n",
            "Iteration 4690: Mean Return = -92.0\n",
            "Iteration 4700: Mean Return = -93.2\n",
            "Iteration 4710: Mean Return = -95.4\n",
            "Iteration 4720: Mean Return = -95.4\n",
            "Iteration 4730: Mean Return = -195.60000000000002\n",
            "Iteration 4740: Mean Return = -92.80000000000001\n",
            "Iteration 4750: Mean Return = -94.0\n",
            "Iteration 4760: Mean Return = 6.6000000000000005\n",
            "Iteration 4770: Mean Return = -92.2\n",
            "Iteration 4780: Mean Return = -95.60000000000001\n",
            "Iteration 4790: Mean Return = -94.60000000000001\n",
            "Iteration 4800: Mean Return = 9.600000000000001\n",
            "Iteration 4810: Mean Return = 7.6000000000000005\n",
            "Iteration 4820: Mean Return = -92.60000000000001\n",
            "Iteration 4830: Mean Return = -96.80000000000001\n",
            "Iteration 4840: Mean Return = -196.60000000000002\n",
            "Iteration 4850: Mean Return = -94.4\n",
            "Iteration 4860: Mean Return = -95.0\n",
            "Iteration 4870: Mean Return = 5.6000000000000005\n",
            "Iteration 4880: Mean Return = 8.200000000000001\n",
            "Iteration 4890: Mean Return = -95.4\n",
            "Iteration 4900: Mean Return = -195.8\n",
            "Iteration 4910: Mean Return = -94.4\n",
            "Iteration 4920: Mean Return = -501.0\n",
            "Iteration 4930: Mean Return = -195.4\n",
            "Iteration 4940: Mean Return = -91.80000000000001\n",
            "Iteration 4950: Mean Return = -298.0\n",
            "Iteration 4960: Mean Return = -194.20000000000002\n",
            "Iteration 4970: Mean Return = -93.60000000000001\n",
            "Iteration 4980: Mean Return = -92.60000000000001\n",
            "Iteration 4990: Mean Return = -298.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pcdQuJ1Q8n7S",
        "outputId": "449b86e1-b541-421d-e7ca-07bd94fcb92b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Mean Return = -1674.6000000000001\n",
            "Iteration 10: Mean Return = -2054.4\n",
            "Iteration 20: Mean Return = -2098.4\n",
            "Iteration 30: Mean Return = -1976.4\n",
            "Iteration 40: Mean Return = -1961.8000000000002\n",
            "Iteration 50: Mean Return = -2236.2000000000003\n",
            "Iteration 60: Mean Return = -1926.6000000000001\n",
            "Iteration 70: Mean Return = -1564.2\n",
            "Iteration 80: Mean Return = -1708.0\n",
            "Iteration 90: Mean Return = -1324.0\n",
            "Iteration 100: Mean Return = -1607.6000000000001\n",
            "Iteration 110: Mean Return = -1351.8000000000002\n",
            "Iteration 120: Mean Return = -1043.8\n",
            "Iteration 130: Mean Return = -1257.6000000000001\n",
            "Iteration 140: Mean Return = -1198.4\n",
            "Iteration 150: Mean Return = -948.0\n",
            "Iteration 160: Mean Return = -745.8000000000001\n",
            "Iteration 170: Mean Return = -981.8000000000001\n",
            "Iteration 180: Mean Return = -302.2\n",
            "Iteration 190: Mean Return = -525.6\n",
            "Iteration 200: Mean Return = -642.6\n",
            "Iteration 210: Mean Return = -503.8\n",
            "Iteration 220: Mean Return = -579.8000000000001\n",
            "Iteration 230: Mean Return = -346.0\n",
            "Iteration 240: Mean Return = -367.20000000000005\n",
            "Iteration 250: Mean Return = -458.6\n",
            "Iteration 260: Mean Return = -153.0\n",
            "Iteration 270: Mean Return = -343.6\n",
            "Iteration 280: Mean Return = -312.0\n",
            "Iteration 290: Mean Return = -346.0\n",
            "Iteration 300: Mean Return = -285.6\n",
            "Iteration 310: Mean Return = -323.40000000000003\n",
            "Iteration 320: Mean Return = -234.0\n",
            "Iteration 330: Mean Return = -139.20000000000002\n",
            "Iteration 340: Mean Return = -202.8\n",
            "Iteration 350: Mean Return = -130.4\n",
            "Iteration 360: Mean Return = -103.0\n",
            "Iteration 370: Mean Return = -150.0\n",
            "Iteration 380: Mean Return = -112.0\n",
            "Iteration 390: Mean Return = -146.6\n",
            "Iteration 400: Mean Return = -158.0\n",
            "Iteration 410: Mean Return = -183.60000000000002\n",
            "Iteration 420: Mean Return = -118.60000000000001\n",
            "Iteration 430: Mean Return = -215.8\n",
            "Iteration 440: Mean Return = -178.20000000000002\n",
            "Iteration 450: Mean Return = -107.4\n",
            "Iteration 460: Mean Return = -88.4\n",
            "Iteration 470: Mean Return = -59.0\n",
            "Iteration 480: Mean Return = -52.800000000000004\n",
            "Iteration 490: Mean Return = -52.800000000000004\n",
            "Iteration 500: Mean Return = -36.0\n",
            "Iteration 510: Mean Return = -54.400000000000006\n",
            "Iteration 520: Mean Return = -82.0\n",
            "Iteration 530: Mean Return = -83.0\n",
            "Iteration 540: Mean Return = -54.0\n",
            "Iteration 550: Mean Return = -87.2\n",
            "Iteration 560: Mean Return = -55.0\n",
            "Iteration 570: Mean Return = -50.400000000000006\n",
            "Iteration 580: Mean Return = -29.0\n",
            "Iteration 590: Mean Return = -16.0\n",
            "Iteration 600: Mean Return = -46.2\n",
            "Iteration 610: Mean Return = -35.0\n",
            "Iteration 620: Mean Return = -97.2\n",
            "Iteration 630: Mean Return = -120.2\n",
            "Iteration 640: Mean Return = -77.4\n",
            "Iteration 650: Mean Return = -66.2\n",
            "Iteration 660: Mean Return = -71.4\n",
            "Iteration 670: Mean Return = -21.6\n",
            "Iteration 680: Mean Return = -27.8\n",
            "Iteration 690: Mean Return = -40.0\n",
            "Iteration 700: Mean Return = -54.6\n",
            "Iteration 710: Mean Return = -32.800000000000004\n",
            "Iteration 720: Mean Return = -67.4\n",
            "Iteration 730: Mean Return = -29.6\n",
            "Iteration 740: Mean Return = -30.8\n",
            "Iteration 750: Mean Return = -40.6\n",
            "Iteration 760: Mean Return = -46.400000000000006\n",
            "Iteration 770: Mean Return = -34.6\n",
            "Iteration 780: Mean Return = -20.8\n",
            "Iteration 790: Mean Return = -53.800000000000004\n",
            "Iteration 800: Mean Return = -32.6\n",
            "Iteration 810: Mean Return = -29.6\n",
            "Iteration 820: Mean Return = -19.200000000000003\n",
            "Iteration 830: Mean Return = -22.8\n",
            "Iteration 840: Mean Return = -29.8\n",
            "Iteration 850: Mean Return = -32.4\n",
            "Iteration 860: Mean Return = -9.4\n",
            "Iteration 870: Mean Return = -31.200000000000003\n",
            "Iteration 880: Mean Return = -25.0\n",
            "Iteration 890: Mean Return = -11.0\n",
            "Iteration 900: Mean Return = -21.6\n",
            "Iteration 910: Mean Return = -12.200000000000001\n",
            "Iteration 920: Mean Return = -23.8\n",
            "Iteration 930: Mean Return = -26.6\n",
            "Iteration 940: Mean Return = -15.600000000000001\n",
            "Iteration 950: Mean Return = -43.0\n",
            "Iteration 960: Mean Return = -17.8\n",
            "Iteration 970: Mean Return = -18.400000000000002\n",
            "Iteration 980: Mean Return = -18.0\n",
            "Iteration 990: Mean Return = -56.0\n",
            "Iteration 1000: Mean Return = -15.600000000000001\n",
            "Iteration 1010: Mean Return = -9.4\n",
            "Iteration 1020: Mean Return = -21.200000000000003\n",
            "Iteration 1030: Mean Return = -11.8\n",
            "Iteration 1040: Mean Return = -37.6\n",
            "Iteration 1050: Mean Return = -9.0\n",
            "Iteration 1060: Mean Return = -3.8000000000000003\n",
            "Iteration 1070: Mean Return = -23.6\n",
            "Iteration 1080: Mean Return = -12.0\n",
            "Iteration 1090: Mean Return = -21.6\n",
            "Iteration 1100: Mean Return = -6.4\n",
            "Iteration 1110: Mean Return = -17.8\n",
            "Iteration 1120: Mean Return = -12.200000000000001\n",
            "Iteration 1130: Mean Return = -4.800000000000001\n",
            "Iteration 1140: Mean Return = -7.4\n",
            "Iteration 1150: Mean Return = -5.800000000000001\n",
            "Iteration 1160: Mean Return = -15.600000000000001\n",
            "Iteration 1170: Mean Return = 0.2\n",
            "Iteration 1180: Mean Return = -15.4\n",
            "Iteration 1190: Mean Return = -12.8\n",
            "Iteration 1200: Mean Return = -13.0\n",
            "Iteration 1210: Mean Return = -10.600000000000001\n",
            "Iteration 1220: Mean Return = -18.2\n",
            "Iteration 1230: Mean Return = -10.0\n",
            "Iteration 1240: Mean Return = -25.0\n",
            "Iteration 1250: Mean Return = -0.2\n",
            "Iteration 1260: Mean Return = -11.0\n",
            "Iteration 1270: Mean Return = -9.600000000000001\n",
            "Iteration 1280: Mean Return = -5.2\n",
            "Iteration 1290: Mean Return = -16.400000000000002\n",
            "Iteration 1300: Mean Return = -19.0\n",
            "Iteration 1310: Mean Return = -16.6\n",
            "Iteration 1320: Mean Return = -10.600000000000001\n",
            "Iteration 1330: Mean Return = -14.8\n",
            "Iteration 1340: Mean Return = -7.2\n",
            "Iteration 1350: Mean Return = -6.0\n",
            "Iteration 1360: Mean Return = -10.200000000000001\n",
            "Iteration 1370: Mean Return = -9.600000000000001\n",
            "Iteration 1380: Mean Return = -9.200000000000001\n",
            "Iteration 1390: Mean Return = -6.6000000000000005\n",
            "Iteration 1400: Mean Return = -18.0\n",
            "Iteration 1410: Mean Return = -4.2\n",
            "Iteration 1420: Mean Return = -7.800000000000001\n",
            "Iteration 1430: Mean Return = -9.8\n",
            "Iteration 1440: Mean Return = -12.600000000000001\n",
            "Iteration 1450: Mean Return = -1.6\n",
            "Iteration 1460: Mean Return = -4.2\n",
            "Iteration 1470: Mean Return = -6.0\n",
            "Iteration 1480: Mean Return = -7.6000000000000005\n",
            "Iteration 1490: Mean Return = -9.8\n",
            "Iteration 1500: Mean Return = -10.0\n",
            "Iteration 1510: Mean Return = -1.8\n",
            "Iteration 1520: Mean Return = 1.4000000000000001\n",
            "Iteration 1530: Mean Return = -1.2000000000000002\n",
            "Iteration 1540: Mean Return = -11.600000000000001\n",
            "Iteration 1550: Mean Return = -8.0\n",
            "Iteration 1560: Mean Return = 0.4\n",
            "Iteration 1570: Mean Return = -15.8\n",
            "Iteration 1580: Mean Return = -2.2\n",
            "Iteration 1590: Mean Return = -1.4000000000000001\n",
            "Iteration 1600: Mean Return = -8.200000000000001\n",
            "Iteration 1610: Mean Return = 0.4\n",
            "Iteration 1620: Mean Return = -9.4\n",
            "Iteration 1630: Mean Return = -2.2\n",
            "Iteration 1640: Mean Return = -2.4000000000000004\n",
            "Iteration 1650: Mean Return = 2.2\n",
            "Iteration 1660: Mean Return = -5.2\n",
            "Iteration 1670: Mean Return = -1.0\n",
            "Iteration 1680: Mean Return = 3.4000000000000004\n",
            "Iteration 1690: Mean Return = 2.6\n",
            "Iteration 1700: Mean Return = 1.6\n",
            "Iteration 1710: Mean Return = -2.4000000000000004\n",
            "Iteration 1720: Mean Return = 3.8000000000000003\n",
            "Iteration 1730: Mean Return = 0.4\n",
            "Iteration 1740: Mean Return = 5.2\n",
            "Iteration 1750: Mean Return = 1.6\n",
            "Iteration 1760: Mean Return = -2.4000000000000004\n",
            "Iteration 1770: Mean Return = -4.0\n",
            "Iteration 1780: Mean Return = 5.0\n",
            "Iteration 1790: Mean Return = -3.6\n",
            "Iteration 1800: Mean Return = 3.4000000000000004\n",
            "Iteration 1810: Mean Return = 3.4000000000000004\n",
            "Iteration 1820: Mean Return = 0.4\n",
            "Iteration 1830: Mean Return = 1.6\n",
            "Iteration 1840: Mean Return = 3.0\n",
            "Iteration 1850: Mean Return = 1.4000000000000001\n",
            "Iteration 1860: Mean Return = -5.4\n",
            "Iteration 1870: Mean Return = 0.0\n",
            "Iteration 1880: Mean Return = 2.0\n",
            "Iteration 1890: Mean Return = 6.2\n",
            "Iteration 1900: Mean Return = 2.2\n",
            "Iteration 1910: Mean Return = -3.0\n",
            "Iteration 1920: Mean Return = -0.4\n",
            "Iteration 1930: Mean Return = 4.0\n",
            "Iteration 1940: Mean Return = 0.6000000000000001\n",
            "Iteration 1950: Mean Return = 3.8000000000000003\n",
            "Iteration 1960: Mean Return = -4.4\n",
            "Iteration 1970: Mean Return = 5.2\n",
            "Iteration 1980: Mean Return = 5.0\n",
            "Iteration 1990: Mean Return = -1.8\n",
            "Iteration 2000: Mean Return = -1.4000000000000001\n",
            "Iteration 2010: Mean Return = 2.4000000000000004\n",
            "Iteration 2020: Mean Return = 1.0\n",
            "Iteration 2030: Mean Return = 2.4000000000000004\n",
            "Iteration 2040: Mean Return = 0.4\n",
            "Iteration 2050: Mean Return = 1.2000000000000002\n",
            "Iteration 2060: Mean Return = 2.8000000000000003\n",
            "Iteration 2070: Mean Return = 0.4\n",
            "Iteration 2080: Mean Return = 4.800000000000001\n",
            "Iteration 2090: Mean Return = 2.0\n",
            "Iteration 2100: Mean Return = 1.8\n",
            "Iteration 2110: Mean Return = 0.4\n",
            "Iteration 2120: Mean Return = 1.0\n",
            "Iteration 2130: Mean Return = -2.8000000000000003\n",
            "Iteration 2140: Mean Return = -3.0\n",
            "Iteration 2150: Mean Return = 0.2\n",
            "Iteration 2160: Mean Return = 2.2\n",
            "Iteration 2170: Mean Return = -0.8\n",
            "Iteration 2180: Mean Return = 2.0\n",
            "Iteration 2190: Mean Return = 4.2\n",
            "Iteration 2200: Mean Return = 2.6\n",
            "Iteration 2210: Mean Return = 4.6000000000000005\n",
            "Iteration 2220: Mean Return = 4.2\n",
            "Iteration 2230: Mean Return = -3.0\n",
            "Iteration 2240: Mean Return = 3.4000000000000004\n",
            "Iteration 2250: Mean Return = 4.4\n",
            "Iteration 2260: Mean Return = -1.8\n",
            "Iteration 2270: Mean Return = 4.2\n",
            "Iteration 2280: Mean Return = -0.8\n",
            "Iteration 2290: Mean Return = 8.0\n",
            "Iteration 2300: Mean Return = -4.0\n",
            "Iteration 2310: Mean Return = 5.0\n",
            "Iteration 2320: Mean Return = -4.6000000000000005\n",
            "Iteration 2330: Mean Return = 4.4\n",
            "Iteration 2340: Mean Return = 7.0\n",
            "Iteration 2350: Mean Return = 6.6000000000000005\n",
            "Iteration 2360: Mean Return = 4.6000000000000005\n",
            "Iteration 2370: Mean Return = 1.6\n",
            "Iteration 2380: Mean Return = 2.8000000000000003\n",
            "Iteration 2390: Mean Return = 2.0\n",
            "Iteration 2400: Mean Return = 5.4\n",
            "Iteration 2410: Mean Return = 7.800000000000001\n",
            "Iteration 2420: Mean Return = 2.4000000000000004\n",
            "Iteration 2430: Mean Return = -3.0\n",
            "Iteration 2440: Mean Return = 6.4\n",
            "Iteration 2450: Mean Return = -2.4000000000000004\n",
            "Iteration 2460: Mean Return = 0.6000000000000001\n",
            "Iteration 2470: Mean Return = 6.4\n",
            "Iteration 2480: Mean Return = 3.6\n",
            "Iteration 2490: Mean Return = 3.4000000000000004\n",
            "Iteration 2500: Mean Return = 4.800000000000001\n",
            "Iteration 2510: Mean Return = 5.2\n",
            "Iteration 2520: Mean Return = 6.2\n",
            "Iteration 2530: Mean Return = 6.0\n",
            "Iteration 2540: Mean Return = 3.8000000000000003\n",
            "Iteration 2550: Mean Return = 7.4\n",
            "Iteration 2560: Mean Return = 4.800000000000001\n",
            "Iteration 2570: Mean Return = 5.2\n",
            "Iteration 2580: Mean Return = 6.800000000000001\n",
            "Iteration 2590: Mean Return = 5.6000000000000005\n",
            "Iteration 2600: Mean Return = -6.4\n",
            "Iteration 2610: Mean Return = 5.6000000000000005\n",
            "Iteration 2620: Mean Return = 2.4000000000000004\n",
            "Iteration 2630: Mean Return = 4.6000000000000005\n",
            "Iteration 2640: Mean Return = 6.0\n",
            "Iteration 2650: Mean Return = -2.6\n",
            "Iteration 2660: Mean Return = 8.6\n",
            "Iteration 2670: Mean Return = 7.0\n",
            "Iteration 2680: Mean Return = 1.6\n",
            "Iteration 2690: Mean Return = 3.4000000000000004\n",
            "Iteration 2700: Mean Return = 3.0\n",
            "Iteration 2710: Mean Return = 5.800000000000001\n",
            "Iteration 2720: Mean Return = 2.6\n",
            "Iteration 2730: Mean Return = 2.6\n",
            "Iteration 2740: Mean Return = 0.2\n",
            "Iteration 2750: Mean Return = 6.6000000000000005\n",
            "Iteration 2760: Mean Return = 3.4000000000000004\n",
            "Iteration 2770: Mean Return = 6.0\n",
            "Iteration 2780: Mean Return = 7.2\n",
            "Iteration 2790: Mean Return = -0.2\n",
            "Iteration 2800: Mean Return = 7.6000000000000005\n",
            "Iteration 2810: Mean Return = 2.4000000000000004\n",
            "Iteration 2820: Mean Return = 5.4\n",
            "Iteration 2830: Mean Return = 4.4\n",
            "Iteration 2840: Mean Return = 3.8000000000000003\n",
            "Iteration 2850: Mean Return = 3.0\n",
            "Iteration 2860: Mean Return = 5.6000000000000005\n",
            "Iteration 2870: Mean Return = 5.6000000000000005\n",
            "Iteration 2880: Mean Return = 4.0\n",
            "Iteration 2890: Mean Return = 4.4\n",
            "Iteration 2900: Mean Return = 1.8\n",
            "Iteration 2910: Mean Return = 4.4\n",
            "Iteration 2920: Mean Return = 3.8000000000000003\n",
            "Iteration 2930: Mean Return = 3.4000000000000004\n",
            "Iteration 2940: Mean Return = 7.0\n",
            "Iteration 2950: Mean Return = 6.4\n",
            "Iteration 2960: Mean Return = 5.6000000000000005\n",
            "Iteration 2970: Mean Return = 5.800000000000001\n",
            "Iteration 2980: Mean Return = 7.4\n",
            "Iteration 2990: Mean Return = 3.8000000000000003\n",
            "Iteration 3000: Mean Return = 8.8\n",
            "Iteration 3010: Mean Return = 6.4\n",
            "Iteration 3020: Mean Return = 7.4\n",
            "Iteration 3030: Mean Return = 4.800000000000001\n",
            "Iteration 3040: Mean Return = 3.8000000000000003\n",
            "Iteration 3050: Mean Return = -2.2\n",
            "Iteration 3060: Mean Return = 3.4000000000000004\n",
            "Iteration 3070: Mean Return = 7.6000000000000005\n",
            "Iteration 3080: Mean Return = 7.4\n",
            "Iteration 3090: Mean Return = 8.200000000000001\n",
            "Iteration 3100: Mean Return = 6.4\n",
            "Iteration 3110: Mean Return = 7.0\n",
            "Iteration 3120: Mean Return = 2.4000000000000004\n",
            "Iteration 3130: Mean Return = 2.4000000000000004\n",
            "Iteration 3140: Mean Return = 5.6000000000000005\n",
            "Iteration 3150: Mean Return = 5.4\n",
            "Iteration 3160: Mean Return = 2.4000000000000004\n",
            "Iteration 3170: Mean Return = 2.0\n",
            "Iteration 3180: Mean Return = 4.800000000000001\n",
            "Iteration 3190: Mean Return = 7.6000000000000005\n",
            "Iteration 3200: Mean Return = 6.800000000000001\n",
            "Iteration 3210: Mean Return = 8.0\n",
            "Iteration 3220: Mean Return = 5.2\n",
            "Iteration 3230: Mean Return = 5.2\n",
            "Iteration 3240: Mean Return = 5.6000000000000005\n",
            "Iteration 3250: Mean Return = 4.6000000000000005\n",
            "Iteration 3260: Mean Return = 6.4\n",
            "Iteration 3270: Mean Return = 3.8000000000000003\n",
            "Iteration 3280: Mean Return = 5.6000000000000005\n",
            "Iteration 3290: Mean Return = 6.0\n",
            "Iteration 3300: Mean Return = 6.0\n",
            "Iteration 3310: Mean Return = 4.6000000000000005\n",
            "Iteration 3320: Mean Return = 6.4\n",
            "Iteration 3330: Mean Return = 6.800000000000001\n",
            "Iteration 3340: Mean Return = 6.4\n",
            "Iteration 3350: Mean Return = 6.2\n",
            "Iteration 3360: Mean Return = 3.0\n",
            "Iteration 3370: Mean Return = 6.800000000000001\n",
            "Iteration 3380: Mean Return = 5.6000000000000005\n",
            "Iteration 3390: Mean Return = 3.4000000000000004\n",
            "Iteration 3400: Mean Return = 7.2\n",
            "Iteration 3410: Mean Return = 3.6\n",
            "Iteration 3420: Mean Return = 5.800000000000001\n",
            "Iteration 3430: Mean Return = 8.0\n",
            "Iteration 3440: Mean Return = 4.2\n",
            "Iteration 3450: Mean Return = 8.200000000000001\n",
            "Iteration 3460: Mean Return = 6.800000000000001\n",
            "Iteration 3470: Mean Return = 6.800000000000001\n",
            "Iteration 3480: Mean Return = 4.2\n",
            "Iteration 3490: Mean Return = 6.2\n",
            "Iteration 3500: Mean Return = 6.2\n",
            "Iteration 3510: Mean Return = 8.200000000000001\n",
            "Iteration 3520: Mean Return = 8.4\n",
            "Iteration 3530: Mean Return = 10.4\n",
            "Iteration 3540: Mean Return = 5.2\n",
            "Iteration 3550: Mean Return = 5.800000000000001\n",
            "Iteration 3560: Mean Return = 8.4\n",
            "Iteration 3570: Mean Return = 4.2\n",
            "Iteration 3580: Mean Return = 3.4000000000000004\n",
            "Iteration 3590: Mean Return = 7.2\n",
            "Iteration 3600: Mean Return = 8.200000000000001\n",
            "Iteration 3610: Mean Return = 7.6000000000000005\n",
            "Iteration 3620: Mean Return = 5.2\n",
            "Iteration 3630: Mean Return = 6.800000000000001\n",
            "Iteration 3640: Mean Return = 7.0\n",
            "Iteration 3650: Mean Return = 6.2\n",
            "Iteration 3660: Mean Return = 6.0\n",
            "Iteration 3670: Mean Return = 8.6\n",
            "Iteration 3680: Mean Return = 5.0\n",
            "Iteration 3690: Mean Return = 8.8\n",
            "Iteration 3700: Mean Return = 6.800000000000001\n",
            "Iteration 3710: Mean Return = 6.0\n",
            "Iteration 3720: Mean Return = 3.8000000000000003\n",
            "Iteration 3730: Mean Return = 5.4\n",
            "Iteration 3740: Mean Return = 4.800000000000001\n",
            "Iteration 3750: Mean Return = 4.6000000000000005\n",
            "Iteration 3760: Mean Return = 9.0\n",
            "Iteration 3770: Mean Return = 7.6000000000000005\n",
            "Iteration 3780: Mean Return = 4.6000000000000005\n",
            "Iteration 3790: Mean Return = 5.6000000000000005\n",
            "Iteration 3800: Mean Return = 5.4\n",
            "Iteration 3810: Mean Return = 6.800000000000001\n",
            "Iteration 3820: Mean Return = 7.6000000000000005\n",
            "Iteration 3830: Mean Return = 3.6\n",
            "Iteration 3840: Mean Return = 6.800000000000001\n",
            "Iteration 3850: Mean Return = 10.600000000000001\n",
            "Iteration 3860: Mean Return = 7.800000000000001\n",
            "Iteration 3870: Mean Return = 6.6000000000000005\n",
            "Iteration 3880: Mean Return = 6.6000000000000005\n",
            "Iteration 3890: Mean Return = 1.6\n",
            "Iteration 3900: Mean Return = 8.0\n",
            "Iteration 3910: Mean Return = 6.800000000000001\n",
            "Iteration 3920: Mean Return = 6.4\n",
            "Iteration 3930: Mean Return = 5.800000000000001\n",
            "Iteration 3940: Mean Return = 7.0\n",
            "Iteration 3950: Mean Return = 7.2\n",
            "Iteration 3960: Mean Return = 7.2\n",
            "Iteration 3970: Mean Return = 7.800000000000001\n",
            "Iteration 3980: Mean Return = 8.200000000000001\n",
            "Iteration 3990: Mean Return = 5.0\n",
            "Iteration 4000: Mean Return = 5.4\n",
            "Iteration 4010: Mean Return = 6.2\n",
            "Iteration 4020: Mean Return = 5.0\n",
            "Iteration 4030: Mean Return = 6.800000000000001\n",
            "Iteration 4040: Mean Return = 6.6000000000000005\n",
            "Iteration 4050: Mean Return = 4.0\n",
            "Iteration 4060: Mean Return = 6.4\n",
            "Iteration 4070: Mean Return = 6.6000000000000005\n",
            "Iteration 4080: Mean Return = 6.0\n",
            "Iteration 4090: Mean Return = 7.6000000000000005\n",
            "Iteration 4100: Mean Return = 7.0\n",
            "Iteration 4110: Mean Return = 7.0\n",
            "Iteration 4120: Mean Return = 8.200000000000001\n",
            "Iteration 4130: Mean Return = 2.6\n",
            "Iteration 4140: Mean Return = 7.2\n",
            "Iteration 4150: Mean Return = 3.4000000000000004\n",
            "Iteration 4160: Mean Return = 7.800000000000001\n",
            "Iteration 4170: Mean Return = 4.6000000000000005\n",
            "Iteration 4180: Mean Return = 4.800000000000001\n",
            "Iteration 4190: Mean Return = 6.0\n",
            "Iteration 4200: Mean Return = 6.4\n",
            "Iteration 4210: Mean Return = 8.0\n",
            "Iteration 4220: Mean Return = 7.6000000000000005\n",
            "Iteration 4230: Mean Return = 6.4\n",
            "Iteration 4240: Mean Return = 7.2\n",
            "Iteration 4250: Mean Return = 7.2\n",
            "Iteration 4260: Mean Return = 7.0\n",
            "Iteration 4270: Mean Return = 8.4\n",
            "Iteration 4280: Mean Return = 5.6000000000000005\n",
            "Iteration 4290: Mean Return = 6.2\n",
            "Iteration 4300: Mean Return = 2.8000000000000003\n",
            "Iteration 4310: Mean Return = 8.4\n",
            "Iteration 4320: Mean Return = 4.800000000000001\n",
            "Iteration 4330: Mean Return = 4.2\n",
            "Iteration 4340: Mean Return = 6.4\n",
            "Iteration 4350: Mean Return = 8.0\n",
            "Iteration 4360: Mean Return = 4.0\n",
            "Iteration 4370: Mean Return = 4.4\n",
            "Iteration 4380: Mean Return = 6.6000000000000005\n",
            "Iteration 4390: Mean Return = 7.6000000000000005\n",
            "Iteration 4400: Mean Return = 6.800000000000001\n",
            "Iteration 4410: Mean Return = 4.2\n",
            "Iteration 4420: Mean Return = 8.6\n",
            "Iteration 4430: Mean Return = 9.0\n",
            "Iteration 4440: Mean Return = 6.4\n",
            "Iteration 4450: Mean Return = 7.2\n",
            "Iteration 4460: Mean Return = 5.800000000000001\n",
            "Iteration 4470: Mean Return = 6.2\n",
            "Iteration 4480: Mean Return = 6.2\n",
            "Iteration 4490: Mean Return = 8.200000000000001\n",
            "Iteration 4500: Mean Return = 7.4\n",
            "Iteration 4510: Mean Return = 8.4\n",
            "Iteration 4520: Mean Return = 8.0\n",
            "Iteration 4530: Mean Return = 9.8\n",
            "Iteration 4540: Mean Return = 6.4\n",
            "Iteration 4550: Mean Return = 8.4\n",
            "Iteration 4560: Mean Return = 5.800000000000001\n",
            "Iteration 4570: Mean Return = 9.4\n",
            "Iteration 4580: Mean Return = 7.6000000000000005\n",
            "Iteration 4590: Mean Return = 4.6000000000000005\n",
            "Iteration 4600: Mean Return = 6.4\n",
            "Iteration 4610: Mean Return = 8.6\n",
            "Iteration 4620: Mean Return = 6.6000000000000005\n",
            "Iteration 4630: Mean Return = 4.6000000000000005\n",
            "Iteration 4640: Mean Return = 9.200000000000001\n",
            "Iteration 4650: Mean Return = 2.4000000000000004\n",
            "Iteration 4660: Mean Return = 9.4\n",
            "Iteration 4670: Mean Return = 8.8\n",
            "Iteration 4680: Mean Return = 7.800000000000001\n",
            "Iteration 4690: Mean Return = 5.0\n",
            "Iteration 4700: Mean Return = 8.4\n",
            "Iteration 4710: Mean Return = 1.6\n",
            "Iteration 4720: Mean Return = 9.4\n",
            "Iteration 4730: Mean Return = 6.800000000000001\n",
            "Iteration 4740: Mean Return = 7.0\n",
            "Iteration 4750: Mean Return = 1.6\n",
            "Iteration 4760: Mean Return = 8.4\n",
            "Iteration 4770: Mean Return = 6.0\n",
            "Iteration 4780: Mean Return = 5.0\n",
            "Iteration 4790: Mean Return = 7.800000000000001\n",
            "Iteration 4800: Mean Return = 6.6000000000000005\n",
            "Iteration 4810: Mean Return = 7.6000000000000005\n",
            "Iteration 4820: Mean Return = 4.4\n",
            "Iteration 4830: Mean Return = 8.4\n",
            "Iteration 4840: Mean Return = 6.0\n",
            "Iteration 4850: Mean Return = 6.2\n",
            "Iteration 4860: Mean Return = 7.0\n",
            "Iteration 4870: Mean Return = 4.6000000000000005\n",
            "Iteration 4880: Mean Return = 6.4\n",
            "Iteration 4890: Mean Return = 4.800000000000001\n",
            "Iteration 4900: Mean Return = 10.4\n",
            "Iteration 4910: Mean Return = 7.4\n",
            "Iteration 4920: Mean Return = 9.0\n",
            "Iteration 4930: Mean Return = 7.4\n",
            "Iteration 4940: Mean Return = 4.800000000000001\n",
            "Iteration 4950: Mean Return = 6.2\n",
            "Iteration 4960: Mean Return = 6.2\n",
            "Iteration 4970: Mean Return = 7.2\n",
            "Iteration 4980: Mean Return = 9.0\n",
            "Iteration 4990: Mean Return = 8.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import torch\n",
        "import gym\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "###  policyGradient  ###\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class PolicyNet(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_layer_size=64):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(500, input_size)\n",
        "        # self.fc1 = torch.nn.Linear(input_size, hidden_layer_size)   # MLP\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, hidden_layer_size),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "        # self.fc2 = torch.nn.Linear(hidden_layer_size, output_size)  # MLP\n",
        "\n",
        "        self.final = torch.nn.Linear(hidden_layer_size, output_size)\n",
        "        self.softmax = torch.nn.Softmax(dim=0)                      # 소프트맥스 필요, 출력 값이 각 action을 실행할 '확률'이라서\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x)\n",
        "        # x = torch.from_numpy(x).float()\n",
        "        x= torch.LongTensor([x])\n",
        "        x = self.embedding(x)\n",
        "        x= x.squeeze(0)\n",
        "\n",
        "        return self.softmax(self.final(self.layer1(x)))\n",
        "\n",
        "    def get_action_and_logp(self, x):\n",
        "        action_prob = self.forward(x)  # action_prob : 각 action을 실행할 '확률'값들에 대한 tensor ,  ex) [0.1, 0.5, 0.2, 0.2]\n",
        "        m = torch.distributions.Categorical(action_prob)  # Categorical한 대상에 대해 확률분포를 생성함. ex) action_prob=[0.1, 0.5, 0.2, 0.2]라면, 0번째 action을 뽑을 확률 0.1, 1번째 action을 뽑을 확률 0.5\n",
        "\n",
        "        action = m.sample()                               # 출력값은 tensor(3) or tensor(1), or tensor(0) 등 index로 출력함\n",
        "        # print(m.sample())                                 # 출력값은 tensor(3) or tensor(1), or tensor(0) 등 index로 출력함\n",
        "        logp = m.log_prob(action)                         # log_prob은 확률값을 log로 변환시키는 함수 , log(tensor(0))과 동일 즉, log(0번째 action의 확률)과 동일\n",
        "        # print(logp)                                     # 출력 : tensor(-2.3026)   // np.log(0.1)  = -2.3025850929940455\n",
        "        return action.item(), logp                        # action_index , log 취한 action 선택 확률\n",
        "\n",
        "    def act(self, x):\n",
        "        action, _ = self.get_action_and_logp(x)\n",
        "        return action\n",
        "\n",
        "class ValueNet(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_layer_size=64):\n",
        "        super(ValueNet, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(500, input_size)\n",
        "        self.fc1 = torch.nn.Linear(input_size, hidden_layer_size)\n",
        "        self.fc2 = torch.nn.Linear(hidden_layer_size, 6)\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, hidden_layer_size),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "        self.final = torch.nn.Linear(hidden_layer_size, 6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= torch.LongTensor([x])\n",
        "        x = self.embedding(x)\n",
        "        x= x.squeeze(0)\n",
        "        return self.final(self.layer1(x))\n",
        "        # return self.fc2(torch.nn.functional.relu(self.fc1(x)))\n",
        "\n",
        "def policyGradient(env, max_num_steps=500, gamma=0.98, lr=0.002,\n",
        "                   num_traj=10, num_iter=200):\n",
        "    input_size = env.observation_space.n  ## STATE SPACE (STATE의 개수)\n",
        "    output_size = env.action_space.n             ## ACTION의 개수\n",
        "    Trajectory = namedtuple('Trajectory', 'states actions rewards dones logp')    # 'Trajectory'에 states actions rewards dones logp 튜플을 할당함\n",
        "\n",
        "\n",
        "    def collect_trajectory():  # Trajectory 모으는 함수 (1 episode 돌리는 함수)\n",
        "        state_list = []\n",
        "        action_list = []\n",
        "        reward_list = []\n",
        "        dones_list = []\n",
        "        logp_list = []\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        while not done and steps <= max_num_steps:   # done(에피소드 종료) 될때까지 루프 \n",
        "            action, logp = policy.get_action_and_logp(state)  # policy = PolicyNet(input_size, output_size)\n",
        "            newstate, reward, done, _, _ = env.step(action)    # s_, r, terminated, truncated, _\n",
        "            state_list.append(state)\n",
        "            action_list.append(action)\n",
        "            reward_list.append(reward)\n",
        "            dones_list.append(done)\n",
        "            logp_list.append(logp)\n",
        "            steps += 1\n",
        "            state = newstate\n",
        "\n",
        "        traj = Trajectory(states=state_list, actions=action_list,\n",
        "                          rewards=reward_list, logp=logp_list, dones=dones_list)\n",
        "      # print(traj)  # Trajectory(states=[array([ 9.5363184e-06,  9.3068667e-03,  3.0773466e-02, -4.3019545e-03],\n",
        "      # dtype=float32), array([ 1.9567365e-04, -1.8624260e-01,  3.0687427e-02,  2.9792932e-01],\n",
        "      # dtype=float32), array([-0.00352918,  0.00842878,  0.03664601,  0.01508044], dtype=float32), array([-0.0033606 ,  0.20300654,  0.03694762, -0.26581872], dtype=float32), array([ 0.00069953,  0.3975822 ,  0.03163125, -0.54662293], dtype=float32), array([ 0.00865117,  0.5922458 ,  0.02069879, -0.82917416], dtype=float32), array([ 0.02049609,  0.3968471 ,  0.00411531, -0.5300539 ], dtype=float32), array([ 0.02843303,  0.5919109 , -0.00648577, -0.82143724], dtype=float32), array([ 0.04027125,  0.3968783 , -0.02291452, -0.5308013 ], dtype=float32), array([ 0.04820881,  0.59231496, -0.03353054, -0.8306156 ], dtype=float32), array([ 0.06005511,  0.39766696, -0.05014285, -0.54866385], dtype=float32), array([ 0.06800845,  0.20328394, -0.06111613, -0.27219164], dtype=float32), array([ 0.07207413,  0.0090849 , -0.06655996,  0.00060612], dtype=float32), array([ 0.07225583, -0.18502247, -0.06654784,  0.27156827], dtype=float32), array([ 0.06855538, -0.37913483, -0.06111648,  0.5425417 ], dtype=float32), array([ 0.06097268, -0.18320957, -0.05026564,  0.23124543], dtype=float32), array([ 0.05730849,  0.01259326, -0.04564074, -0.07685973], dtype=float32), array([ 0.05756036, -0.1818457 , -0.04717793,  0.20108126], dtype=float32), array([ 0.05392344, -0.37626228, -0.0431563 ,  0.4785165 ], dtype=float32), array([ 0.0463982 , -0.5707492 , -0.03358598,  0.75729126], dtype=float32), array([ 0.03498321, -0.37518087, -0.01844015,  0.45423177], dtype=float32), array([ 0.02747959, -0.5700373 , -0.00935551,  0.7410456 ], dtype=float32), array([ 0.01607885, -0.37478745,  0.0054654 ,  0.44543317], dtype=float32), array([ 0.0085831 , -0.5699863 ,  0.01437406,  0.7398339 ], dtype=float32), array([-0.00281663, -0.76530373,  0.02917074,  1.0370055 ], dtype=float32), array([-0.0181227 , -0.57058144,  0.04991085,  0.75362134], dtype=float32), array([-0.02953433, -0.7663547 ,  0.06498328,  1.0615833 ], dtype=float32), array([-0.04486142, -0.5721506 ,  0.08621494,  0.78998363], dtype=float32), array([-0.05630443, -0.768344  ,  0.10201462,  1.1084964 ], dtype=float32), array([-0.07167131, -0.5746998 ,  0.12418454,  0.8494806 ], dtype=float32), array([-0.08316531, -0.38147032,  0.14117415,  0.59828496], dtype=float32), array([-0.09079471, -0.18857652,  0.15313986,  0.35319027], dtype=float32), array([-0.09456625,  0.00407392,  0.16020367,  0.11244383], dtype=float32), array([-0.09448477,  0.19658096,  0.16245253, -0.12572043], dtype=float32), array([-0.09055315,  0.38904798,  0.15993813, -0.36306855], dtype=float32), array([-0.08277219,  0.58157825,  0.15267676, -0.60135657], dtype=float32), array([-0.07114062,  0.774272  ,  0.14064963, -0.8423221 ], dtype=float32), array([-0.05565519,  0.5775394 ,  0.12380318, -0.50892246], dtype=float32), array([-0.0441044 ,  0.3809106 ,  0.11362474, -0.17993148], dtype=float32), array([-0.03648619,  0.18436155,  0.11002611,  0.14632481], dtype=float32), array([-0.03279895, -0.01214998,  0.1129526 ,  0.47159216], dtype=float32), array([-0.03304195, -0.20867096,  0.12238444,  0.79763263], dtype=float32), array([-0.03721537, -0.01542167,  0.1383371 ,  0.54581815], dtype=float32), array([-0.03752381,  0.17751318,  0.14925346,  0.29972214], dtype=float32), array([-0.03397354, -0.01938604,  0.15524791,  0.6355052 ], dtype=float32), array([-0.03436126,  0.17326893,  0.167958  ,  0.395458  ], dtype=float32), array([-0.03089589,  0.36565927,  0.17586717,  0.16008124], dtype=float32), array([-0.0235827 ,  0.5578845 ,  0.17906879, -0.07237025], dtype=float32), array([-0.01242501,  0.75004774,  0.1776214 , -0.30364075], dtype=float32), array([0.00257595, 0.55289793, 0.17154858, 0.0393778 ], dtype=float32), array([ 0.0136339 ,  0.74519783,  0.17233613, -0.19464979], dtype=float32), array([0.02853786, 0.5480834 , 0.16844313, 0.14705835], dtype=float32), array([ 0.03949953,  0.74044305,  0.1713843 , -0.08810893], dtype=float32), array([0.05430839, 0.54333186, 0.16962212, 0.25336695], dtype=float32), array([0.06517503, 0.73567706, 0.17468946, 0.01861984], dtype=float32), array([ 0.07988857,  0.9279195 ,  0.17506185, -0.21425721], dtype=float32), array([0.09844696, 0.7307833 , 0.17077671, 0.12813324], dtype=float32), array([0.11306263, 0.53367877, 0.17333938, 0.46945378], dtype=float32), array([0.1237362 , 0.33658645, 0.18272845, 0.8113689 ], dtype=float32), array([0.13046794, 0.13949473, 0.19895583, 1.1555083 ], dtype=float32)], actions=[0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1], rewards=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], dones=[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True], logp=[tensor(-0.7011, grad_fn=<SqueezeBackward1>), tensor(-0.4942, grad_fn=<SqueezeBackward1>), tensor(-0.6693, grad_fn=<SqueezeBackward1>), tensor(-0.8719, grad_fn=<SqueezeBackward1>), tensor(-1.0927, grad_fn=<SqueezeBackward1>), tensor(-0.3000, grad_fn=<SqueezeBackward1>), tensor(-1.0943, grad_fn=<SqueezeBackward1>), tensor(-0.2976, grad_fn=<SqueezeBackward1>), tensor(-1.1101, grad_fn=<SqueezeBackward1>), tensor(-0.2912, grad_fn=<SqueezeBackward1>), tensor(-0.3850, grad_fn=<SqueezeBackward1>), tensor(-0.5034, grad_fn=<SqueezeBackward1>), tensor(-0.6458, grad_fn=<SqueezeBackward1>), tensor(-0.8513, grad_fn=<SqueezeBackward1>), tensor(-0.3972, grad_fn=<SqueezeBackward1>), tensor(-0.5714, grad_fn=<SqueezeBackward1>), tensor(-0.6144, grad_fn=<SqueezeBackward1>), tensor(-0.8118, grad_fn=<SqueezeBackward1>), tensor(-1.0638, grad_fn=<SqueezeBackward1>), tensor(-0.2649, grad_fn=<SqueezeBackward1>), tensor(-1.0635, grad_fn=<SqueezeBackward1>), tensor(-0.2634, grad_fn=<SqueezeBackward1>), tensor(-1.0795, grad_fn=<SqueezeBackward1>), tensor(-1.4881, grad_fn=<SqueezeBackward1>), tensor(-0.1490, grad_fn=<SqueezeBackward1>), tensor(-1.5465, grad_fn=<SqueezeBackward1>), tensor(-0.1374, grad_fn=<SqueezeBackward1>), tensor(-1.6341, grad_fn=<SqueezeBackward1>), tensor(-0.1220, grad_fn=<SqueezeBackward1>), tensor(-0.1897, grad_fn=<SqueezeBackward1>), tensor(-0.2872, grad_fn=<SqueezeBackward1>), tensor(-0.4043, grad_fn=<SqueezeBackward1>), tensor(-0.5435, grad_fn=<SqueezeBackward1>), tensor(-0.7004, grad_fn=<SqueezeBackward1>), tensor(-0.9015, grad_fn=<SqueezeBackward1>), tensor(-1.0939, grad_fn=<SqueezeBackward1>), tensor(-0.3111, grad_fn=<SqueezeBackward1>), tensor(-0.4323, grad_fn=<SqueezeBackward1>), tensor(-0.5986, grad_fn=<SqueezeBackward1>), tensor(-0.8267, grad_fn=<SqueezeBackward1>), tensor(-1.1433, grad_fn=<SqueezeBackward1>), tensor(-0.2386, grad_fn=<SqueezeBackward1>), tensor(-0.3441, grad_fn=<SqueezeBackward1>), tensor(-0.9646, grad_fn=<SqueezeBackward1>), tensor(-0.3061, grad_fn=<SqueezeBackward1>), tensor(-0.4233, grad_fn=<SqueezeBackward1>), tensor(-0.5728, grad_fn=<SqueezeBackward1>), tensor(-0.7365, grad_fn=<SqueezeBackward1>), tensor(-0.5085, grad_fn=<SqueezeBackward1>), tensor(-0.6748, grad_fn=<SqueezeBackward1>), tensor(-0.5544, grad_fn=<SqueezeBackward1>), tensor(-0.6252, grad_fn=<SqueezeBackward1>), tensor(-0.6050, grad_fn=<SqueezeBackward1>), tensor(-0.5652, grad_fn=<SqueezeBackward1>), tensor(-0.7268, grad_fn=<SqueezeBackward1>), tensor(-0.5218, grad_fn=<SqueezeBackward1>), tensor(-0.7160, grad_fn=<SqueezeBackward1>), tensor(-1.0044, grad_fn=<SqueezeBackward1>), tensor(-1.3989, grad_fn=<SqueezeBackward1>), tensor(-0.1645, grad_fn=<SqueezeBackward1>)])\n",
        "\n",
        "        return traj\n",
        "\n",
        "    def calc_returns(rewards):\n",
        "        dis_rewards = [gamma**i * r for i, r in enumerate(rewards)]  ## [ gamma**0 * r, gamma**1 * r, gamma**2 * r, gamma**3 * r, ...]\n",
        "        return [sum(dis_rewards[i:]) for i in range(len(dis_rewards))]  ## [R(0~end), R(1~end), R(2~end), ...]\n",
        "\n",
        "    policy = PolicyNet(input_size, output_size)\n",
        "    policy_optimizer = torch.optim.Adam(policy.parameters(), lr=lr)\n",
        "\n",
        "    value = ValueNet(input_size)\n",
        "    value_optimizer = torch.optim.Adam(value.parameters(), lr=lr)\n",
        "\n",
        "    mean_return_list = []\n",
        "    for it in range(num_iter):\n",
        "        traj_list = [collect_trajectory() for _ in range(num_traj)]    #\n",
        "        returns = [calc_returns(traj.rewards) for traj in traj_list]   # traj.rewards는 traj(Trajectory에서 tuple 요소중 rewards를 뽑아냄)\n",
        "                                                                       # returns는 2차원 : [[Return(0~end), Return(1~end), Return(2~end), ...],  [Return(0~end), Return(1~end), Return(2~end), ...], ... , [Return(0~end), Return(1~end), Return(2~end), ...] ]\n",
        "\n",
        "        # ====================================#\n",
        "        # policy gradient                    #\n",
        "        # ====================================#\n",
        "        # policy_loss_terms = [-1. * traj.logp[j] * (torch.Tensor([returns[i][0]]))                   # returns[i][0] :   Return(0~end)로만 참고함 ([Return(0~end), Return(0~end), Return(0~end), ..., ...])   Expectation of Return이 목적함수(로스 펑션으로 만들기 위해 -1 추가)\n",
        "        #                     for i, traj in enumerate(traj_list) for j in range(len(traj.actions))]  # policy gradient, policyNet에서의 로스 펑션, 값이 작을수록 Loss가 적은 형태로 식을 만들기 위해 앞에 -1 붙임\n",
        "\n",
        "        # ====================================#\n",
        "        # policy gradient with reward-to-go  #\n",
        "        # ====================================#\n",
        "        # policy_loss_terms = [-1. * traj.logp[j] * (torch.Tensor([returns[i][j]]))                   # returns[i][j]] :   Return(j~end)로 참고함 => [Return(0~end), Return(1~end), Return(2~end), ~, Return(end-1~end) , ..., ... , ]    Expectation of Return이 목적함수\n",
        "        #                     for i, traj in enumerate(traj_list) for j in range(len(traj.actions))]  # policy gradient, policyNet에서의 로스 펑션, 값이 작을수록 Loss가 적은 형태로 식을 만들기 위해 앞에 -1 붙임\n",
        "\n",
        "        #====================================#\n",
        "        # policy gradient with base function #\n",
        "        #====================================#\n",
        "        policy_loss_terms = [-1. * traj.logp[j] * (returns[i][j] - value(traj.states[j]))             # returns[i][j] - value(traj.states[j]  :   리턴값에 대해 표준화 적용 (평균값을 빼줌), 평균값은 곧 기대값이고 V(s)를 통해 구함. V(s)는 ValueNet에서 구함.\n",
        "                             for i, traj in enumerate(traj_list) for j in range(len(traj.actions))]   # policy gradient, policyNet에서의 로스 펑션, 값이 작을수록 Loss가 적은 형태로 식을 만들기 위해 앞에 -1 붙임\n",
        "\n",
        "\n",
        "        policy_loss = 1. / num_traj * torch.cat(policy_loss_terms).sum()                              # 각  returns의 원소들을 합산하고 나눔  =>  Expectation of Return 계산\n",
        "        policy_optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        policy_optimizer.step()\n",
        "\n",
        "        value_loss_terms = [1. / len(traj.actions) * (value(traj.states[j]) - returns[i][j])**2.      # ValueNet에서의 로스 펑션, MSE 적용\n",
        "                            for i, traj in enumerate(traj_list) for j in range(len(traj.actions))]\n",
        "        value_loss = 1. / num_traj * torch.cat(value_loss_terms).sum()\n",
        "        value_optimizer.zero_grad()\n",
        "        value_loss.backward()\n",
        "        value_optimizer.step()\n",
        "\n",
        "        mean_return = 1. / num_traj * \\\n",
        "            sum([traj_returns[0] for traj_returns in returns])\n",
        "        mean_return_list.append(mean_return)\n",
        "        pd.DataFrame(mean_return_list).to_csv('/content/drive/MyDrive/강화학습/policy_gradient_model_record.csv')\n",
        "\n",
        "        if it % 10 == 0:\n",
        "            print('Iteration {}: Mean Return = {}'.format(it, mean_return))\n",
        "            torch.save(policy.state_dict(), '/content/drive/MyDrive/강화학습/policy_gradient_model_temp.pth')\n",
        "\n",
        "    torch.save(policy.state_dict(), '/content/drive/MyDrive/강화학습/policy_gradient_model_last.pth')\n",
        "\n",
        "    return policy, mean_return_list\n",
        "\n",
        "env = gym.make('Taxi-v3').unwrapped\n",
        "env._max_episode_steps=500\n",
        "agent, mean_return_list = policyGradient(env, num_iter=5000, max_num_steps=500, gamma=1.0, num_traj=5)\n",
        "\n",
        "\n",
        "plt.plot(mean_return_list)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Mean Return')\n",
        "plt.savefig('pg1_returns.png', format='png', dpi=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Gradient Metric"
      ],
      "metadata": {
        "id": "yFwbKGtWMOw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric_random = pd.read_csv('/content/drive/MyDrive/강화학습/policy_gradient_model_record.csv', index_col = 0).rename(columns = {'0':'return'})\n",
        "metric_700 = pd.read_csv('/content/drive/MyDrive/강화학습/policy_gradient_model_record1_seed700.csv', index_col = 0).rename(columns = {'0':'return'})\n",
        "metric_2022 = pd.read_csv('/content/drive/MyDrive/강화학습/policy_gradient_model_record2_seed2022.csv', index_col = 0).rename(columns = {'0':'return'})"
      ],
      "metadata": {
        "id": "8yrReZVSGwUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric_random.iloc[-100:].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "HYUjy38AHW2l",
        "outputId": "ee7f2bac-c1c3-44a6-c31e-5cf40da63c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           return\n",
              "count  100.000000\n",
              "mean     7.160000\n",
              "std      1.427578\n",
              "min      4.000000\n",
              "25%      6.200000\n",
              "50%      7.200000\n",
              "75%      8.000000\n",
              "max     10.800000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7711f8ca-95df-48e0-acf7-fb245cf943e2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>100.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>7.160000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.427578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>6.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>7.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>8.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>10.800000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7711f8ca-95df-48e0-acf7-fb245cf943e2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7711f8ca-95df-48e0-acf7-fb245cf943e2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7711f8ca-95df-48e0-acf7-fb245cf943e2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric_700.iloc[-100:].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "rHQ4mNs9Hzfm",
        "outputId": "9779d6d1-fcca-488c-8547-61d39ef693d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           return\n",
              "count  100.000000\n",
              "mean  -117.246000\n",
              "std     99.267804\n",
              "min   -501.000000\n",
              "25%   -195.400000\n",
              "50%    -94.400000\n",
              "75%    -67.000000\n",
              "max     10.200000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-74b24695-56a6-4369-8bd5-1634d2c62b13\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>100.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-117.246000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>99.267804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-501.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-195.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-94.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>-67.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>10.200000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74b24695-56a6-4369-8bd5-1634d2c62b13')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-74b24695-56a6-4369-8bd5-1634d2c62b13 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-74b24695-56a6-4369-8bd5-1634d2c62b13');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric_2022.iloc[-100:].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "wMy8fo7kHzY2",
        "outputId": "9682aeb6-2bf7-4c04-ef48-f862f074c1a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           return\n",
              "count  100.000000\n",
              "mean    -1.134000\n",
              "std     27.592036\n",
              "min    -95.800000\n",
              "25%      5.600000\n",
              "50%      6.800000\n",
              "75%      7.800000\n",
              "max     11.400000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5250fc9d-326a-4479-8de3-0d6acb19746e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>100.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-1.134000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>27.592036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-95.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>5.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>6.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>7.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>11.400000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5250fc9d-326a-4479-8de3-0d6acb19746e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5250fc9d-326a-4479-8de3-0d6acb19746e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5250fc9d-326a-4479-8de3-0d6acb19746e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Double DQN (Fail & Trial)"
      ],
      "metadata": {
        "id": "4fnYFSOuFb2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# libraries\n",
        "import gym\n",
        "import collections\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# pytorch library is used for deep learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.002\n",
        "gamma = 0.98\n",
        "buffer_limit = 200000        # size of replay buffer, 버퍼에 저장할 데이터 개수\n",
        "batch_size = 1024\n",
        "\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.001\n",
        "MAX_EPISODE = 500\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)    # double-ended queue(deque)를 저장소로 선택,\n",
        "                                                                # buffer_size는 50000으로 설정,\n",
        "                                                                # 50000개가 차면 그 이후부터는 가장 오래된 데이터를 삭제하고 새 데이터를 보관\n",
        "    \n",
        "    def put(self, transition):  # transition 데이터 넣기 메소드\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, n):     # transition 데이터 추출 메소드 (train할떄 batch만들듯이 random으로 데이터 추출해감)\n",
        "        mini_batch = random.sample(self.buffer, n)  # n개만큼 데이터 추출\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], [] # (s,a,r,s')과 종료여부\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done_mask = transition  # buffer에서 추출된 mini_batch에서 transition 데이터 추출\n",
        "            s_lst.append(s)\n",
        "            a_lst.append([a])\n",
        "            r_lst.append([r])\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask_lst.append([done_mask])\n",
        "\n",
        "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
        "               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
        "               torch.tensor(done_mask_lst)\n",
        "\n",
        "    def size(self):  # buffer의 사이즈 확인하는 용도의 메소드\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "class Qnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Qnet, self).__init__() # 초기화\n",
        "        self.embedding = torch.nn.Embedding(500, 500)\n",
        "        self.fc1 = nn.Linear(500, 128)  # 가중치 3개(히든 레이어 2개 적용) 입력값(state)은 4개 (cart position, cart 속도, pole 각도, pole 속도)\n",
        "        self.fc2 = nn.Linear(128, 64) # 가중치 3개(히든 레이어 2개 적용)\n",
        "        self.fc3 = nn.Linear(64, 6) # 가중치 3개(히든 레이어 2개 적용), output(Q(s,a))은 2개\n",
        "        self.softmax = torch.nn.Softmax(dim=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x.long())\n",
        "        x= x.squeeze(0)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "      \n",
        "    def prob_sample_action(self, obs):  # epsilon-greedy로 action 선택하는 함수,  obs : state input 값\n",
        "        out = self.forward(obs)  # forward를 진행 (신경망 통과), out은 tensor 형태\n",
        "        action_prob = self.softmax(out)\n",
        "        m = torch.distributions.Categorical(action_prob)\n",
        "        action = m.sample()\n",
        "        return action.item()   \n",
        "\n",
        "    def sample_action(self, obs, epsilon):  # epsilon-greedy로 action 선택하는 함수,  obs : state input 값\n",
        "        out = self.forward(obs)  # forward를 진행 (신경망 통과), out은 tensor 형태\n",
        "        # print(out, out.argmax())\n",
        "        coin = random.random()\n",
        "        if coin < epsilon:\n",
        "            return random.randint(0,5)\n",
        "        else : \n",
        "            return out.argmax().item()   \n",
        "\n",
        "def train(q, q_target, memory, optimizer):\n",
        "    for i in range(10):\n",
        "        s,a,r,s_prime,done_mask = memory.sample(batch_size)  # memory에서 데이터 sampling(추출하기), 배치 사이즈만큼\n",
        "\n",
        "        q_out = q(s)  # q_out.shape = (batch_size, 2)  // 2는 액션의 개수\n",
        "        q_a = q_out.gather(1,a)  # action_list인 a를 활용하여, Q테이블에서 각 state에 맞는 action 값을 취함.\n",
        "                                  # => 그에 대한 value 값을 q_a에 텐서로 저장\n",
        "\n",
        "        # DQN\n",
        "        # max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)  # 타겟넷에 대한 output 값에 대해 dim=1의 max값을 취함\n",
        "\n",
        "        # Double DQN\n",
        "        argmax_Q = q(s_prime).max(1)[1].unsqueeze(1)\n",
        "        max_q_prime = q_target(s_prime).gather(1, argmax_Q)\n",
        "\n",
        "        target = r + gamma * max_q_prime * done_mask # terminate stae면 done_mask가 0이므로 뒤의 항 없어짐\n",
        "        \n",
        "        # MSE Loss\n",
        "        loss = F.mse_loss(q_a, target)\n",
        "\n",
        "        # Smooth L1 Loss\n",
        "        #loss = F.smooth_l1_loss(q_a, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def epsilon_annealing(episode, max_episode, min_eps, max_eps):\n",
        "    if max_episode == 0:\n",
        "        return min_eps\n",
        "    slope = (min_eps - max_eps) / max_episode\n",
        "    return max(slope * episode + max_eps, min_eps)\n",
        "\n",
        "def main():\n",
        "    env = gym.make('Taxi-v3').unwrapped\n",
        "    q = Qnet()            # 메인 넷\n",
        "    q_target = Qnet()     # 타겟 넷  , 별개로 학습\n",
        "    q_target.load_state_dict(q.state_dict())\n",
        "    memory = ReplayBuffer()  ## transition을 담고 있는 테이블에 대한 객체 선언\n",
        "\n",
        "    print_interval = 20   #\n",
        "    score = 0.0           #\n",
        "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
        "    # boost = 1 \n",
        "\n",
        "    return_list = []\n",
        "    for n_epi in range(5000):\n",
        "        # epsilon = max(0.01, 0.5 - 0.02*(n_epi/200)*boost) #Linear annealing from 8% to 1%, epsilon 값을 0.01이 되기 전까지 점점 줄여가도록 함.\n",
        "        epsilon = epsilon_annealing(n_epi, MAX_EPISODE, EPS_END, EPS_START)\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "\n",
        "        if n_epi > 500:\n",
        "          boost = 5\n",
        "        \n",
        "\n",
        "        for i in range(15000):  # 200step으로 제한\n",
        "        # while not done:\n",
        "            if n_epi > 1000:\n",
        "              a = q.sample_action(torch.LongTensor([s]), epsilon)  # action을 policy(eplsilon-grredy)에 따라 선택\n",
        "\n",
        "            else:\n",
        "              a = q.prob_sample_action(torch.LongTensor([s]))\n",
        "              # print(s, a)\n",
        "\n",
        "            # a = random.randint(0,5)\n",
        "            s_prime, r, done, info, _ = env.step(a)  # 단계 진행\n",
        "            # print(s_prime, a, done)\n",
        "            # print(s, a, s_prime, r, done)\n",
        "            done_mask = 0.0 if done else 1.0\n",
        "            memory.put((s,a,r,s_prime, done_mask))  # r값이 너무 크면, train이 잘 안되서 줄엿음 (왜? 튜닝/테크닉의 영역이라는 교수님의 말씀)\n",
        "                                                          # replace_buffer 객체에 transition 데이터 저장\n",
        "            s = s_prime\n",
        "\n",
        "            score += r\n",
        "            if done:\n",
        "                return_list.append(score)\n",
        "                break\n",
        "\n",
        "        if memory.size()>20000:  # 메모리 사이즈가 2000까지 차기 전에는 train(x), 데이터가 적을때는 훈련이 무의미하므로\n",
        "            print('train', n_epi)\n",
        "            train(q, q_target, memory, optimizer)\n",
        "\n",
        "        print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(n_epi, score, memory.size(), epsilon*100))\n",
        "        score = 0.0\n",
        "\n",
        "        # if n_epi%print_interval==0 and n_epi!=0:  # 20번쨰 episode마다 메인넷(q)의 가중치값을 타겟넷(q_target)으로 카피함.\n",
        "        #     q_target.load_state_dict(q.state_dict())\n",
        "        #     print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n",
        "        #                                                     n_epi, score/print_interval, memory.size(), epsilon*100))\n",
        "        #     score = 0.0\n",
        "\n",
        "        \n",
        "        pd.DataFrame(return_list).to_csv('Double_DQN_model_record2.csv')\n",
        "        torch.save(q.state_dict(), 'Double_DQN_model_last2.pth')\n",
        "\n",
        "    env.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "ktcjY1LGX8BO",
        "outputId": "b0beab66-0c7f-4b9e-e5b1-65e8607527b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_episode :0, score : -3460.0, n_buffer : 988, eps : 90.0%\n",
            "n_episode :1, score : -3477.0, n_buffer : 2002, eps : 89.8%\n",
            "n_episode :2, score : -52140.0, n_buffer : 16417, eps : 89.6%\n",
            "n_episode :3, score : -3049.0, n_buffer : 17291, eps : 89.5%\n",
            "train 4\n",
            "n_episode :4, score : -39063.0, n_buffer : 28268, eps : 89.3%\n",
            "train 5\n",
            "n_episode :5, score : -26999.0, n_buffer : 40321, eps : 89.1%\n",
            "train 6\n",
            "n_episode :6, score : -878.0, n_buffer : 41202, eps : 88.9%\n",
            "train 7\n",
            "n_episode :7, score : -17466.0, n_buffer : 56202, eps : 88.7%\n",
            "train 8\n",
            "n_episode :8, score : -17286.0, n_buffer : 71202, eps : 88.6%\n",
            "train 9\n",
            "n_episode :9, score : -15990.0, n_buffer : 86202, eps : 88.4%\n",
            "train 10\n",
            "n_episode :10, score : -9115.0, n_buffer : 94681, eps : 88.2%\n",
            "train 11\n",
            "n_episode :11, score : -9563.0, n_buffer : 104211, eps : 88.0%\n",
            "train 12\n",
            "n_episode :12, score : -15018.0, n_buffer : 119211, eps : 87.8%\n",
            "train 13\n",
            "n_episode :13, score : -8909.0, n_buffer : 128123, eps : 87.7%\n",
            "train 14\n",
            "n_episode :14, score : -15036.0, n_buffer : 143123, eps : 87.5%\n",
            "train 15\n",
            "n_episode :15, score : -2729.0, n_buffer : 145855, eps : 87.3%\n",
            "train 16\n",
            "n_episode :16, score : -15099.0, n_buffer : 160855, eps : 87.1%\n",
            "train 17\n",
            "n_episode :17, score : -2830.0, n_buffer : 163544, eps : 86.9%\n",
            "train 18\n",
            "n_episode :18, score : -15009.0, n_buffer : 178544, eps : 86.8%\n",
            "train 19\n",
            "n_episode :19, score : -1724.0, n_buffer : 180271, eps : 86.6%\n",
            "train 20\n",
            "n_episode :20, score : -15009.0, n_buffer : 195271, eps : 86.4%\n",
            "train 21\n",
            "n_episode :21, score : -3789.0, n_buffer : 199072, eps : 86.2%\n",
            "train 22\n",
            "n_episode :22, score : -996.0, n_buffer : 200000, eps : 86.0%\n",
            "train 23\n",
            "n_episode :23, score : -15288.0, n_buffer : 200000, eps : 85.9%\n",
            "train 24\n",
            "n_episode :24, score : -15018.0, n_buffer : 200000, eps : 85.7%\n",
            "train 25\n",
            "n_episode :25, score : -15009.0, n_buffer : 200000, eps : 85.5%\n",
            "train 26\n",
            "n_episode :26, score : -15027.0, n_buffer : 200000, eps : 85.3%\n",
            "train 27\n",
            "n_episode :27, score : -1804.0, n_buffer : 200000, eps : 85.1%\n",
            "train 28\n",
            "n_episode :28, score : -3135.0, n_buffer : 200000, eps : 85.0%\n",
            "train 29\n",
            "n_episode :29, score : -15036.0, n_buffer : 200000, eps : 84.8%\n",
            "train 30\n",
            "n_episode :30, score : -179.0, n_buffer : 200000, eps : 84.6%\n",
            "train 31\n",
            "n_episode :31, score : -1964.0, n_buffer : 200000, eps : 84.4%\n",
            "train 32\n",
            "n_episode :32, score : -1600.0, n_buffer : 200000, eps : 84.2%\n",
            "train 33\n",
            "n_episode :33, score : -576.0, n_buffer : 200000, eps : 84.1%\n",
            "train 34\n",
            "n_episode :34, score : -934.0, n_buffer : 200000, eps : 83.9%\n",
            "train 35\n",
            "n_episode :35, score : -2728.0, n_buffer : 200000, eps : 83.7%\n",
            "train 36\n",
            "n_episode :36, score : -503.0, n_buffer : 200000, eps : 83.5%\n",
            "train 37\n",
            "n_episode :37, score : -3506.0, n_buffer : 200000, eps : 83.3%\n",
            "train 38\n",
            "n_episode :38, score : -805.0, n_buffer : 200000, eps : 83.2%\n",
            "train 39\n",
            "n_episode :39, score : -648.0, n_buffer : 200000, eps : 83.0%\n",
            "train 40\n",
            "n_episode :40, score : -4069.0, n_buffer : 200000, eps : 82.8%\n",
            "train 41\n",
            "n_episode :41, score : -271.0, n_buffer : 200000, eps : 82.6%\n",
            "train 42\n",
            "n_episode :42, score : -15081.0, n_buffer : 200000, eps : 82.4%\n",
            "train 43\n",
            "n_episode :43, score : -1888.0, n_buffer : 200000, eps : 82.3%\n",
            "train 44\n",
            "n_episode :44, score : -1382.0, n_buffer : 200000, eps : 82.1%\n",
            "train 45\n",
            "n_episode :45, score : -2509.0, n_buffer : 200000, eps : 81.9%\n",
            "train 46\n",
            "n_episode :46, score : -1386.0, n_buffer : 200000, eps : 81.7%\n",
            "train 47\n",
            "n_episode :47, score : -361.0, n_buffer : 200000, eps : 81.5%\n",
            "train 48\n",
            "n_episode :48, score : -505.0, n_buffer : 200000, eps : 81.4%\n",
            "train 49\n",
            "n_episode :49, score : -330.0, n_buffer : 200000, eps : 81.2%\n",
            "train 50\n",
            "n_episode :50, score : -4922.0, n_buffer : 200000, eps : 81.0%\n",
            "train 51\n",
            "n_episode :51, score : -25.0, n_buffer : 200000, eps : 80.8%\n",
            "train 52\n",
            "n_episode :52, score : -2573.0, n_buffer : 200000, eps : 80.7%\n",
            "train 53\n",
            "n_episode :53, score : -16206.0, n_buffer : 200000, eps : 80.5%\n",
            "train 54\n",
            "n_episode :54, score : -12367.0, n_buffer : 200000, eps : 80.3%\n",
            "train 55\n",
            "n_episode :55, score : -7285.0, n_buffer : 200000, eps : 80.1%\n",
            "train 56\n",
            "n_episode :56, score : -160.0, n_buffer : 200000, eps : 79.9%\n",
            "train 57\n",
            "n_episode :57, score : -32.0, n_buffer : 200000, eps : 79.8%\n",
            "train 58\n",
            "n_episode :58, score : -2296.0, n_buffer : 200000, eps : 79.6%\n",
            "train 59\n",
            "n_episode :59, score : -937.0, n_buffer : 200000, eps : 79.4%\n",
            "train 60\n",
            "n_episode :60, score : -353.0, n_buffer : 200000, eps : 79.2%\n",
            "train 61\n",
            "n_episode :61, score : -1073.0, n_buffer : 200000, eps : 79.0%\n",
            "train 62\n",
            "n_episode :62, score : -2015.0, n_buffer : 200000, eps : 78.9%\n",
            "train 63\n",
            "n_episode :63, score : -1049.0, n_buffer : 200000, eps : 78.7%\n",
            "train 64\n",
            "n_episode :64, score : -1315.0, n_buffer : 200000, eps : 78.5%\n",
            "train 65\n",
            "n_episode :65, score : -11855.0, n_buffer : 200000, eps : 78.3%\n",
            "train 66\n",
            "n_episode :66, score : -15747.0, n_buffer : 200000, eps : 78.1%\n",
            "train 67\n",
            "n_episode :67, score : -1642.0, n_buffer : 200000, eps : 78.0%\n",
            "train 68\n",
            "n_episode :68, score : -2167.0, n_buffer : 200000, eps : 77.8%\n",
            "train 69\n",
            "n_episode :69, score : -3531.0, n_buffer : 200000, eps : 77.6%\n",
            "train 70\n",
            "n_episode :70, score : -397.0, n_buffer : 200000, eps : 77.4%\n",
            "train 71\n",
            "n_episode :71, score : -1344.0, n_buffer : 200000, eps : 77.2%\n",
            "train 72\n",
            "n_episode :72, score : -801.0, n_buffer : 200000, eps : 77.1%\n",
            "train 73\n",
            "n_episode :73, score : -270.0, n_buffer : 200000, eps : 76.9%\n",
            "train 74\n",
            "n_episode :74, score : -426.0, n_buffer : 200000, eps : 76.7%\n",
            "train 75\n",
            "n_episode :75, score : -2661.0, n_buffer : 200000, eps : 76.5%\n",
            "train 76\n",
            "n_episode :76, score : -2707.0, n_buffer : 200000, eps : 76.3%\n",
            "train 77\n",
            "n_episode :77, score : -415.0, n_buffer : 200000, eps : 76.2%\n",
            "train 78\n",
            "n_episode :78, score : -2056.0, n_buffer : 200000, eps : 76.0%\n",
            "train 79\n",
            "n_episode :79, score : -630.0, n_buffer : 200000, eps : 75.8%\n",
            "train 80\n",
            "n_episode :80, score : -533.0, n_buffer : 200000, eps : 75.6%\n",
            "train 81\n",
            "n_episode :81, score : -360.0, n_buffer : 200000, eps : 75.4%\n",
            "train 82\n",
            "n_episode :82, score : -744.0, n_buffer : 200000, eps : 75.3%\n",
            "train 83\n",
            "n_episode :83, score : -1041.0, n_buffer : 200000, eps : 75.1%\n",
            "train 84\n",
            "n_episode :84, score : -2883.0, n_buffer : 200000, eps : 74.9%\n",
            "train 85\n",
            "n_episode :85, score : -2304.0, n_buffer : 200000, eps : 74.7%\n",
            "train 86\n",
            "n_episode :86, score : -1149.0, n_buffer : 200000, eps : 74.5%\n",
            "train 87\n",
            "n_episode :87, score : -738.0, n_buffer : 200000, eps : 74.4%\n",
            "train 88\n",
            "n_episode :88, score : -1263.0, n_buffer : 200000, eps : 74.2%\n",
            "train 89\n",
            "n_episode :89, score : -808.0, n_buffer : 200000, eps : 74.0%\n",
            "train 90\n",
            "n_episode :90, score : -7334.0, n_buffer : 200000, eps : 73.8%\n",
            "train 91\n",
            "n_episode :91, score : -340.0, n_buffer : 200000, eps : 73.6%\n",
            "train 92\n",
            "n_episode :92, score : -1397.0, n_buffer : 200000, eps : 73.5%\n",
            "train 93\n",
            "n_episode :93, score : -480.0, n_buffer : 200000, eps : 73.3%\n",
            "train 94\n",
            "n_episode :94, score : -556.0, n_buffer : 200000, eps : 73.1%\n",
            "train 95\n",
            "n_episode :95, score : -264.0, n_buffer : 200000, eps : 72.9%\n",
            "train 96\n",
            "n_episode :96, score : -178.0, n_buffer : 200000, eps : 72.7%\n",
            "train 97\n",
            "n_episode :97, score : -1026.0, n_buffer : 200000, eps : 72.6%\n",
            "train 98\n",
            "n_episode :98, score : -190.0, n_buffer : 200000, eps : 72.4%\n",
            "train 99\n",
            "n_episode :99, score : -1782.0, n_buffer : 200000, eps : 72.2%\n",
            "train 100\n",
            "n_episode :100, score : -7393.0, n_buffer : 200000, eps : 72.0%\n",
            "train 101\n",
            "n_episode :101, score : -208.0, n_buffer : 200000, eps : 71.8%\n",
            "train 102\n",
            "n_episode :102, score : -5714.0, n_buffer : 200000, eps : 71.7%\n",
            "train 103\n",
            "n_episode :103, score : -15081.0, n_buffer : 200000, eps : 71.5%\n",
            "train 104\n",
            "n_episode :104, score : -15135.0, n_buffer : 200000, eps : 71.3%\n",
            "train 105\n",
            "n_episode :105, score : -719.0, n_buffer : 200000, eps : 71.1%\n",
            "train 106\n",
            "n_episode :106, score : -51.0, n_buffer : 200000, eps : 70.9%\n",
            "train 107\n",
            "n_episode :107, score : -3662.0, n_buffer : 200000, eps : 70.8%\n",
            "train 108\n",
            "n_episode :108, score : -395.0, n_buffer : 200000, eps : 70.6%\n",
            "train 109\n",
            "n_episode :109, score : -580.0, n_buffer : 200000, eps : 70.4%\n",
            "train 110\n",
            "n_episode :110, score : -140.0, n_buffer : 200000, eps : 70.2%\n",
            "train 111\n",
            "n_episode :111, score : -420.0, n_buffer : 200000, eps : 70.0%\n",
            "train 112\n",
            "n_episode :112, score : -2077.0, n_buffer : 200000, eps : 69.9%\n",
            "train 113\n",
            "n_episode :113, score : -589.0, n_buffer : 200000, eps : 69.7%\n",
            "train 114\n",
            "n_episode :114, score : -177.0, n_buffer : 200000, eps : 69.5%\n",
            "train 115\n",
            "n_episode :115, score : -296.0, n_buffer : 200000, eps : 69.3%\n",
            "train 116\n",
            "n_episode :116, score : -2768.0, n_buffer : 200000, eps : 69.1%\n",
            "train 117\n",
            "n_episode :117, score : -4577.0, n_buffer : 200000, eps : 69.0%\n",
            "train 118\n",
            "n_episode :118, score : -88.0, n_buffer : 200000, eps : 68.8%\n",
            "train 119\n",
            "n_episode :119, score : -924.0, n_buffer : 200000, eps : 68.6%\n",
            "train 120\n",
            "n_episode :120, score : -114.0, n_buffer : 200000, eps : 68.4%\n",
            "train 121\n",
            "n_episode :121, score : -717.0, n_buffer : 200000, eps : 68.2%\n",
            "train 122\n",
            "n_episode :122, score : -461.0, n_buffer : 200000, eps : 68.1%\n",
            "train 123\n",
            "n_episode :123, score : -1471.0, n_buffer : 200000, eps : 67.9%\n",
            "train 124\n",
            "n_episode :124, score : -403.0, n_buffer : 200000, eps : 67.7%\n",
            "train 125\n",
            "n_episode :125, score : -591.0, n_buffer : 200000, eps : 67.5%\n",
            "train 126\n",
            "n_episode :126, score : -1198.0, n_buffer : 200000, eps : 67.3%\n",
            "train 127\n",
            "n_episode :127, score : -260.0, n_buffer : 200000, eps : 67.2%\n",
            "train 128\n",
            "n_episode :128, score : -294.0, n_buffer : 200000, eps : 67.0%\n",
            "train 129\n",
            "n_episode :129, score : -1809.0, n_buffer : 200000, eps : 66.8%\n",
            "train 130\n",
            "n_episode :130, score : -181.0, n_buffer : 200000, eps : 66.6%\n",
            "train 131\n",
            "n_episode :131, score : -910.0, n_buffer : 200000, eps : 66.4%\n",
            "train 132\n",
            "n_episode :132, score : -383.0, n_buffer : 200000, eps : 66.3%\n",
            "train 133\n",
            "n_episode :133, score : -409.0, n_buffer : 200000, eps : 66.1%\n",
            "train 134\n",
            "n_episode :134, score : -1235.0, n_buffer : 200000, eps : 65.9%\n",
            "train 135\n",
            "n_episode :135, score : -3979.0, n_buffer : 200000, eps : 65.7%\n",
            "train 136\n",
            "n_episode :136, score : -1224.0, n_buffer : 200000, eps : 65.5%\n",
            "train 137\n",
            "n_episode :137, score : -1080.0, n_buffer : 200000, eps : 65.4%\n",
            "train 138\n",
            "n_episode :138, score : -1487.0, n_buffer : 200000, eps : 65.2%\n",
            "train 139\n",
            "n_episode :139, score : -4953.0, n_buffer : 200000, eps : 65.0%\n",
            "train 140\n",
            "n_episode :140, score : -1259.0, n_buffer : 200000, eps : 64.8%\n",
            "train 141\n",
            "n_episode :141, score : -209.0, n_buffer : 200000, eps : 64.6%\n",
            "train 142\n",
            "n_episode :142, score : -1604.0, n_buffer : 200000, eps : 64.5%\n",
            "train 143\n",
            "n_episode :143, score : -493.0, n_buffer : 200000, eps : 64.3%\n",
            "train 144\n",
            "n_episode :144, score : -65.0, n_buffer : 200000, eps : 64.1%\n",
            "train 145\n",
            "n_episode :145, score : -2547.0, n_buffer : 200000, eps : 63.9%\n",
            "train 146\n",
            "n_episode :146, score : -1863.0, n_buffer : 200000, eps : 63.7%\n",
            "train 147\n",
            "n_episode :147, score : -380.0, n_buffer : 200000, eps : 63.6%\n",
            "train 148\n",
            "n_episode :148, score : -1642.0, n_buffer : 200000, eps : 63.4%\n",
            "train 149\n",
            "n_episode :149, score : -827.0, n_buffer : 200000, eps : 63.2%\n",
            "train 150\n",
            "n_episode :150, score : -1190.0, n_buffer : 200000, eps : 63.0%\n",
            "train 151\n",
            "n_episode :151, score : -308.0, n_buffer : 200000, eps : 62.9%\n",
            "train 152\n",
            "n_episode :152, score : 0.0, n_buffer : 200000, eps : 62.7%\n",
            "train 153\n",
            "n_episode :153, score : -194.0, n_buffer : 200000, eps : 62.5%\n",
            "train 154\n",
            "n_episode :154, score : -165.0, n_buffer : 200000, eps : 62.3%\n",
            "train 155\n",
            "n_episode :155, score : -56.0, n_buffer : 200000, eps : 62.1%\n",
            "train 156\n",
            "n_episode :156, score : -382.0, n_buffer : 200000, eps : 62.0%\n",
            "train 157\n",
            "n_episode :157, score : -348.0, n_buffer : 200000, eps : 61.8%\n",
            "train 158\n",
            "n_episode :158, score : -2556.0, n_buffer : 200000, eps : 61.6%\n",
            "train 159\n",
            "n_episode :159, score : -641.0, n_buffer : 200000, eps : 61.4%\n",
            "train 160\n",
            "n_episode :160, score : -677.0, n_buffer : 200000, eps : 61.2%\n",
            "train 161\n",
            "n_episode :161, score : -136.0, n_buffer : 200000, eps : 61.1%\n",
            "train 162\n",
            "n_episode :162, score : -880.0, n_buffer : 200000, eps : 60.9%\n",
            "train 163\n",
            "n_episode :163, score : -503.0, n_buffer : 200000, eps : 60.7%\n",
            "train 164\n",
            "n_episode :164, score : -1068.0, n_buffer : 200000, eps : 60.5%\n",
            "train 165\n",
            "n_episode :165, score : -361.0, n_buffer : 200000, eps : 60.3%\n",
            "train 166\n",
            "n_episode :166, score : -1391.0, n_buffer : 200000, eps : 60.2%\n",
            "train 167\n",
            "n_episode :167, score : -988.0, n_buffer : 200000, eps : 60.0%\n",
            "train 168\n",
            "n_episode :168, score : -501.0, n_buffer : 200000, eps : 59.8%\n",
            "train 169\n",
            "n_episode :169, score : -184.0, n_buffer : 200000, eps : 59.6%\n",
            "train 170\n",
            "n_episode :170, score : -2476.0, n_buffer : 200000, eps : 59.4%\n",
            "train 171\n",
            "n_episode :171, score : -2340.0, n_buffer : 200000, eps : 59.3%\n",
            "train 172\n",
            "n_episode :172, score : -271.0, n_buffer : 200000, eps : 59.1%\n",
            "train 173\n",
            "n_episode :173, score : -324.0, n_buffer : 200000, eps : 58.9%\n",
            "train 174\n",
            "n_episode :174, score : -231.0, n_buffer : 200000, eps : 58.7%\n",
            "train 175\n",
            "n_episode :175, score : -575.0, n_buffer : 200000, eps : 58.5%\n",
            "train 176\n",
            "n_episode :176, score : -851.0, n_buffer : 200000, eps : 58.4%\n",
            "train 177\n",
            "n_episode :177, score : 7.0, n_buffer : 200000, eps : 58.2%\n",
            "train 178\n",
            "n_episode :178, score : -550.0, n_buffer : 200000, eps : 58.0%\n",
            "train 179\n",
            "n_episode :179, score : -682.0, n_buffer : 200000, eps : 57.8%\n",
            "train 180\n",
            "n_episode :180, score : -807.0, n_buffer : 200000, eps : 57.6%\n",
            "train 181\n",
            "n_episode :181, score : -636.0, n_buffer : 200000, eps : 57.5%\n",
            "train 182\n",
            "n_episode :182, score : -285.0, n_buffer : 200000, eps : 57.3%\n",
            "train 183\n",
            "n_episode :183, score : -26.0, n_buffer : 200000, eps : 57.1%\n",
            "train 184\n",
            "n_episode :184, score : -1036.0, n_buffer : 200000, eps : 56.9%\n",
            "train 185\n",
            "n_episode :185, score : -525.0, n_buffer : 200000, eps : 56.7%\n",
            "train 186\n",
            "n_episode :186, score : -125.0, n_buffer : 200000, eps : 56.6%\n",
            "train 187\n",
            "n_episode :187, score : -451.0, n_buffer : 200000, eps : 56.4%\n",
            "train 188\n",
            "n_episode :188, score : -186.0, n_buffer : 200000, eps : 56.2%\n",
            "train 189\n",
            "n_episode :189, score : -397.0, n_buffer : 200000, eps : 56.0%\n",
            "train 190\n",
            "n_episode :190, score : -780.0, n_buffer : 200000, eps : 55.8%\n",
            "train 191\n",
            "n_episode :191, score : -676.0, n_buffer : 200000, eps : 55.7%\n",
            "train 192\n",
            "n_episode :192, score : -1331.0, n_buffer : 200000, eps : 55.5%\n",
            "train 193\n",
            "n_episode :193, score : -254.0, n_buffer : 200000, eps : 55.3%\n",
            "train 194\n",
            "n_episode :194, score : -338.0, n_buffer : 200000, eps : 55.1%\n",
            "train 195\n",
            "n_episode :195, score : -389.0, n_buffer : 200000, eps : 54.9%\n",
            "train 196\n",
            "n_episode :196, score : -66.0, n_buffer : 200000, eps : 54.8%\n",
            "train 197\n",
            "n_episode :197, score : -292.0, n_buffer : 200000, eps : 54.6%\n",
            "train 198\n",
            "n_episode :198, score : -4955.0, n_buffer : 200000, eps : 54.4%\n",
            "train 199\n",
            "n_episode :199, score : -1798.0, n_buffer : 200000, eps : 54.2%\n",
            "train 200\n",
            "n_episode :200, score : -716.0, n_buffer : 200000, eps : 54.0%\n",
            "train 201\n",
            "n_episode :201, score : -3010.0, n_buffer : 200000, eps : 53.9%\n",
            "train 202\n",
            "n_episode :202, score : -4801.0, n_buffer : 200000, eps : 53.7%\n",
            "train 203\n",
            "n_episode :203, score : -2576.0, n_buffer : 200000, eps : 53.5%\n",
            "train 204\n",
            "n_episode :204, score : -1871.0, n_buffer : 200000, eps : 53.3%\n",
            "train 205\n",
            "n_episode :205, score : -1139.0, n_buffer : 200000, eps : 53.1%\n",
            "train 206\n",
            "n_episode :206, score : -105.0, n_buffer : 200000, eps : 53.0%\n",
            "train 207\n",
            "n_episode :207, score : -354.0, n_buffer : 200000, eps : 52.8%\n",
            "train 208\n",
            "n_episode :208, score : -306.0, n_buffer : 200000, eps : 52.6%\n",
            "train 209\n",
            "n_episode :209, score : -1047.0, n_buffer : 200000, eps : 52.4%\n",
            "train 210\n",
            "n_episode :210, score : -75.0, n_buffer : 200000, eps : 52.2%\n",
            "train 211\n",
            "n_episode :211, score : -327.0, n_buffer : 200000, eps : 52.1%\n",
            "train 212\n",
            "n_episode :212, score : -622.0, n_buffer : 200000, eps : 51.9%\n",
            "train 213\n",
            "n_episode :213, score : -3114.0, n_buffer : 200000, eps : 51.7%\n",
            "train 214\n",
            "n_episode :214, score : -200.0, n_buffer : 200000, eps : 51.5%\n",
            "train 215\n",
            "n_episode :215, score : -109.0, n_buffer : 200000, eps : 51.3%\n",
            "train 216\n",
            "n_episode :216, score : -1471.0, n_buffer : 200000, eps : 51.2%\n",
            "train 217\n",
            "n_episode :217, score : -12395.0, n_buffer : 200000, eps : 51.0%\n",
            "train 218\n",
            "n_episode :218, score : -457.0, n_buffer : 200000, eps : 50.8%\n",
            "train 219\n",
            "n_episode :219, score : -909.0, n_buffer : 200000, eps : 50.6%\n",
            "train 220\n",
            "n_episode :220, score : -4590.0, n_buffer : 200000, eps : 50.4%\n",
            "train 221\n",
            "n_episode :221, score : -136.0, n_buffer : 200000, eps : 50.3%\n",
            "train 222\n",
            "n_episode :222, score : -324.0, n_buffer : 200000, eps : 50.1%\n",
            "train 223\n",
            "n_episode :223, score : -1576.0, n_buffer : 200000, eps : 49.9%\n",
            "train 224\n",
            "n_episode :224, score : -2498.0, n_buffer : 200000, eps : 49.7%\n",
            "train 225\n",
            "n_episode :225, score : -1513.0, n_buffer : 200000, eps : 49.5%\n",
            "train 226\n",
            "n_episode :226, score : -3855.0, n_buffer : 200000, eps : 49.4%\n",
            "train 227\n",
            "n_episode :227, score : -338.0, n_buffer : 200000, eps : 49.2%\n",
            "train 228\n",
            "n_episode :228, score : -1858.0, n_buffer : 200000, eps : 49.0%\n",
            "train 229\n",
            "n_episode :229, score : -1292.0, n_buffer : 200000, eps : 48.8%\n",
            "train 230\n",
            "n_episode :230, score : -1457.0, n_buffer : 200000, eps : 48.6%\n",
            "train 231\n",
            "n_episode :231, score : -628.0, n_buffer : 200000, eps : 48.5%\n",
            "train 232\n",
            "n_episode :232, score : -806.0, n_buffer : 200000, eps : 48.3%\n",
            "train 233\n",
            "n_episode :233, score : -226.0, n_buffer : 200000, eps : 48.1%\n",
            "train 234\n",
            "n_episode :234, score : -654.0, n_buffer : 200000, eps : 47.9%\n",
            "train 235\n",
            "n_episode :235, score : -708.0, n_buffer : 200000, eps : 47.7%\n",
            "train 236\n",
            "n_episode :236, score : -1021.0, n_buffer : 200000, eps : 47.6%\n",
            "train 237\n",
            "n_episode :237, score : -1311.0, n_buffer : 200000, eps : 47.4%\n",
            "train 238\n",
            "n_episode :238, score : -17.0, n_buffer : 200000, eps : 47.2%\n",
            "train 239\n",
            "n_episode :239, score : -2423.0, n_buffer : 200000, eps : 47.0%\n",
            "train 240\n",
            "n_episode :240, score : -2622.0, n_buffer : 200000, eps : 46.8%\n",
            "train 241\n",
            "n_episode :241, score : -1158.0, n_buffer : 200000, eps : 46.7%\n",
            "train 242\n",
            "n_episode :242, score : -123.0, n_buffer : 200000, eps : 46.5%\n",
            "train 243\n",
            "n_episode :243, score : -3578.0, n_buffer : 200000, eps : 46.3%\n",
            "train 244\n",
            "n_episode :244, score : -2427.0, n_buffer : 200000, eps : 46.1%\n",
            "train 245\n",
            "n_episode :245, score : -1380.0, n_buffer : 200000, eps : 45.9%\n",
            "train 246\n",
            "n_episode :246, score : -267.0, n_buffer : 200000, eps : 45.8%\n",
            "train 247\n",
            "n_episode :247, score : -1643.0, n_buffer : 200000, eps : 45.6%\n",
            "train 248\n",
            "n_episode :248, score : -1858.0, n_buffer : 200000, eps : 45.4%\n",
            "train 249\n",
            "n_episode :249, score : -128.0, n_buffer : 200000, eps : 45.2%\n",
            "train 250\n",
            "n_episode :250, score : -392.0, n_buffer : 200000, eps : 45.1%\n",
            "train 251\n",
            "n_episode :251, score : -201.0, n_buffer : 200000, eps : 44.9%\n",
            "train 252\n",
            "n_episode :252, score : -514.0, n_buffer : 200000, eps : 44.7%\n",
            "train 253\n",
            "n_episode :253, score : -358.0, n_buffer : 200000, eps : 44.5%\n",
            "train 254\n",
            "n_episode :254, score : -3229.0, n_buffer : 200000, eps : 44.3%\n",
            "train 255\n",
            "n_episode :255, score : -88.0, n_buffer : 200000, eps : 44.2%\n",
            "train 256\n",
            "n_episode :256, score : -1894.0, n_buffer : 200000, eps : 44.0%\n",
            "train 257\n",
            "n_episode :257, score : -1211.0, n_buffer : 200000, eps : 43.8%\n",
            "train 258\n",
            "n_episode :258, score : -1284.0, n_buffer : 200000, eps : 43.6%\n",
            "train 259\n",
            "n_episode :259, score : -1978.0, n_buffer : 200000, eps : 43.4%\n",
            "train 260\n",
            "n_episode :260, score : -1166.0, n_buffer : 200000, eps : 43.3%\n",
            "train 261\n",
            "n_episode :261, score : -787.0, n_buffer : 200000, eps : 43.1%\n",
            "train 262\n",
            "n_episode :262, score : -806.0, n_buffer : 200000, eps : 42.9%\n",
            "train 263\n",
            "n_episode :263, score : -960.0, n_buffer : 200000, eps : 42.7%\n",
            "train 264\n",
            "n_episode :264, score : -367.0, n_buffer : 200000, eps : 42.5%\n",
            "train 265\n",
            "n_episode :265, score : -658.0, n_buffer : 200000, eps : 42.4%\n",
            "train 266\n",
            "n_episode :266, score : -56.0, n_buffer : 200000, eps : 42.2%\n",
            "train 267\n",
            "n_episode :267, score : -1151.0, n_buffer : 200000, eps : 42.0%\n",
            "train 268\n",
            "n_episode :268, score : -770.0, n_buffer : 200000, eps : 41.8%\n",
            "train 269\n",
            "n_episode :269, score : -3511.0, n_buffer : 200000, eps : 41.6%\n",
            "train 270\n",
            "n_episode :270, score : -984.0, n_buffer : 200000, eps : 41.5%\n",
            "train 271\n",
            "n_episode :271, score : -2282.0, n_buffer : 200000, eps : 41.3%\n",
            "train 272\n",
            "n_episode :272, score : -292.0, n_buffer : 200000, eps : 41.1%\n",
            "train 273\n",
            "n_episode :273, score : -1517.0, n_buffer : 200000, eps : 40.9%\n",
            "train 274\n",
            "n_episode :274, score : -206.0, n_buffer : 200000, eps : 40.7%\n",
            "train 275\n",
            "n_episode :275, score : -2489.0, n_buffer : 200000, eps : 40.6%\n",
            "train 276\n",
            "n_episode :276, score : -633.0, n_buffer : 200000, eps : 40.4%\n",
            "train 277\n",
            "n_episode :277, score : -226.0, n_buffer : 200000, eps : 40.2%\n",
            "train 278\n",
            "n_episode :278, score : -597.0, n_buffer : 200000, eps : 40.0%\n",
            "train 279\n",
            "n_episode :279, score : -93.0, n_buffer : 200000, eps : 39.8%\n",
            "train 280\n",
            "n_episode :280, score : -955.0, n_buffer : 200000, eps : 39.7%\n",
            "train 281\n",
            "n_episode :281, score : -853.0, n_buffer : 200000, eps : 39.5%\n",
            "train 282\n",
            "n_episode :282, score : -1390.0, n_buffer : 200000, eps : 39.3%\n",
            "train 283\n",
            "n_episode :283, score : -3644.0, n_buffer : 200000, eps : 39.1%\n",
            "train 284\n",
            "n_episode :284, score : -562.0, n_buffer : 200000, eps : 38.9%\n",
            "train 285\n",
            "n_episode :285, score : -7919.0, n_buffer : 200000, eps : 38.8%\n",
            "train 286\n",
            "n_episode :286, score : -1249.0, n_buffer : 200000, eps : 38.6%\n",
            "train 287\n",
            "n_episode :287, score : -1595.0, n_buffer : 200000, eps : 38.4%\n",
            "train 288\n",
            "n_episode :288, score : -1495.0, n_buffer : 200000, eps : 38.2%\n",
            "train 289\n",
            "n_episode :289, score : -630.0, n_buffer : 200000, eps : 38.0%\n",
            "train 290\n",
            "n_episode :290, score : -131.0, n_buffer : 200000, eps : 37.9%\n",
            "train 291\n",
            "n_episode :291, score : -1263.0, n_buffer : 200000, eps : 37.7%\n",
            "train 292\n",
            "n_episode :292, score : -161.0, n_buffer : 200000, eps : 37.5%\n",
            "train 293\n",
            "n_episode :293, score : -139.0, n_buffer : 200000, eps : 37.3%\n",
            "train 294\n",
            "n_episode :294, score : -1686.0, n_buffer : 200000, eps : 37.1%\n",
            "train 295\n",
            "n_episode :295, score : -40.0, n_buffer : 200000, eps : 37.0%\n",
            "train 296\n",
            "n_episode :296, score : -1092.0, n_buffer : 200000, eps : 36.8%\n",
            "train 297\n",
            "n_episode :297, score : -239.0, n_buffer : 200000, eps : 36.6%\n",
            "train 298\n",
            "n_episode :298, score : -3068.0, n_buffer : 200000, eps : 36.4%\n",
            "train 299\n",
            "n_episode :299, score : -397.0, n_buffer : 200000, eps : 36.2%\n",
            "train 300\n",
            "n_episode :300, score : -311.0, n_buffer : 200000, eps : 36.1%\n",
            "train 301\n",
            "n_episode :301, score : -421.0, n_buffer : 200000, eps : 35.9%\n",
            "train 302\n",
            "n_episode :302, score : -995.0, n_buffer : 200000, eps : 35.7%\n",
            "train 303\n",
            "n_episode :303, score : -915.0, n_buffer : 200000, eps : 35.5%\n",
            "train 304\n",
            "n_episode :304, score : -270.0, n_buffer : 200000, eps : 35.3%\n",
            "train 305\n",
            "n_episode :305, score : -2700.0, n_buffer : 200000, eps : 35.2%\n",
            "train 306\n",
            "n_episode :306, score : -408.0, n_buffer : 200000, eps : 35.0%\n",
            "train 307\n",
            "n_episode :307, score : -255.0, n_buffer : 200000, eps : 34.8%\n",
            "train 308\n",
            "n_episode :308, score : -196.0, n_buffer : 200000, eps : 34.6%\n",
            "train 309\n",
            "n_episode :309, score : -426.0, n_buffer : 200000, eps : 34.4%\n",
            "train 310\n",
            "n_episode :310, score : -758.0, n_buffer : 200000, eps : 34.3%\n",
            "train 311\n",
            "n_episode :311, score : -276.0, n_buffer : 200000, eps : 34.1%\n",
            "train 312\n",
            "n_episode :312, score : -1033.0, n_buffer : 200000, eps : 33.9%\n",
            "train 313\n",
            "n_episode :313, score : -25.0, n_buffer : 200000, eps : 33.7%\n",
            "train 314\n",
            "n_episode :314, score : -415.0, n_buffer : 200000, eps : 33.5%\n",
            "train 315\n",
            "n_episode :315, score : -254.0, n_buffer : 200000, eps : 33.4%\n",
            "train 316\n",
            "n_episode :316, score : -434.0, n_buffer : 200000, eps : 33.2%\n",
            "train 317\n",
            "n_episode :317, score : -769.0, n_buffer : 200000, eps : 33.0%\n",
            "train 318\n",
            "n_episode :318, score : -656.0, n_buffer : 200000, eps : 32.8%\n",
            "train 319\n",
            "n_episode :319, score : -1960.0, n_buffer : 200000, eps : 32.6%\n",
            "train 320\n",
            "n_episode :320, score : -576.0, n_buffer : 200000, eps : 32.5%\n",
            "train 321\n",
            "n_episode :321, score : -2181.0, n_buffer : 200000, eps : 32.3%\n",
            "train 322\n",
            "n_episode :322, score : -9189.0, n_buffer : 200000, eps : 32.1%\n",
            "train 323\n",
            "n_episode :323, score : -438.0, n_buffer : 200000, eps : 31.9%\n",
            "train 324\n",
            "n_episode :324, score : -654.0, n_buffer : 200000, eps : 31.7%\n",
            "train 325\n",
            "n_episode :325, score : -875.0, n_buffer : 200000, eps : 31.6%\n",
            "train 326\n",
            "n_episode :326, score : -283.0, n_buffer : 200000, eps : 31.4%\n",
            "train 327\n",
            "n_episode :327, score : -1142.0, n_buffer : 200000, eps : 31.2%\n",
            "train 328\n",
            "n_episode :328, score : -457.0, n_buffer : 200000, eps : 31.0%\n",
            "train 329\n",
            "n_episode :329, score : -1735.0, n_buffer : 200000, eps : 30.8%\n",
            "train 330\n",
            "n_episode :330, score : -29.0, n_buffer : 200000, eps : 30.7%\n",
            "train 331\n",
            "n_episode :331, score : -3817.0, n_buffer : 200000, eps : 30.5%\n",
            "train 332\n",
            "n_episode :332, score : -1416.0, n_buffer : 200000, eps : 30.3%\n",
            "train 333\n",
            "n_episode :333, score : -136.0, n_buffer : 200000, eps : 30.1%\n",
            "train 334\n",
            "n_episode :334, score : -514.0, n_buffer : 200000, eps : 29.9%\n",
            "train 335\n",
            "n_episode :335, score : -984.0, n_buffer : 200000, eps : 29.8%\n",
            "train 336\n",
            "n_episode :336, score : -1032.0, n_buffer : 200000, eps : 29.6%\n",
            "train 337\n",
            "n_episode :337, score : -433.0, n_buffer : 200000, eps : 29.4%\n",
            "train 338\n",
            "n_episode :338, score : -356.0, n_buffer : 200000, eps : 29.2%\n",
            "train 339\n",
            "n_episode :339, score : -3531.0, n_buffer : 200000, eps : 29.0%\n",
            "train 340\n",
            "n_episode :340, score : -1458.0, n_buffer : 200000, eps : 28.9%\n",
            "train 341\n",
            "n_episode :341, score : -390.0, n_buffer : 200000, eps : 28.7%\n",
            "train 342\n",
            "n_episode :342, score : -2860.0, n_buffer : 200000, eps : 28.5%\n",
            "train 343\n",
            "n_episode :343, score : -1648.0, n_buffer : 200000, eps : 28.3%\n",
            "train 344\n",
            "n_episode :344, score : -142.0, n_buffer : 200000, eps : 28.1%\n",
            "train 345\n",
            "n_episode :345, score : -1113.0, n_buffer : 200000, eps : 28.0%\n",
            "train 346\n",
            "n_episode :346, score : -66.0, n_buffer : 200000, eps : 27.8%\n",
            "train 347\n",
            "n_episode :347, score : -3745.0, n_buffer : 200000, eps : 27.6%\n",
            "train 348\n",
            "n_episode :348, score : -927.0, n_buffer : 200000, eps : 27.4%\n",
            "train 349\n",
            "n_episode :349, score : -102.0, n_buffer : 200000, eps : 27.2%\n",
            "train 350\n",
            "n_episode :350, score : -2203.0, n_buffer : 200000, eps : 27.1%\n",
            "train 351\n",
            "n_episode :351, score : -235.0, n_buffer : 200000, eps : 26.9%\n",
            "train 352\n",
            "n_episode :352, score : -1042.0, n_buffer : 200000, eps : 26.7%\n",
            "train 353\n",
            "n_episode :353, score : -1354.0, n_buffer : 200000, eps : 26.5%\n",
            "train 354\n",
            "n_episode :354, score : -705.0, n_buffer : 200000, eps : 26.4%\n",
            "train 355\n",
            "n_episode :355, score : -261.0, n_buffer : 200000, eps : 26.2%\n",
            "train 356\n",
            "n_episode :356, score : -6.0, n_buffer : 200000, eps : 26.0%\n",
            "train 357\n",
            "n_episode :357, score : -1516.0, n_buffer : 200000, eps : 25.8%\n",
            "train 358\n",
            "n_episode :358, score : -273.0, n_buffer : 200000, eps : 25.6%\n",
            "train 359\n",
            "n_episode :359, score : -12.0, n_buffer : 200000, eps : 25.5%\n",
            "train 360\n",
            "n_episode :360, score : -920.0, n_buffer : 200000, eps : 25.3%\n",
            "train 361\n",
            "n_episode :361, score : -712.0, n_buffer : 200000, eps : 25.1%\n",
            "train 362\n",
            "n_episode :362, score : -6514.0, n_buffer : 200000, eps : 24.9%\n",
            "train 363\n",
            "n_episode :363, score : -2760.0, n_buffer : 200000, eps : 24.7%\n",
            "train 364\n",
            "n_episode :364, score : -392.0, n_buffer : 200000, eps : 24.6%\n",
            "train 365\n",
            "n_episode :365, score : -719.0, n_buffer : 200000, eps : 24.4%\n",
            "train 366\n",
            "n_episode :366, score : -7148.0, n_buffer : 200000, eps : 24.2%\n",
            "train 367\n",
            "n_episode :367, score : -632.0, n_buffer : 200000, eps : 24.0%\n",
            "train 368\n",
            "n_episode :368, score : -573.0, n_buffer : 200000, eps : 23.8%\n",
            "train 369\n",
            "n_episode :369, score : -374.0, n_buffer : 200000, eps : 23.7%\n",
            "train 370\n",
            "n_episode :370, score : -2246.0, n_buffer : 200000, eps : 23.5%\n",
            "train 371\n",
            "n_episode :371, score : -202.0, n_buffer : 200000, eps : 23.3%\n",
            "train 372\n",
            "n_episode :372, score : -506.0, n_buffer : 200000, eps : 23.1%\n",
            "train 373\n",
            "n_episode :373, score : -1917.0, n_buffer : 200000, eps : 22.9%\n",
            "train 374\n",
            "n_episode :374, score : -1711.0, n_buffer : 200000, eps : 22.8%\n",
            "train 375\n",
            "n_episode :375, score : -715.0, n_buffer : 200000, eps : 22.6%\n",
            "train 376\n",
            "n_episode :376, score : -668.0, n_buffer : 200000, eps : 22.4%\n",
            "train 377\n",
            "n_episode :377, score : -1810.0, n_buffer : 200000, eps : 22.2%\n",
            "train 378\n",
            "n_episode :378, score : -874.0, n_buffer : 200000, eps : 22.0%\n",
            "train 379\n",
            "n_episode :379, score : -373.0, n_buffer : 200000, eps : 21.9%\n",
            "train 380\n",
            "n_episode :380, score : -1647.0, n_buffer : 200000, eps : 21.7%\n",
            "train 381\n",
            "n_episode :381, score : -1285.0, n_buffer : 200000, eps : 21.5%\n",
            "train 382\n",
            "n_episode :382, score : -1286.0, n_buffer : 200000, eps : 21.3%\n",
            "train 383\n",
            "n_episode :383, score : -977.0, n_buffer : 200000, eps : 21.1%\n",
            "train 384\n",
            "n_episode :384, score : -125.0, n_buffer : 200000, eps : 21.0%\n",
            "train 385\n",
            "n_episode :385, score : -582.0, n_buffer : 200000, eps : 20.8%\n",
            "train 386\n",
            "n_episode :386, score : -626.0, n_buffer : 200000, eps : 20.6%\n",
            "train 387\n",
            "n_episode :387, score : -842.0, n_buffer : 200000, eps : 20.4%\n",
            "train 388\n",
            "n_episode :388, score : -152.0, n_buffer : 200000, eps : 20.2%\n",
            "train 389\n",
            "n_episode :389, score : -2327.0, n_buffer : 200000, eps : 20.1%\n",
            "train 390\n",
            "n_episode :390, score : -188.0, n_buffer : 200000, eps : 19.9%\n",
            "train 391\n",
            "n_episode :391, score : -2006.0, n_buffer : 200000, eps : 19.7%\n",
            "train 392\n",
            "n_episode :392, score : -2138.0, n_buffer : 200000, eps : 19.5%\n",
            "train 393\n",
            "n_episode :393, score : -840.0, n_buffer : 200000, eps : 19.3%\n",
            "train 394\n",
            "n_episode :394, score : -250.0, n_buffer : 200000, eps : 19.2%\n",
            "train 395\n",
            "n_episode :395, score : -956.0, n_buffer : 200000, eps : 19.0%\n",
            "train 396\n",
            "n_episode :396, score : -454.0, n_buffer : 200000, eps : 18.8%\n",
            "train 397\n",
            "n_episode :397, score : -1274.0, n_buffer : 200000, eps : 18.6%\n",
            "train 398\n",
            "n_episode :398, score : -395.0, n_buffer : 200000, eps : 18.4%\n",
            "train 399\n",
            "n_episode :399, score : -591.0, n_buffer : 200000, eps : 18.3%\n",
            "train 400\n",
            "n_episode :400, score : -540.0, n_buffer : 200000, eps : 18.1%\n",
            "train 401\n",
            "n_episode :401, score : -84.0, n_buffer : 200000, eps : 17.9%\n",
            "train 402\n",
            "n_episode :402, score : -17.0, n_buffer : 200000, eps : 17.7%\n",
            "train 403\n",
            "n_episode :403, score : -3830.0, n_buffer : 200000, eps : 17.5%\n",
            "train 404\n",
            "n_episode :404, score : -60.0, n_buffer : 200000, eps : 17.4%\n",
            "train 405\n",
            "n_episode :405, score : -3267.0, n_buffer : 200000, eps : 17.2%\n",
            "train 406\n",
            "n_episode :406, score : -783.0, n_buffer : 200000, eps : 17.0%\n",
            "train 407\n",
            "n_episode :407, score : -864.0, n_buffer : 200000, eps : 16.8%\n",
            "train 408\n",
            "n_episode :408, score : -1169.0, n_buffer : 200000, eps : 16.6%\n",
            "train 409\n",
            "n_episode :409, score : -35.0, n_buffer : 200000, eps : 16.5%\n",
            "train 410\n",
            "n_episode :410, score : -423.0, n_buffer : 200000, eps : 16.3%\n",
            "train 411\n",
            "n_episode :411, score : -2075.0, n_buffer : 200000, eps : 16.1%\n",
            "train 412\n",
            "n_episode :412, score : -644.0, n_buffer : 200000, eps : 15.9%\n",
            "train 413\n",
            "n_episode :413, score : -4230.0, n_buffer : 200000, eps : 15.7%\n",
            "train 414\n",
            "n_episode :414, score : -2887.0, n_buffer : 200000, eps : 15.6%\n",
            "train 415\n",
            "n_episode :415, score : -756.0, n_buffer : 200000, eps : 15.4%\n",
            "train 416\n",
            "n_episode :416, score : -1211.0, n_buffer : 200000, eps : 15.2%\n",
            "train 417\n",
            "n_episode :417, score : -1136.0, n_buffer : 200000, eps : 15.0%\n",
            "train 418\n",
            "n_episode :418, score : -51.0, n_buffer : 200000, eps : 14.8%\n",
            "train 419\n",
            "n_episode :419, score : -752.0, n_buffer : 200000, eps : 14.7%\n",
            "train 420\n",
            "n_episode :420, score : -5053.0, n_buffer : 200000, eps : 14.5%\n",
            "train 421\n",
            "n_episode :421, score : -863.0, n_buffer : 200000, eps : 14.3%\n",
            "train 422\n",
            "n_episode :422, score : -408.0, n_buffer : 200000, eps : 14.1%\n",
            "train 423\n",
            "n_episode :423, score : -1033.0, n_buffer : 200000, eps : 13.9%\n",
            "train 424\n",
            "n_episode :424, score : -182.0, n_buffer : 200000, eps : 13.8%\n",
            "train 425\n",
            "n_episode :425, score : -2468.0, n_buffer : 200000, eps : 13.6%\n",
            "train 426\n",
            "n_episode :426, score : -2216.0, n_buffer : 200000, eps : 13.4%\n",
            "train 427\n",
            "n_episode :427, score : -1099.0, n_buffer : 200000, eps : 13.2%\n",
            "train 428\n",
            "n_episode :428, score : -193.0, n_buffer : 200000, eps : 13.0%\n",
            "train 429\n",
            "n_episode :429, score : -500.0, n_buffer : 200000, eps : 12.9%\n",
            "train 430\n",
            "n_episode :430, score : -1318.0, n_buffer : 200000, eps : 12.7%\n",
            "train 431\n",
            "n_episode :431, score : -3766.0, n_buffer : 200000, eps : 12.5%\n",
            "train 432\n",
            "n_episode :432, score : -5447.0, n_buffer : 200000, eps : 12.3%\n",
            "train 433\n",
            "n_episode :433, score : -1472.0, n_buffer : 200000, eps : 12.1%\n",
            "train 434\n",
            "n_episode :434, score : -7767.0, n_buffer : 200000, eps : 12.0%\n",
            "train 435\n",
            "n_episode :435, score : -1041.0, n_buffer : 200000, eps : 11.8%\n",
            "train 436\n",
            "n_episode :436, score : -1283.0, n_buffer : 200000, eps : 11.6%\n",
            "train 437\n",
            "n_episode :437, score : -1033.0, n_buffer : 200000, eps : 11.4%\n",
            "train 438\n",
            "n_episode :438, score : -1452.0, n_buffer : 200000, eps : 11.2%\n",
            "train 439\n",
            "n_episode :439, score : -221.0, n_buffer : 200000, eps : 11.1%\n",
            "train 440\n",
            "n_episode :440, score : -14.0, n_buffer : 200000, eps : 10.9%\n",
            "train 441\n",
            "n_episode :441, score : -323.0, n_buffer : 200000, eps : 10.7%\n",
            "train 442\n",
            "n_episode :442, score : -339.0, n_buffer : 200000, eps : 10.5%\n",
            "train 443\n",
            "n_episode :443, score : -67.0, n_buffer : 200000, eps : 10.3%\n",
            "train 444\n",
            "n_episode :444, score : -1221.0, n_buffer : 200000, eps : 10.2%\n",
            "train 445\n",
            "n_episode :445, score : -307.0, n_buffer : 200000, eps : 10.0%\n",
            "train 446\n",
            "n_episode :446, score : -733.0, n_buffer : 200000, eps : 9.8%\n",
            "train 447\n",
            "n_episode :447, score : -1472.0, n_buffer : 200000, eps : 9.6%\n",
            "train 448\n",
            "n_episode :448, score : -6162.0, n_buffer : 200000, eps : 9.4%\n",
            "train 449\n",
            "n_episode :449, score : -553.0, n_buffer : 200000, eps : 9.3%\n",
            "train 450\n",
            "n_episode :450, score : -1039.0, n_buffer : 200000, eps : 9.1%\n",
            "train 451\n",
            "n_episode :451, score : -999.0, n_buffer : 200000, eps : 8.9%\n",
            "train 452\n",
            "n_episode :452, score : -908.0, n_buffer : 200000, eps : 8.7%\n",
            "train 453\n",
            "n_episode :453, score : -932.0, n_buffer : 200000, eps : 8.6%\n",
            "train 454\n",
            "n_episode :454, score : -508.0, n_buffer : 200000, eps : 8.4%\n",
            "train 455\n",
            "n_episode :455, score : -573.0, n_buffer : 200000, eps : 8.2%\n",
            "train 456\n",
            "n_episode :456, score : -128.0, n_buffer : 200000, eps : 8.0%\n",
            "train 457\n",
            "n_episode :457, score : -194.0, n_buffer : 200000, eps : 7.8%\n",
            "train 458\n",
            "n_episode :458, score : -2034.0, n_buffer : 200000, eps : 7.7%\n",
            "train 459\n",
            "n_episode :459, score : -306.0, n_buffer : 200000, eps : 7.5%\n",
            "train 460\n",
            "n_episode :460, score : -598.0, n_buffer : 200000, eps : 7.3%\n",
            "train 461\n",
            "n_episode :461, score : -789.0, n_buffer : 200000, eps : 7.1%\n",
            "train 462\n",
            "n_episode :462, score : -1297.0, n_buffer : 200000, eps : 6.9%\n",
            "train 463\n",
            "n_episode :463, score : -966.0, n_buffer : 200000, eps : 6.8%\n",
            "train 464\n",
            "n_episode :464, score : -477.0, n_buffer : 200000, eps : 6.6%\n",
            "train 465\n",
            "n_episode :465, score : -1563.0, n_buffer : 200000, eps : 6.4%\n",
            "train 466\n",
            "n_episode :466, score : -157.0, n_buffer : 200000, eps : 6.2%\n",
            "train 467\n",
            "n_episode :467, score : -93.0, n_buffer : 200000, eps : 6.0%\n",
            "train 468\n",
            "n_episode :468, score : -813.0, n_buffer : 200000, eps : 5.9%\n",
            "train 469\n",
            "n_episode :469, score : -97.0, n_buffer : 200000, eps : 5.7%\n",
            "train 470\n",
            "n_episode :470, score : -1132.0, n_buffer : 200000, eps : 5.5%\n",
            "train 471\n",
            "n_episode :471, score : -167.0, n_buffer : 200000, eps : 5.3%\n",
            "train 472\n",
            "n_episode :472, score : -423.0, n_buffer : 200000, eps : 5.1%\n",
            "train 473\n",
            "n_episode :473, score : -1702.0, n_buffer : 200000, eps : 5.0%\n",
            "train 474\n",
            "n_episode :474, score : -289.0, n_buffer : 200000, eps : 4.8%\n",
            "train 475\n",
            "n_episode :475, score : -47.0, n_buffer : 200000, eps : 4.6%\n",
            "train 476\n",
            "n_episode :476, score : -597.0, n_buffer : 200000, eps : 4.4%\n",
            "train 477\n",
            "n_episode :477, score : -591.0, n_buffer : 200000, eps : 4.2%\n",
            "train 478\n",
            "n_episode :478, score : -237.0, n_buffer : 200000, eps : 4.1%\n",
            "train 479\n",
            "n_episode :479, score : -150.0, n_buffer : 200000, eps : 3.9%\n",
            "train 480\n",
            "n_episode :480, score : -1625.0, n_buffer : 200000, eps : 3.7%\n",
            "train 481\n",
            "n_episode :481, score : -1029.0, n_buffer : 200000, eps : 3.5%\n",
            "train 482\n",
            "n_episode :482, score : -9162.0, n_buffer : 200000, eps : 3.3%\n",
            "train 483\n",
            "n_episode :483, score : -2021.0, n_buffer : 200000, eps : 3.2%\n",
            "train 484\n",
            "n_episode :484, score : -181.0, n_buffer : 200000, eps : 3.0%\n",
            "train 485\n",
            "n_episode :485, score : -38.0, n_buffer : 200000, eps : 2.8%\n",
            "train 486\n",
            "n_episode :486, score : -524.0, n_buffer : 200000, eps : 2.6%\n",
            "train 487\n",
            "n_episode :487, score : -2529.0, n_buffer : 200000, eps : 2.4%\n",
            "train 488\n",
            "n_episode :488, score : -350.0, n_buffer : 200000, eps : 2.3%\n",
            "train 489\n",
            "n_episode :489, score : -2675.0, n_buffer : 200000, eps : 2.1%\n",
            "train 490\n",
            "n_episode :490, score : -461.0, n_buffer : 200000, eps : 1.9%\n",
            "train 491\n",
            "n_episode :491, score : -1938.0, n_buffer : 200000, eps : 1.7%\n",
            "train 492\n",
            "n_episode :492, score : -195.0, n_buffer : 200000, eps : 1.5%\n",
            "train 493\n",
            "n_episode :493, score : -728.0, n_buffer : 200000, eps : 1.4%\n",
            "train 494\n",
            "n_episode :494, score : -523.0, n_buffer : 200000, eps : 1.2%\n",
            "train 495\n",
            "n_episode :495, score : -718.0, n_buffer : 200000, eps : 1.0%\n",
            "train 496\n",
            "n_episode :496, score : -1013.0, n_buffer : 200000, eps : 0.8%\n",
            "train 497\n",
            "n_episode :497, score : -1567.0, n_buffer : 200000, eps : 0.6%\n",
            "train 498\n",
            "n_episode :498, score : -1014.0, n_buffer : 200000, eps : 0.5%\n",
            "train 499\n",
            "n_episode :499, score : -1660.0, n_buffer : 200000, eps : 0.3%\n",
            "train 500\n",
            "n_episode :500, score : -1614.0, n_buffer : 200000, eps : 0.1%\n",
            "train 501\n",
            "n_episode :501, score : -556.0, n_buffer : 200000, eps : 0.1%\n",
            "train 502\n",
            "n_episode :502, score : -1508.0, n_buffer : 200000, eps : 0.1%\n",
            "train 503\n",
            "n_episode :503, score : -928.0, n_buffer : 200000, eps : 0.1%\n",
            "train 504\n",
            "n_episode :504, score : -343.0, n_buffer : 200000, eps : 0.1%\n",
            "train 505\n",
            "n_episode :505, score : -3593.0, n_buffer : 200000, eps : 0.1%\n",
            "train 506\n",
            "n_episode :506, score : -561.0, n_buffer : 200000, eps : 0.1%\n",
            "train 507\n",
            "n_episode :507, score : -605.0, n_buffer : 200000, eps : 0.1%\n",
            "train 508\n",
            "n_episode :508, score : -848.0, n_buffer : 200000, eps : 0.1%\n",
            "train 509\n",
            "n_episode :509, score : -1277.0, n_buffer : 200000, eps : 0.1%\n",
            "train 510\n",
            "n_episode :510, score : -1373.0, n_buffer : 200000, eps : 0.1%\n",
            "train 511\n",
            "n_episode :511, score : -202.0, n_buffer : 200000, eps : 0.1%\n",
            "train 512\n",
            "n_episode :512, score : -655.0, n_buffer : 200000, eps : 0.1%\n",
            "train 513\n",
            "n_episode :513, score : -62.0, n_buffer : 200000, eps : 0.1%\n",
            "train 514\n",
            "n_episode :514, score : -826.0, n_buffer : 200000, eps : 0.1%\n",
            "train 515\n",
            "n_episode :515, score : -967.0, n_buffer : 200000, eps : 0.1%\n",
            "train 516\n",
            "n_episode :516, score : -168.0, n_buffer : 200000, eps : 0.1%\n",
            "train 517\n",
            "n_episode :517, score : -825.0, n_buffer : 200000, eps : 0.1%\n",
            "train 518\n",
            "n_episode :518, score : -616.0, n_buffer : 200000, eps : 0.1%\n",
            "train 519\n",
            "n_episode :519, score : -853.0, n_buffer : 200000, eps : 0.1%\n",
            "train 520\n",
            "n_episode :520, score : -1234.0, n_buffer : 200000, eps : 0.1%\n",
            "train 521\n",
            "n_episode :521, score : -750.0, n_buffer : 200000, eps : 0.1%\n",
            "train 522\n",
            "n_episode :522, score : -294.0, n_buffer : 200000, eps : 0.1%\n",
            "train 523\n",
            "n_episode :523, score : -522.0, n_buffer : 200000, eps : 0.1%\n",
            "train 524\n",
            "n_episode :524, score : -289.0, n_buffer : 200000, eps : 0.1%\n",
            "train 525\n",
            "n_episode :525, score : 5.0, n_buffer : 200000, eps : 0.1%\n",
            "train 526\n",
            "n_episode :526, score : -795.0, n_buffer : 200000, eps : 0.1%\n",
            "train 527\n",
            "n_episode :527, score : -397.0, n_buffer : 200000, eps : 0.1%\n",
            "train 528\n",
            "n_episode :528, score : -1655.0, n_buffer : 200000, eps : 0.1%\n",
            "train 529\n",
            "n_episode :529, score : -2569.0, n_buffer : 200000, eps : 0.1%\n",
            "train 530\n",
            "n_episode :530, score : -414.0, n_buffer : 200000, eps : 0.1%\n",
            "train 531\n",
            "n_episode :531, score : -831.0, n_buffer : 200000, eps : 0.1%\n",
            "train 532\n",
            "n_episode :532, score : -1073.0, n_buffer : 200000, eps : 0.1%\n",
            "train 533\n",
            "n_episode :533, score : -249.0, n_buffer : 200000, eps : 0.1%\n",
            "train 534\n",
            "n_episode :534, score : -44.0, n_buffer : 200000, eps : 0.1%\n",
            "train 535\n",
            "n_episode :535, score : -1880.0, n_buffer : 200000, eps : 0.1%\n",
            "train 536\n",
            "n_episode :536, score : -160.0, n_buffer : 200000, eps : 0.1%\n",
            "train 537\n",
            "n_episode :537, score : -713.0, n_buffer : 200000, eps : 0.1%\n",
            "train 538\n",
            "n_episode :538, score : -1295.0, n_buffer : 200000, eps : 0.1%\n",
            "train 539\n",
            "n_episode :539, score : -4525.0, n_buffer : 200000, eps : 0.1%\n",
            "train 540\n",
            "n_episode :540, score : -616.0, n_buffer : 200000, eps : 0.1%\n",
            "train 541\n",
            "n_episode :541, score : -1320.0, n_buffer : 200000, eps : 0.1%\n",
            "train 542\n",
            "n_episode :542, score : -224.0, n_buffer : 200000, eps : 0.1%\n",
            "train 543\n",
            "n_episode :543, score : -256.0, n_buffer : 200000, eps : 0.1%\n",
            "train 544\n",
            "n_episode :544, score : -856.0, n_buffer : 200000, eps : 0.1%\n",
            "train 545\n",
            "n_episode :545, score : -26.0, n_buffer : 200000, eps : 0.1%\n",
            "train 546\n",
            "n_episode :546, score : -5232.0, n_buffer : 200000, eps : 0.1%\n",
            "train 547\n",
            "n_episode :547, score : -298.0, n_buffer : 200000, eps : 0.1%\n",
            "train 548\n",
            "n_episode :548, score : -592.0, n_buffer : 200000, eps : 0.1%\n",
            "train 549\n",
            "n_episode :549, score : -1002.0, n_buffer : 200000, eps : 0.1%\n",
            "train 550\n",
            "n_episode :550, score : -222.0, n_buffer : 200000, eps : 0.1%\n",
            "train 551\n",
            "n_episode :551, score : -439.0, n_buffer : 200000, eps : 0.1%\n",
            "train 552\n",
            "n_episode :552, score : -3390.0, n_buffer : 200000, eps : 0.1%\n",
            "train 553\n",
            "n_episode :553, score : -1027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 554\n",
            "n_episode :554, score : -1255.0, n_buffer : 200000, eps : 0.1%\n",
            "train 555\n",
            "n_episode :555, score : -113.0, n_buffer : 200000, eps : 0.1%\n",
            "train 556\n",
            "n_episode :556, score : -115.0, n_buffer : 200000, eps : 0.1%\n",
            "train 557\n",
            "n_episode :557, score : -44.0, n_buffer : 200000, eps : 0.1%\n",
            "train 558\n",
            "n_episode :558, score : -1280.0, n_buffer : 200000, eps : 0.1%\n",
            "train 559\n",
            "n_episode :559, score : -1369.0, n_buffer : 200000, eps : 0.1%\n",
            "train 560\n",
            "n_episode :560, score : -494.0, n_buffer : 200000, eps : 0.1%\n",
            "train 561\n",
            "n_episode :561, score : -479.0, n_buffer : 200000, eps : 0.1%\n",
            "train 562\n",
            "n_episode :562, score : -2029.0, n_buffer : 200000, eps : 0.1%\n",
            "train 563\n",
            "n_episode :563, score : -636.0, n_buffer : 200000, eps : 0.1%\n",
            "train 564\n",
            "n_episode :564, score : -282.0, n_buffer : 200000, eps : 0.1%\n",
            "train 565\n",
            "n_episode :565, score : -43.0, n_buffer : 200000, eps : 0.1%\n",
            "train 566\n",
            "n_episode :566, score : -442.0, n_buffer : 200000, eps : 0.1%\n",
            "train 567\n",
            "n_episode :567, score : -1142.0, n_buffer : 200000, eps : 0.1%\n",
            "train 568\n",
            "n_episode :568, score : -766.0, n_buffer : 200000, eps : 0.1%\n",
            "train 569\n",
            "n_episode :569, score : -987.0, n_buffer : 200000, eps : 0.1%\n",
            "train 570\n",
            "n_episode :570, score : -549.0, n_buffer : 200000, eps : 0.1%\n",
            "train 571\n",
            "n_episode :571, score : -318.0, n_buffer : 200000, eps : 0.1%\n",
            "train 572\n",
            "n_episode :572, score : -1817.0, n_buffer : 200000, eps : 0.1%\n",
            "train 573\n",
            "n_episode :573, score : -1123.0, n_buffer : 200000, eps : 0.1%\n",
            "train 574\n",
            "n_episode :574, score : -1729.0, n_buffer : 200000, eps : 0.1%\n",
            "train 575\n",
            "n_episode :575, score : -1417.0, n_buffer : 200000, eps : 0.1%\n",
            "train 576\n",
            "n_episode :576, score : -1498.0, n_buffer : 200000, eps : 0.1%\n",
            "train 577\n",
            "n_episode :577, score : -130.0, n_buffer : 200000, eps : 0.1%\n",
            "train 578\n",
            "n_episode :578, score : -409.0, n_buffer : 200000, eps : 0.1%\n",
            "train 579\n",
            "n_episode :579, score : -535.0, n_buffer : 200000, eps : 0.1%\n",
            "train 580\n",
            "n_episode :580, score : -1832.0, n_buffer : 200000, eps : 0.1%\n",
            "train 581\n",
            "n_episode :581, score : -1217.0, n_buffer : 200000, eps : 0.1%\n",
            "train 582\n",
            "n_episode :582, score : -30.0, n_buffer : 200000, eps : 0.1%\n",
            "train 583\n",
            "n_episode :583, score : -316.0, n_buffer : 200000, eps : 0.1%\n",
            "train 584\n",
            "n_episode :584, score : -833.0, n_buffer : 200000, eps : 0.1%\n",
            "train 585\n",
            "n_episode :585, score : -795.0, n_buffer : 200000, eps : 0.1%\n",
            "train 586\n",
            "n_episode :586, score : -1038.0, n_buffer : 200000, eps : 0.1%\n",
            "train 587\n",
            "n_episode :587, score : -1483.0, n_buffer : 200000, eps : 0.1%\n",
            "train 588\n",
            "n_episode :588, score : -5942.0, n_buffer : 200000, eps : 0.1%\n",
            "train 589\n",
            "n_episode :589, score : -1381.0, n_buffer : 200000, eps : 0.1%\n",
            "train 590\n",
            "n_episode :590, score : -215.0, n_buffer : 200000, eps : 0.1%\n",
            "train 591\n",
            "n_episode :591, score : -805.0, n_buffer : 200000, eps : 0.1%\n",
            "train 592\n",
            "n_episode :592, score : -730.0, n_buffer : 200000, eps : 0.1%\n",
            "train 593\n",
            "n_episode :593, score : -804.0, n_buffer : 200000, eps : 0.1%\n",
            "train 594\n",
            "n_episode :594, score : -467.0, n_buffer : 200000, eps : 0.1%\n",
            "train 595\n",
            "n_episode :595, score : -3229.0, n_buffer : 200000, eps : 0.1%\n",
            "train 596\n",
            "n_episode :596, score : -2386.0, n_buffer : 200000, eps : 0.1%\n",
            "train 597\n",
            "n_episode :597, score : -1025.0, n_buffer : 200000, eps : 0.1%\n",
            "train 598\n",
            "n_episode :598, score : -822.0, n_buffer : 200000, eps : 0.1%\n",
            "train 599\n",
            "n_episode :599, score : -1140.0, n_buffer : 200000, eps : 0.1%\n",
            "train 600\n",
            "n_episode :600, score : -338.0, n_buffer : 200000, eps : 0.1%\n",
            "train 601\n",
            "n_episode :601, score : -1750.0, n_buffer : 200000, eps : 0.1%\n",
            "train 602\n",
            "n_episode :602, score : -2229.0, n_buffer : 200000, eps : 0.1%\n",
            "train 603\n",
            "n_episode :603, score : -1078.0, n_buffer : 200000, eps : 0.1%\n",
            "train 604\n",
            "n_episode :604, score : -250.0, n_buffer : 200000, eps : 0.1%\n",
            "train 605\n",
            "n_episode :605, score : -1253.0, n_buffer : 200000, eps : 0.1%\n",
            "train 606\n",
            "n_episode :606, score : -1532.0, n_buffer : 200000, eps : 0.1%\n",
            "train 607\n",
            "n_episode :607, score : -1015.0, n_buffer : 200000, eps : 0.1%\n",
            "train 608\n",
            "n_episode :608, score : -180.0, n_buffer : 200000, eps : 0.1%\n",
            "train 609\n",
            "n_episode :609, score : -2504.0, n_buffer : 200000, eps : 0.1%\n",
            "train 610\n",
            "n_episode :610, score : -621.0, n_buffer : 200000, eps : 0.1%\n",
            "train 611\n",
            "n_episode :611, score : -95.0, n_buffer : 200000, eps : 0.1%\n",
            "train 612\n",
            "n_episode :612, score : -2002.0, n_buffer : 200000, eps : 0.1%\n",
            "train 613\n",
            "n_episode :613, score : -433.0, n_buffer : 200000, eps : 0.1%\n",
            "train 614\n",
            "n_episode :614, score : -892.0, n_buffer : 200000, eps : 0.1%\n",
            "train 615\n",
            "n_episode :615, score : -520.0, n_buffer : 200000, eps : 0.1%\n",
            "train 616\n",
            "n_episode :616, score : -687.0, n_buffer : 200000, eps : 0.1%\n",
            "train 617\n",
            "n_episode :617, score : -694.0, n_buffer : 200000, eps : 0.1%\n",
            "train 618\n",
            "n_episode :618, score : -2128.0, n_buffer : 200000, eps : 0.1%\n",
            "train 619\n",
            "n_episode :619, score : -157.0, n_buffer : 200000, eps : 0.1%\n",
            "train 620\n",
            "n_episode :620, score : -934.0, n_buffer : 200000, eps : 0.1%\n",
            "train 621\n",
            "n_episode :621, score : -544.0, n_buffer : 200000, eps : 0.1%\n",
            "train 622\n",
            "n_episode :622, score : -2650.0, n_buffer : 200000, eps : 0.1%\n",
            "train 623\n",
            "n_episode :623, score : -94.0, n_buffer : 200000, eps : 0.1%\n",
            "train 624\n",
            "n_episode :624, score : -2130.0, n_buffer : 200000, eps : 0.1%\n",
            "train 625\n",
            "n_episode :625, score : -502.0, n_buffer : 200000, eps : 0.1%\n",
            "train 626\n",
            "n_episode :626, score : -1101.0, n_buffer : 200000, eps : 0.1%\n",
            "train 627\n",
            "n_episode :627, score : -595.0, n_buffer : 200000, eps : 0.1%\n",
            "train 628\n",
            "n_episode :628, score : -890.0, n_buffer : 200000, eps : 0.1%\n",
            "train 629\n",
            "n_episode :629, score : -2165.0, n_buffer : 200000, eps : 0.1%\n",
            "train 630\n",
            "n_episode :630, score : -260.0, n_buffer : 200000, eps : 0.1%\n",
            "train 631\n",
            "n_episode :631, score : -1272.0, n_buffer : 200000, eps : 0.1%\n",
            "train 632\n",
            "n_episode :632, score : -204.0, n_buffer : 200000, eps : 0.1%\n",
            "train 633\n",
            "n_episode :633, score : -190.0, n_buffer : 200000, eps : 0.1%\n",
            "train 634\n",
            "n_episode :634, score : -517.0, n_buffer : 200000, eps : 0.1%\n",
            "train 635\n",
            "n_episode :635, score : -16.0, n_buffer : 200000, eps : 0.1%\n",
            "train 636\n",
            "n_episode :636, score : -965.0, n_buffer : 200000, eps : 0.1%\n",
            "train 637\n",
            "n_episode :637, score : -1559.0, n_buffer : 200000, eps : 0.1%\n",
            "train 638\n",
            "n_episode :638, score : -1416.0, n_buffer : 200000, eps : 0.1%\n",
            "train 639\n",
            "n_episode :639, score : -1579.0, n_buffer : 200000, eps : 0.1%\n",
            "train 640\n",
            "n_episode :640, score : -984.0, n_buffer : 200000, eps : 0.1%\n",
            "train 641\n",
            "n_episode :641, score : -910.0, n_buffer : 200000, eps : 0.1%\n",
            "train 642\n",
            "n_episode :642, score : -1671.0, n_buffer : 200000, eps : 0.1%\n",
            "train 643\n",
            "n_episode :643, score : -371.0, n_buffer : 200000, eps : 0.1%\n",
            "train 644\n",
            "n_episode :644, score : -1993.0, n_buffer : 200000, eps : 0.1%\n",
            "train 645\n",
            "n_episode :645, score : -41.0, n_buffer : 200000, eps : 0.1%\n",
            "train 646\n",
            "n_episode :646, score : -184.0, n_buffer : 200000, eps : 0.1%\n",
            "train 647\n",
            "n_episode :647, score : -3266.0, n_buffer : 200000, eps : 0.1%\n",
            "train 648\n",
            "n_episode :648, score : -2603.0, n_buffer : 200000, eps : 0.1%\n",
            "train 649\n",
            "n_episode :649, score : -715.0, n_buffer : 200000, eps : 0.1%\n",
            "train 650\n",
            "n_episode :650, score : -1874.0, n_buffer : 200000, eps : 0.1%\n",
            "train 651\n",
            "n_episode :651, score : -4029.0, n_buffer : 200000, eps : 0.1%\n",
            "train 652\n",
            "n_episode :652, score : -704.0, n_buffer : 200000, eps : 0.1%\n",
            "train 653\n",
            "n_episode :653, score : -2696.0, n_buffer : 200000, eps : 0.1%\n",
            "train 654\n",
            "n_episode :654, score : -434.0, n_buffer : 200000, eps : 0.1%\n",
            "train 655\n",
            "n_episode :655, score : -32.0, n_buffer : 200000, eps : 0.1%\n",
            "train 656\n",
            "n_episode :656, score : -264.0, n_buffer : 200000, eps : 0.1%\n",
            "train 657\n",
            "n_episode :657, score : -365.0, n_buffer : 200000, eps : 0.1%\n",
            "train 658\n",
            "n_episode :658, score : -278.0, n_buffer : 200000, eps : 0.1%\n",
            "train 659\n",
            "n_episode :659, score : -1851.0, n_buffer : 200000, eps : 0.1%\n",
            "train 660\n",
            "n_episode :660, score : -4673.0, n_buffer : 200000, eps : 0.1%\n",
            "train 661\n",
            "n_episode :661, score : -627.0, n_buffer : 200000, eps : 0.1%\n",
            "train 662\n",
            "n_episode :662, score : -194.0, n_buffer : 200000, eps : 0.1%\n",
            "train 663\n",
            "n_episode :663, score : -801.0, n_buffer : 200000, eps : 0.1%\n",
            "train 664\n",
            "n_episode :664, score : -1137.0, n_buffer : 200000, eps : 0.1%\n",
            "train 665\n",
            "n_episode :665, score : -969.0, n_buffer : 200000, eps : 0.1%\n",
            "train 666\n",
            "n_episode :666, score : -187.0, n_buffer : 200000, eps : 0.1%\n",
            "train 667\n",
            "n_episode :667, score : -54.0, n_buffer : 200000, eps : 0.1%\n",
            "train 668\n",
            "n_episode :668, score : -248.0, n_buffer : 200000, eps : 0.1%\n",
            "train 669\n",
            "n_episode :669, score : -116.0, n_buffer : 200000, eps : 0.1%\n",
            "train 670\n",
            "n_episode :670, score : -1144.0, n_buffer : 200000, eps : 0.1%\n",
            "train 671\n",
            "n_episode :671, score : -350.0, n_buffer : 200000, eps : 0.1%\n",
            "train 672\n",
            "n_episode :672, score : -363.0, n_buffer : 200000, eps : 0.1%\n",
            "train 673\n",
            "n_episode :673, score : -1245.0, n_buffer : 200000, eps : 0.1%\n",
            "train 674\n",
            "n_episode :674, score : -247.0, n_buffer : 200000, eps : 0.1%\n",
            "train 675\n",
            "n_episode :675, score : -858.0, n_buffer : 200000, eps : 0.1%\n",
            "train 676\n",
            "n_episode :676, score : -653.0, n_buffer : 200000, eps : 0.1%\n",
            "train 677\n",
            "n_episode :677, score : -4157.0, n_buffer : 200000, eps : 0.1%\n",
            "train 678\n",
            "n_episode :678, score : -2161.0, n_buffer : 200000, eps : 0.1%\n",
            "train 679\n",
            "n_episode :679, score : -4511.0, n_buffer : 200000, eps : 0.1%\n",
            "train 680\n",
            "n_episode :680, score : -377.0, n_buffer : 200000, eps : 0.1%\n",
            "train 681\n",
            "n_episode :681, score : -40.0, n_buffer : 200000, eps : 0.1%\n",
            "train 682\n",
            "n_episode :682, score : -154.0, n_buffer : 200000, eps : 0.1%\n",
            "train 683\n",
            "n_episode :683, score : -225.0, n_buffer : 200000, eps : 0.1%\n",
            "train 684\n",
            "n_episode :684, score : -67.0, n_buffer : 200000, eps : 0.1%\n",
            "train 685\n",
            "n_episode :685, score : -1174.0, n_buffer : 200000, eps : 0.1%\n",
            "train 686\n",
            "n_episode :686, score : -1831.0, n_buffer : 200000, eps : 0.1%\n",
            "train 687\n",
            "n_episode :687, score : -325.0, n_buffer : 200000, eps : 0.1%\n",
            "train 688\n",
            "n_episode :688, score : -72.0, n_buffer : 200000, eps : 0.1%\n",
            "train 689\n",
            "n_episode :689, score : -1174.0, n_buffer : 200000, eps : 0.1%\n",
            "train 690\n",
            "n_episode :690, score : -67.0, n_buffer : 200000, eps : 0.1%\n",
            "train 691\n",
            "n_episode :691, score : -2330.0, n_buffer : 200000, eps : 0.1%\n",
            "train 692\n",
            "n_episode :692, score : 7.0, n_buffer : 200000, eps : 0.1%\n",
            "train 693\n",
            "n_episode :693, score : -616.0, n_buffer : 200000, eps : 0.1%\n",
            "train 694\n",
            "n_episode :694, score : -251.0, n_buffer : 200000, eps : 0.1%\n",
            "train 695\n",
            "n_episode :695, score : -73.0, n_buffer : 200000, eps : 0.1%\n",
            "train 696\n",
            "n_episode :696, score : -837.0, n_buffer : 200000, eps : 0.1%\n",
            "train 697\n",
            "n_episode :697, score : -362.0, n_buffer : 200000, eps : 0.1%\n",
            "train 698\n",
            "n_episode :698, score : -303.0, n_buffer : 200000, eps : 0.1%\n",
            "train 699\n",
            "n_episode :699, score : -333.0, n_buffer : 200000, eps : 0.1%\n",
            "train 700\n",
            "n_episode :700, score : -3871.0, n_buffer : 200000, eps : 0.1%\n",
            "train 701\n",
            "n_episode :701, score : -270.0, n_buffer : 200000, eps : 0.1%\n",
            "train 702\n",
            "n_episode :702, score : -1726.0, n_buffer : 200000, eps : 0.1%\n",
            "train 703\n",
            "n_episode :703, score : -1690.0, n_buffer : 200000, eps : 0.1%\n",
            "train 704\n",
            "n_episode :704, score : -633.0, n_buffer : 200000, eps : 0.1%\n",
            "train 705\n",
            "n_episode :705, score : -957.0, n_buffer : 200000, eps : 0.1%\n",
            "train 706\n",
            "n_episode :706, score : -1435.0, n_buffer : 200000, eps : 0.1%\n",
            "train 707\n",
            "n_episode :707, score : -646.0, n_buffer : 200000, eps : 0.1%\n",
            "train 708\n",
            "n_episode :708, score : -505.0, n_buffer : 200000, eps : 0.1%\n",
            "train 709\n",
            "n_episode :709, score : -1972.0, n_buffer : 200000, eps : 0.1%\n",
            "train 710\n",
            "n_episode :710, score : -641.0, n_buffer : 200000, eps : 0.1%\n",
            "train 711\n",
            "n_episode :711, score : -1113.0, n_buffer : 200000, eps : 0.1%\n",
            "train 712\n",
            "n_episode :712, score : -5583.0, n_buffer : 200000, eps : 0.1%\n",
            "train 713\n",
            "n_episode :713, score : -704.0, n_buffer : 200000, eps : 0.1%\n",
            "train 714\n",
            "n_episode :714, score : -737.0, n_buffer : 200000, eps : 0.1%\n",
            "train 715\n",
            "n_episode :715, score : -458.0, n_buffer : 200000, eps : 0.1%\n",
            "train 716\n",
            "n_episode :716, score : -3038.0, n_buffer : 200000, eps : 0.1%\n",
            "train 717\n",
            "n_episode :717, score : -3649.0, n_buffer : 200000, eps : 0.1%\n",
            "train 718\n",
            "n_episode :718, score : -779.0, n_buffer : 200000, eps : 0.1%\n",
            "train 719\n",
            "n_episode :719, score : -3931.0, n_buffer : 200000, eps : 0.1%\n",
            "train 720\n",
            "n_episode :720, score : -58.0, n_buffer : 200000, eps : 0.1%\n",
            "train 721\n",
            "n_episode :721, score : -1025.0, n_buffer : 200000, eps : 0.1%\n",
            "train 722\n",
            "n_episode :722, score : -790.0, n_buffer : 200000, eps : 0.1%\n",
            "train 723\n",
            "n_episode :723, score : -641.0, n_buffer : 200000, eps : 0.1%\n",
            "train 724\n",
            "n_episode :724, score : -72.0, n_buffer : 200000, eps : 0.1%\n",
            "train 725\n",
            "n_episode :725, score : -4066.0, n_buffer : 200000, eps : 0.1%\n",
            "train 726\n",
            "n_episode :726, score : -1644.0, n_buffer : 200000, eps : 0.1%\n",
            "train 727\n",
            "n_episode :727, score : -605.0, n_buffer : 200000, eps : 0.1%\n",
            "train 728\n",
            "n_episode :728, score : -150.0, n_buffer : 200000, eps : 0.1%\n",
            "train 729\n",
            "n_episode :729, score : -1341.0, n_buffer : 200000, eps : 0.1%\n",
            "train 730\n",
            "n_episode :730, score : -139.0, n_buffer : 200000, eps : 0.1%\n",
            "train 731\n",
            "n_episode :731, score : -1864.0, n_buffer : 200000, eps : 0.1%\n",
            "train 732\n",
            "n_episode :732, score : -110.0, n_buffer : 200000, eps : 0.1%\n",
            "train 733\n",
            "n_episode :733, score : -2429.0, n_buffer : 200000, eps : 0.1%\n",
            "train 734\n",
            "n_episode :734, score : -213.0, n_buffer : 200000, eps : 0.1%\n",
            "train 735\n",
            "n_episode :735, score : -1185.0, n_buffer : 200000, eps : 0.1%\n",
            "train 736\n",
            "n_episode :736, score : -298.0, n_buffer : 200000, eps : 0.1%\n",
            "train 737\n",
            "n_episode :737, score : -70.0, n_buffer : 200000, eps : 0.1%\n",
            "train 738\n",
            "n_episode :738, score : -446.0, n_buffer : 200000, eps : 0.1%\n",
            "train 739\n",
            "n_episode :739, score : -1153.0, n_buffer : 200000, eps : 0.1%\n",
            "train 740\n",
            "n_episode :740, score : -438.0, n_buffer : 200000, eps : 0.1%\n",
            "train 741\n",
            "n_episode :741, score : -2582.0, n_buffer : 200000, eps : 0.1%\n",
            "train 742\n",
            "n_episode :742, score : -1206.0, n_buffer : 200000, eps : 0.1%\n",
            "train 743\n",
            "n_episode :743, score : -22.0, n_buffer : 200000, eps : 0.1%\n",
            "train 744\n",
            "n_episode :744, score : -3301.0, n_buffer : 200000, eps : 0.1%\n",
            "train 745\n",
            "n_episode :745, score : -2260.0, n_buffer : 200000, eps : 0.1%\n",
            "train 746\n",
            "n_episode :746, score : -2298.0, n_buffer : 200000, eps : 0.1%\n",
            "train 747\n",
            "n_episode :747, score : -859.0, n_buffer : 200000, eps : 0.1%\n",
            "train 748\n",
            "n_episode :748, score : -1771.0, n_buffer : 200000, eps : 0.1%\n",
            "train 749\n",
            "n_episode :749, score : -299.0, n_buffer : 200000, eps : 0.1%\n",
            "train 750\n",
            "n_episode :750, score : -1547.0, n_buffer : 200000, eps : 0.1%\n",
            "train 751\n",
            "n_episode :751, score : -395.0, n_buffer : 200000, eps : 0.1%\n",
            "train 752\n",
            "n_episode :752, score : -1898.0, n_buffer : 200000, eps : 0.1%\n",
            "train 753\n",
            "n_episode :753, score : -2801.0, n_buffer : 200000, eps : 0.1%\n",
            "train 754\n",
            "n_episode :754, score : -584.0, n_buffer : 200000, eps : 0.1%\n",
            "train 755\n",
            "n_episode :755, score : -1475.0, n_buffer : 200000, eps : 0.1%\n",
            "train 756\n",
            "n_episode :756, score : -57.0, n_buffer : 200000, eps : 0.1%\n",
            "train 757\n",
            "n_episode :757, score : -1519.0, n_buffer : 200000, eps : 0.1%\n",
            "train 758\n",
            "n_episode :758, score : -1596.0, n_buffer : 200000, eps : 0.1%\n",
            "train 759\n",
            "n_episode :759, score : -770.0, n_buffer : 200000, eps : 0.1%\n",
            "train 760\n",
            "n_episode :760, score : -111.0, n_buffer : 200000, eps : 0.1%\n",
            "train 761\n",
            "n_episode :761, score : -818.0, n_buffer : 200000, eps : 0.1%\n",
            "train 762\n",
            "n_episode :762, score : -1059.0, n_buffer : 200000, eps : 0.1%\n",
            "train 763\n",
            "n_episode :763, score : -304.0, n_buffer : 200000, eps : 0.1%\n",
            "train 764\n",
            "n_episode :764, score : -459.0, n_buffer : 200000, eps : 0.1%\n",
            "train 765\n",
            "n_episode :765, score : -702.0, n_buffer : 200000, eps : 0.1%\n",
            "train 766\n",
            "n_episode :766, score : -941.0, n_buffer : 200000, eps : 0.1%\n",
            "train 767\n",
            "n_episode :767, score : -186.0, n_buffer : 200000, eps : 0.1%\n",
            "train 768\n",
            "n_episode :768, score : -1679.0, n_buffer : 200000, eps : 0.1%\n",
            "train 769\n",
            "n_episode :769, score : -368.0, n_buffer : 200000, eps : 0.1%\n",
            "train 770\n",
            "n_episode :770, score : -2564.0, n_buffer : 200000, eps : 0.1%\n",
            "train 771\n",
            "n_episode :771, score : -6183.0, n_buffer : 200000, eps : 0.1%\n",
            "train 772\n",
            "n_episode :772, score : -1106.0, n_buffer : 200000, eps : 0.1%\n",
            "train 773\n",
            "n_episode :773, score : -712.0, n_buffer : 200000, eps : 0.1%\n",
            "train 774\n",
            "n_episode :774, score : -367.0, n_buffer : 200000, eps : 0.1%\n",
            "train 775\n",
            "n_episode :775, score : -262.0, n_buffer : 200000, eps : 0.1%\n",
            "train 776\n",
            "n_episode :776, score : -5137.0, n_buffer : 200000, eps : 0.1%\n",
            "train 777\n",
            "n_episode :777, score : -1850.0, n_buffer : 200000, eps : 0.1%\n",
            "train 778\n",
            "n_episode :778, score : -91.0, n_buffer : 200000, eps : 0.1%\n",
            "train 779\n",
            "n_episode :779, score : -410.0, n_buffer : 200000, eps : 0.1%\n",
            "train 780\n",
            "n_episode :780, score : -2162.0, n_buffer : 200000, eps : 0.1%\n",
            "train 781\n",
            "n_episode :781, score : -2065.0, n_buffer : 200000, eps : 0.1%\n",
            "train 782\n",
            "n_episode :782, score : -303.0, n_buffer : 200000, eps : 0.1%\n",
            "train 783\n",
            "n_episode :783, score : -323.0, n_buffer : 200000, eps : 0.1%\n",
            "train 784\n",
            "n_episode :784, score : -86.0, n_buffer : 200000, eps : 0.1%\n",
            "train 785\n",
            "n_episode :785, score : -626.0, n_buffer : 200000, eps : 0.1%\n",
            "train 786\n",
            "n_episode :786, score : -3242.0, n_buffer : 200000, eps : 0.1%\n",
            "train 787\n",
            "n_episode :787, score : -240.0, n_buffer : 200000, eps : 0.1%\n",
            "train 788\n",
            "n_episode :788, score : -755.0, n_buffer : 200000, eps : 0.1%\n",
            "train 789\n",
            "n_episode :789, score : -938.0, n_buffer : 200000, eps : 0.1%\n",
            "train 790\n",
            "n_episode :790, score : -1131.0, n_buffer : 200000, eps : 0.1%\n",
            "train 791\n",
            "n_episode :791, score : -540.0, n_buffer : 200000, eps : 0.1%\n",
            "train 792\n",
            "n_episode :792, score : -2335.0, n_buffer : 200000, eps : 0.1%\n",
            "train 793\n",
            "n_episode :793, score : -978.0, n_buffer : 200000, eps : 0.1%\n",
            "train 794\n",
            "n_episode :794, score : -763.0, n_buffer : 200000, eps : 0.1%\n",
            "train 795\n",
            "n_episode :795, score : -1919.0, n_buffer : 200000, eps : 0.1%\n",
            "train 796\n",
            "n_episode :796, score : -266.0, n_buffer : 200000, eps : 0.1%\n",
            "train 797\n",
            "n_episode :797, score : -807.0, n_buffer : 200000, eps : 0.1%\n",
            "train 798\n",
            "n_episode :798, score : -641.0, n_buffer : 200000, eps : 0.1%\n",
            "train 799\n",
            "n_episode :799, score : -293.0, n_buffer : 200000, eps : 0.1%\n",
            "train 800\n",
            "n_episode :800, score : -28.0, n_buffer : 200000, eps : 0.1%\n",
            "train 801\n",
            "n_episode :801, score : -336.0, n_buffer : 200000, eps : 0.1%\n",
            "train 802\n",
            "n_episode :802, score : -1627.0, n_buffer : 200000, eps : 0.1%\n",
            "train 803\n",
            "n_episode :803, score : -1003.0, n_buffer : 200000, eps : 0.1%\n",
            "train 804\n",
            "n_episode :804, score : -1314.0, n_buffer : 200000, eps : 0.1%\n",
            "train 805\n",
            "n_episode :805, score : -866.0, n_buffer : 200000, eps : 0.1%\n",
            "train 806\n",
            "n_episode :806, score : -472.0, n_buffer : 200000, eps : 0.1%\n",
            "train 807\n",
            "n_episode :807, score : -2483.0, n_buffer : 200000, eps : 0.1%\n",
            "train 808\n",
            "n_episode :808, score : -24.0, n_buffer : 200000, eps : 0.1%\n",
            "train 809\n",
            "n_episode :809, score : -1116.0, n_buffer : 200000, eps : 0.1%\n",
            "train 810\n",
            "n_episode :810, score : -1317.0, n_buffer : 200000, eps : 0.1%\n",
            "train 811\n",
            "n_episode :811, score : -25.0, n_buffer : 200000, eps : 0.1%\n",
            "train 812\n",
            "n_episode :812, score : -1856.0, n_buffer : 200000, eps : 0.1%\n",
            "train 813\n",
            "n_episode :813, score : -162.0, n_buffer : 200000, eps : 0.1%\n",
            "train 814\n",
            "n_episode :814, score : -759.0, n_buffer : 200000, eps : 0.1%\n",
            "train 815\n",
            "n_episode :815, score : -34.0, n_buffer : 200000, eps : 0.1%\n",
            "train 816\n",
            "n_episode :816, score : -133.0, n_buffer : 200000, eps : 0.1%\n",
            "train 817\n",
            "n_episode :817, score : -824.0, n_buffer : 200000, eps : 0.1%\n",
            "train 818\n",
            "n_episode :818, score : -968.0, n_buffer : 200000, eps : 0.1%\n",
            "train 819\n",
            "n_episode :819, score : -2858.0, n_buffer : 200000, eps : 0.1%\n",
            "train 820\n",
            "n_episode :820, score : -178.0, n_buffer : 200000, eps : 0.1%\n",
            "train 821\n",
            "n_episode :821, score : -1184.0, n_buffer : 200000, eps : 0.1%\n",
            "train 822\n",
            "n_episode :822, score : -532.0, n_buffer : 200000, eps : 0.1%\n",
            "train 823\n",
            "n_episode :823, score : -1650.0, n_buffer : 200000, eps : 0.1%\n",
            "train 824\n",
            "n_episode :824, score : -5431.0, n_buffer : 200000, eps : 0.1%\n",
            "train 825\n",
            "n_episode :825, score : -3651.0, n_buffer : 200000, eps : 0.1%\n",
            "train 826\n",
            "n_episode :826, score : -2160.0, n_buffer : 200000, eps : 0.1%\n",
            "train 827\n",
            "n_episode :827, score : -820.0, n_buffer : 200000, eps : 0.1%\n",
            "train 828\n",
            "n_episode :828, score : -2681.0, n_buffer : 200000, eps : 0.1%\n",
            "train 829\n",
            "n_episode :829, score : -656.0, n_buffer : 200000, eps : 0.1%\n",
            "train 830\n",
            "n_episode :830, score : -508.0, n_buffer : 200000, eps : 0.1%\n",
            "train 831\n",
            "n_episode :831, score : -1565.0, n_buffer : 200000, eps : 0.1%\n",
            "train 832\n",
            "n_episode :832, score : -76.0, n_buffer : 200000, eps : 0.1%\n",
            "train 833\n",
            "n_episode :833, score : -93.0, n_buffer : 200000, eps : 0.1%\n",
            "train 834\n",
            "n_episode :834, score : -2716.0, n_buffer : 200000, eps : 0.1%\n",
            "train 835\n",
            "n_episode :835, score : -2422.0, n_buffer : 200000, eps : 0.1%\n",
            "train 836\n",
            "n_episode :836, score : -173.0, n_buffer : 200000, eps : 0.1%\n",
            "train 837\n",
            "n_episode :837, score : -979.0, n_buffer : 200000, eps : 0.1%\n",
            "train 838\n",
            "n_episode :838, score : -5610.0, n_buffer : 200000, eps : 0.1%\n",
            "train 839\n",
            "n_episode :839, score : -13.0, n_buffer : 200000, eps : 0.1%\n",
            "train 840\n",
            "n_episode :840, score : -3320.0, n_buffer : 200000, eps : 0.1%\n",
            "train 841\n",
            "n_episode :841, score : -29.0, n_buffer : 200000, eps : 0.1%\n",
            "train 842\n",
            "n_episode :842, score : -477.0, n_buffer : 200000, eps : 0.1%\n",
            "train 843\n",
            "n_episode :843, score : -577.0, n_buffer : 200000, eps : 0.1%\n",
            "train 844\n",
            "n_episode :844, score : -1862.0, n_buffer : 200000, eps : 0.1%\n",
            "train 845\n",
            "n_episode :845, score : -1067.0, n_buffer : 200000, eps : 0.1%\n",
            "train 846\n",
            "n_episode :846, score : -605.0, n_buffer : 200000, eps : 0.1%\n",
            "train 847\n",
            "n_episode :847, score : -415.0, n_buffer : 200000, eps : 0.1%\n",
            "train 848\n",
            "n_episode :848, score : -329.0, n_buffer : 200000, eps : 0.1%\n",
            "train 849\n",
            "n_episode :849, score : -668.0, n_buffer : 200000, eps : 0.1%\n",
            "train 850\n",
            "n_episode :850, score : -822.0, n_buffer : 200000, eps : 0.1%\n",
            "train 851\n",
            "n_episode :851, score : -8653.0, n_buffer : 200000, eps : 0.1%\n",
            "train 852\n",
            "n_episode :852, score : -897.0, n_buffer : 200000, eps : 0.1%\n",
            "train 853\n",
            "n_episode :853, score : -1321.0, n_buffer : 200000, eps : 0.1%\n",
            "train 854\n",
            "n_episode :854, score : -4252.0, n_buffer : 200000, eps : 0.1%\n",
            "train 855\n",
            "n_episode :855, score : -628.0, n_buffer : 200000, eps : 0.1%\n",
            "train 856\n",
            "n_episode :856, score : -413.0, n_buffer : 200000, eps : 0.1%\n",
            "train 857\n",
            "n_episode :857, score : -1619.0, n_buffer : 200000, eps : 0.1%\n",
            "train 858\n",
            "n_episode :858, score : -915.0, n_buffer : 200000, eps : 0.1%\n",
            "train 859\n",
            "n_episode :859, score : -352.0, n_buffer : 200000, eps : 0.1%\n",
            "train 860\n",
            "n_episode :860, score : -782.0, n_buffer : 200000, eps : 0.1%\n",
            "train 861\n",
            "n_episode :861, score : -2736.0, n_buffer : 200000, eps : 0.1%\n",
            "train 862\n",
            "n_episode :862, score : -2037.0, n_buffer : 200000, eps : 0.1%\n",
            "train 863\n",
            "n_episode :863, score : -104.0, n_buffer : 200000, eps : 0.1%\n",
            "train 864\n",
            "n_episode :864, score : -1423.0, n_buffer : 200000, eps : 0.1%\n",
            "train 865\n",
            "n_episode :865, score : -1386.0, n_buffer : 200000, eps : 0.1%\n",
            "train 866\n",
            "n_episode :866, score : -19.0, n_buffer : 200000, eps : 0.1%\n",
            "train 867\n",
            "n_episode :867, score : -1887.0, n_buffer : 200000, eps : 0.1%\n",
            "train 868\n",
            "n_episode :868, score : -4423.0, n_buffer : 200000, eps : 0.1%\n",
            "train 869\n",
            "n_episode :869, score : -1423.0, n_buffer : 200000, eps : 0.1%\n",
            "train 870\n",
            "n_episode :870, score : -1323.0, n_buffer : 200000, eps : 0.1%\n",
            "train 871\n",
            "n_episode :871, score : -604.0, n_buffer : 200000, eps : 0.1%\n",
            "train 872\n",
            "n_episode :872, score : -1686.0, n_buffer : 200000, eps : 0.1%\n",
            "train 873\n",
            "n_episode :873, score : -542.0, n_buffer : 200000, eps : 0.1%\n",
            "train 874\n",
            "n_episode :874, score : -1158.0, n_buffer : 200000, eps : 0.1%\n",
            "train 875\n",
            "n_episode :875, score : -44.0, n_buffer : 200000, eps : 0.1%\n",
            "train 876\n",
            "n_episode :876, score : -812.0, n_buffer : 200000, eps : 0.1%\n",
            "train 877\n",
            "n_episode :877, score : -1355.0, n_buffer : 200000, eps : 0.1%\n",
            "train 878\n",
            "n_episode :878, score : -130.0, n_buffer : 200000, eps : 0.1%\n",
            "train 879\n",
            "n_episode :879, score : -2081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 880\n",
            "n_episode :880, score : -223.0, n_buffer : 200000, eps : 0.1%\n",
            "train 881\n",
            "n_episode :881, score : -2118.0, n_buffer : 200000, eps : 0.1%\n",
            "train 882\n",
            "n_episode :882, score : -966.0, n_buffer : 200000, eps : 0.1%\n",
            "train 883\n",
            "n_episode :883, score : -1557.0, n_buffer : 200000, eps : 0.1%\n",
            "train 884\n",
            "n_episode :884, score : -207.0, n_buffer : 200000, eps : 0.1%\n",
            "train 885\n",
            "n_episode :885, score : -1662.0, n_buffer : 200000, eps : 0.1%\n",
            "train 886\n",
            "n_episode :886, score : -1888.0, n_buffer : 200000, eps : 0.1%\n",
            "train 887\n",
            "n_episode :887, score : -62.0, n_buffer : 200000, eps : 0.1%\n",
            "train 888\n",
            "n_episode :888, score : -287.0, n_buffer : 200000, eps : 0.1%\n",
            "train 889\n",
            "n_episode :889, score : -1252.0, n_buffer : 200000, eps : 0.1%\n",
            "train 890\n",
            "n_episode :890, score : -495.0, n_buffer : 200000, eps : 0.1%\n",
            "train 891\n",
            "n_episode :891, score : -324.0, n_buffer : 200000, eps : 0.1%\n",
            "train 892\n",
            "n_episode :892, score : -171.0, n_buffer : 200000, eps : 0.1%\n",
            "train 893\n",
            "n_episode :893, score : -480.0, n_buffer : 200000, eps : 0.1%\n",
            "train 894\n",
            "n_episode :894, score : -837.0, n_buffer : 200000, eps : 0.1%\n",
            "train 895\n",
            "n_episode :895, score : -491.0, n_buffer : 200000, eps : 0.1%\n",
            "train 896\n",
            "n_episode :896, score : -609.0, n_buffer : 200000, eps : 0.1%\n",
            "train 897\n",
            "n_episode :897, score : -207.0, n_buffer : 200000, eps : 0.1%\n",
            "train 898\n",
            "n_episode :898, score : -348.0, n_buffer : 200000, eps : 0.1%\n",
            "train 899\n",
            "n_episode :899, score : -185.0, n_buffer : 200000, eps : 0.1%\n",
            "train 900\n",
            "n_episode :900, score : -352.0, n_buffer : 200000, eps : 0.1%\n",
            "train 901\n",
            "n_episode :901, score : -889.0, n_buffer : 200000, eps : 0.1%\n",
            "train 902\n",
            "n_episode :902, score : -681.0, n_buffer : 200000, eps : 0.1%\n",
            "train 903\n",
            "n_episode :903, score : -4935.0, n_buffer : 200000, eps : 0.1%\n",
            "train 904\n",
            "n_episode :904, score : -1123.0, n_buffer : 200000, eps : 0.1%\n",
            "train 905\n",
            "n_episode :905, score : -1211.0, n_buffer : 200000, eps : 0.1%\n",
            "train 906\n",
            "n_episode :906, score : -4885.0, n_buffer : 200000, eps : 0.1%\n",
            "train 907\n",
            "n_episode :907, score : -1816.0, n_buffer : 200000, eps : 0.1%\n",
            "train 908\n",
            "n_episode :908, score : -35.0, n_buffer : 200000, eps : 0.1%\n",
            "train 909\n",
            "n_episode :909, score : -657.0, n_buffer : 200000, eps : 0.1%\n",
            "train 910\n",
            "n_episode :910, score : -5669.0, n_buffer : 200000, eps : 0.1%\n",
            "train 911\n",
            "n_episode :911, score : -334.0, n_buffer : 200000, eps : 0.1%\n",
            "train 912\n",
            "n_episode :912, score : -530.0, n_buffer : 200000, eps : 0.1%\n",
            "train 913\n",
            "n_episode :913, score : -7100.0, n_buffer : 200000, eps : 0.1%\n",
            "train 914\n",
            "n_episode :914, score : -3004.0, n_buffer : 200000, eps : 0.1%\n",
            "train 915\n",
            "n_episode :915, score : -486.0, n_buffer : 200000, eps : 0.1%\n",
            "train 916\n",
            "n_episode :916, score : -299.0, n_buffer : 200000, eps : 0.1%\n",
            "train 917\n",
            "n_episode :917, score : -653.0, n_buffer : 200000, eps : 0.1%\n",
            "train 918\n",
            "n_episode :918, score : -105.0, n_buffer : 200000, eps : 0.1%\n",
            "train 919\n",
            "n_episode :919, score : -188.0, n_buffer : 200000, eps : 0.1%\n",
            "train 920\n",
            "n_episode :920, score : -1676.0, n_buffer : 200000, eps : 0.1%\n",
            "train 921\n",
            "n_episode :921, score : -6862.0, n_buffer : 200000, eps : 0.1%\n",
            "train 922\n",
            "n_episode :922, score : -27.0, n_buffer : 200000, eps : 0.1%\n",
            "train 923\n",
            "n_episode :923, score : -121.0, n_buffer : 200000, eps : 0.1%\n",
            "train 924\n",
            "n_episode :924, score : -120.0, n_buffer : 200000, eps : 0.1%\n",
            "train 925\n",
            "n_episode :925, score : -1247.0, n_buffer : 200000, eps : 0.1%\n",
            "train 926\n",
            "n_episode :926, score : -3394.0, n_buffer : 200000, eps : 0.1%\n",
            "train 927\n",
            "n_episode :927, score : -468.0, n_buffer : 200000, eps : 0.1%\n",
            "train 928\n",
            "n_episode :928, score : -1769.0, n_buffer : 200000, eps : 0.1%\n",
            "train 929\n",
            "n_episode :929, score : -89.0, n_buffer : 200000, eps : 0.1%\n",
            "train 930\n",
            "n_episode :930, score : -6719.0, n_buffer : 200000, eps : 0.1%\n",
            "train 931\n",
            "n_episode :931, score : -4650.0, n_buffer : 200000, eps : 0.1%\n",
            "train 932\n",
            "n_episode :932, score : -5577.0, n_buffer : 200000, eps : 0.1%\n",
            "train 933\n",
            "n_episode :933, score : -1735.0, n_buffer : 200000, eps : 0.1%\n",
            "train 934\n",
            "n_episode :934, score : -2100.0, n_buffer : 200000, eps : 0.1%\n",
            "train 935\n",
            "n_episode :935, score : -542.0, n_buffer : 200000, eps : 0.1%\n",
            "train 936\n",
            "n_episode :936, score : -1413.0, n_buffer : 200000, eps : 0.1%\n",
            "train 937\n",
            "n_episode :937, score : -593.0, n_buffer : 200000, eps : 0.1%\n",
            "train 938\n",
            "n_episode :938, score : -438.0, n_buffer : 200000, eps : 0.1%\n",
            "train 939\n",
            "n_episode :939, score : -3133.0, n_buffer : 200000, eps : 0.1%\n",
            "train 940\n",
            "n_episode :940, score : -1797.0, n_buffer : 200000, eps : 0.1%\n",
            "train 941\n",
            "n_episode :941, score : -882.0, n_buffer : 200000, eps : 0.1%\n",
            "train 942\n",
            "n_episode :942, score : -1333.0, n_buffer : 200000, eps : 0.1%\n",
            "train 943\n",
            "n_episode :943, score : -2.0, n_buffer : 200000, eps : 0.1%\n",
            "train 944\n",
            "n_episode :944, score : -1516.0, n_buffer : 200000, eps : 0.1%\n",
            "train 945\n",
            "n_episode :945, score : -618.0, n_buffer : 200000, eps : 0.1%\n",
            "train 946\n",
            "n_episode :946, score : -2229.0, n_buffer : 200000, eps : 0.1%\n",
            "train 947\n",
            "n_episode :947, score : -202.0, n_buffer : 200000, eps : 0.1%\n",
            "train 948\n",
            "n_episode :948, score : -2169.0, n_buffer : 200000, eps : 0.1%\n",
            "train 949\n",
            "n_episode :949, score : -5493.0, n_buffer : 200000, eps : 0.1%\n",
            "train 950\n",
            "n_episode :950, score : -788.0, n_buffer : 200000, eps : 0.1%\n",
            "train 951\n",
            "n_episode :951, score : -617.0, n_buffer : 200000, eps : 0.1%\n",
            "train 952\n",
            "n_episode :952, score : -1437.0, n_buffer : 200000, eps : 0.1%\n",
            "train 953\n",
            "n_episode :953, score : -2249.0, n_buffer : 200000, eps : 0.1%\n",
            "train 954\n",
            "n_episode :954, score : -1347.0, n_buffer : 200000, eps : 0.1%\n",
            "train 955\n",
            "n_episode :955, score : -872.0, n_buffer : 200000, eps : 0.1%\n",
            "train 956\n",
            "n_episode :956, score : -711.0, n_buffer : 200000, eps : 0.1%\n",
            "train 957\n",
            "n_episode :957, score : -559.0, n_buffer : 200000, eps : 0.1%\n",
            "train 958\n",
            "n_episode :958, score : -2068.0, n_buffer : 200000, eps : 0.1%\n",
            "train 959\n",
            "n_episode :959, score : -214.0, n_buffer : 200000, eps : 0.1%\n",
            "train 960\n",
            "n_episode :960, score : -686.0, n_buffer : 200000, eps : 0.1%\n",
            "train 961\n",
            "n_episode :961, score : -74.0, n_buffer : 200000, eps : 0.1%\n",
            "train 962\n",
            "n_episode :962, score : -1779.0, n_buffer : 200000, eps : 0.1%\n",
            "train 963\n",
            "n_episode :963, score : -183.0, n_buffer : 200000, eps : 0.1%\n",
            "train 964\n",
            "n_episode :964, score : -1154.0, n_buffer : 200000, eps : 0.1%\n",
            "train 965\n",
            "n_episode :965, score : -275.0, n_buffer : 200000, eps : 0.1%\n",
            "train 966\n",
            "n_episode :966, score : -241.0, n_buffer : 200000, eps : 0.1%\n",
            "train 967\n",
            "n_episode :967, score : -970.0, n_buffer : 200000, eps : 0.1%\n",
            "train 968\n",
            "n_episode :968, score : -1854.0, n_buffer : 200000, eps : 0.1%\n",
            "train 969\n",
            "n_episode :969, score : -291.0, n_buffer : 200000, eps : 0.1%\n",
            "train 970\n",
            "n_episode :970, score : -1863.0, n_buffer : 200000, eps : 0.1%\n",
            "train 971\n",
            "n_episode :971, score : -441.0, n_buffer : 200000, eps : 0.1%\n",
            "train 972\n",
            "n_episode :972, score : -1242.0, n_buffer : 200000, eps : 0.1%\n",
            "train 973\n",
            "n_episode :973, score : -1725.0, n_buffer : 200000, eps : 0.1%\n",
            "train 974\n",
            "n_episode :974, score : -1444.0, n_buffer : 200000, eps : 0.1%\n",
            "train 975\n",
            "n_episode :975, score : -675.0, n_buffer : 200000, eps : 0.1%\n",
            "train 976\n",
            "n_episode :976, score : -318.0, n_buffer : 200000, eps : 0.1%\n",
            "train 977\n",
            "n_episode :977, score : -669.0, n_buffer : 200000, eps : 0.1%\n",
            "train 978\n",
            "n_episode :978, score : -622.0, n_buffer : 200000, eps : 0.1%\n",
            "train 979\n",
            "n_episode :979, score : -1487.0, n_buffer : 200000, eps : 0.1%\n",
            "train 980\n",
            "n_episode :980, score : -38.0, n_buffer : 200000, eps : 0.1%\n",
            "train 981\n",
            "n_episode :981, score : -530.0, n_buffer : 200000, eps : 0.1%\n",
            "train 982\n",
            "n_episode :982, score : -674.0, n_buffer : 200000, eps : 0.1%\n",
            "train 983\n",
            "n_episode :983, score : -331.0, n_buffer : 200000, eps : 0.1%\n",
            "train 984\n",
            "n_episode :984, score : -120.0, n_buffer : 200000, eps : 0.1%\n",
            "train 985\n",
            "n_episode :985, score : -167.0, n_buffer : 200000, eps : 0.1%\n",
            "train 986\n",
            "n_episode :986, score : -869.0, n_buffer : 200000, eps : 0.1%\n",
            "train 987\n",
            "n_episode :987, score : -761.0, n_buffer : 200000, eps : 0.1%\n",
            "train 988\n",
            "n_episode :988, score : -2911.0, n_buffer : 200000, eps : 0.1%\n",
            "train 989\n",
            "n_episode :989, score : -4210.0, n_buffer : 200000, eps : 0.1%\n",
            "train 990\n",
            "n_episode :990, score : -667.0, n_buffer : 200000, eps : 0.1%\n",
            "train 991\n",
            "n_episode :991, score : -847.0, n_buffer : 200000, eps : 0.1%\n",
            "train 992\n",
            "n_episode :992, score : -1733.0, n_buffer : 200000, eps : 0.1%\n",
            "train 993\n",
            "n_episode :993, score : -3972.0, n_buffer : 200000, eps : 0.1%\n",
            "train 994\n",
            "n_episode :994, score : -1189.0, n_buffer : 200000, eps : 0.1%\n",
            "train 995\n",
            "n_episode :995, score : -117.0, n_buffer : 200000, eps : 0.1%\n",
            "train 996\n",
            "n_episode :996, score : -265.0, n_buffer : 200000, eps : 0.1%\n",
            "train 997\n",
            "n_episode :997, score : -2907.0, n_buffer : 200000, eps : 0.1%\n",
            "train 998\n",
            "n_episode :998, score : -4542.0, n_buffer : 200000, eps : 0.1%\n",
            "train 999\n",
            "n_episode :999, score : -295.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1000\n",
            "n_episode :1000, score : -1204.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1001\n",
            "n_episode :1001, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1002\n",
            "n_episode :1002, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1003\n",
            "n_episode :1003, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1004\n",
            "n_episode :1004, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1005\n",
            "n_episode :1005, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1006\n",
            "n_episode :1006, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1007\n",
            "n_episode :1007, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1008\n",
            "n_episode :1008, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1009\n",
            "n_episode :1009, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1010\n",
            "n_episode :1010, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1011\n",
            "n_episode :1011, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1012\n",
            "n_episode :1012, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1013\n",
            "n_episode :1013, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1014\n",
            "n_episode :1014, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1015\n",
            "n_episode :1015, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1016\n",
            "n_episode :1016, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1017\n",
            "n_episode :1017, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1018\n",
            "n_episode :1018, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1019\n",
            "n_episode :1019, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1020\n",
            "n_episode :1020, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1021\n",
            "n_episode :1021, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1022\n",
            "n_episode :1022, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1023\n",
            "n_episode :1023, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1024\n",
            "n_episode :1024, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1025\n",
            "n_episode :1025, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1026\n",
            "n_episode :1026, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1027\n",
            "n_episode :1027, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1028\n",
            "n_episode :1028, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1029\n",
            "n_episode :1029, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1030\n",
            "n_episode :1030, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1031\n",
            "n_episode :1031, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1032\n",
            "n_episode :1032, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1033\n",
            "n_episode :1033, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1034\n",
            "n_episode :1034, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1035\n",
            "n_episode :1035, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1036\n",
            "n_episode :1036, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1037\n",
            "n_episode :1037, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1038\n",
            "n_episode :1038, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1039\n",
            "n_episode :1039, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1040\n",
            "n_episode :1040, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1041\n",
            "n_episode :1041, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1042\n",
            "n_episode :1042, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1043\n",
            "n_episode :1043, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1044\n",
            "n_episode :1044, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1045\n",
            "n_episode :1045, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1046\n",
            "n_episode :1046, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1047\n",
            "n_episode :1047, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1048\n",
            "n_episode :1048, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1049\n",
            "n_episode :1049, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1050\n",
            "n_episode :1050, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1051\n",
            "n_episode :1051, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1052\n",
            "n_episode :1052, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1053\n",
            "n_episode :1053, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1054\n",
            "n_episode :1054, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1055\n",
            "n_episode :1055, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1056\n",
            "n_episode :1056, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1057\n",
            "n_episode :1057, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1058\n",
            "n_episode :1058, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1059\n",
            "n_episode :1059, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1060\n",
            "n_episode :1060, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1061\n",
            "n_episode :1061, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1062\n",
            "n_episode :1062, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1063\n",
            "n_episode :1063, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1064\n",
            "n_episode :1064, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1065\n",
            "n_episode :1065, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1066\n",
            "n_episode :1066, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1067\n",
            "n_episode :1067, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1068\n",
            "n_episode :1068, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1069\n",
            "n_episode :1069, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1070\n",
            "n_episode :1070, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1071\n",
            "n_episode :1071, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1072\n",
            "n_episode :1072, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1073\n",
            "n_episode :1073, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1074\n",
            "n_episode :1074, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1075\n",
            "n_episode :1075, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1076\n",
            "n_episode :1076, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1077\n",
            "n_episode :1077, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1078\n",
            "n_episode :1078, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1079\n",
            "n_episode :1079, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1080\n",
            "n_episode :1080, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1081\n",
            "n_episode :1081, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1082\n",
            "n_episode :1082, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1083\n",
            "n_episode :1083, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1084\n",
            "n_episode :1084, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1085\n",
            "n_episode :1085, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1086\n",
            "n_episode :1086, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1087\n",
            "n_episode :1087, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1088\n",
            "n_episode :1088, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1089\n",
            "n_episode :1089, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1090\n",
            "n_episode :1090, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1091\n",
            "n_episode :1091, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1092\n",
            "n_episode :1092, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1093\n",
            "n_episode :1093, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1094\n",
            "n_episode :1094, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1095\n",
            "n_episode :1095, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1096\n",
            "n_episode :1096, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1097\n",
            "n_episode :1097, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1098\n",
            "n_episode :1098, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1099\n",
            "n_episode :1099, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1100\n",
            "n_episode :1100, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1101\n",
            "n_episode :1101, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1102\n",
            "n_episode :1102, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1103\n",
            "n_episode :1103, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1104\n",
            "n_episode :1104, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1105\n",
            "n_episode :1105, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1106\n",
            "n_episode :1106, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1107\n",
            "n_episode :1107, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1108\n",
            "n_episode :1108, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1109\n",
            "n_episode :1109, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1110\n",
            "n_episode :1110, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1111\n",
            "n_episode :1111, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1112\n",
            "n_episode :1112, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1113\n",
            "n_episode :1113, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1114\n",
            "n_episode :1114, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1115\n",
            "n_episode :1115, score : -15126.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1116\n",
            "n_episode :1116, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1117\n",
            "n_episode :1117, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1118\n",
            "n_episode :1118, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1119\n",
            "n_episode :1119, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1120\n",
            "n_episode :1120, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1121\n",
            "n_episode :1121, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1122\n",
            "n_episode :1122, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1123\n",
            "n_episode :1123, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1124\n",
            "n_episode :1124, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1125\n",
            "n_episode :1125, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1126\n",
            "n_episode :1126, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1127\n",
            "n_episode :1127, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1128\n",
            "n_episode :1128, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1129\n",
            "n_episode :1129, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1130\n",
            "n_episode :1130, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1131\n",
            "n_episode :1131, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1132\n",
            "n_episode :1132, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1133\n",
            "n_episode :1133, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1134\n",
            "n_episode :1134, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1135\n",
            "n_episode :1135, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1136\n",
            "n_episode :1136, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1137\n",
            "n_episode :1137, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1138\n",
            "n_episode :1138, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1139\n",
            "n_episode :1139, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1140\n",
            "n_episode :1140, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1141\n",
            "n_episode :1141, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1142\n",
            "n_episode :1142, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1143\n",
            "n_episode :1143, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1144\n",
            "n_episode :1144, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1145\n",
            "n_episode :1145, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1146\n",
            "n_episode :1146, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1147\n",
            "n_episode :1147, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1148\n",
            "n_episode :1148, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1149\n",
            "n_episode :1149, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1150\n",
            "n_episode :1150, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1151\n",
            "n_episode :1151, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1152\n",
            "n_episode :1152, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1153\n",
            "n_episode :1153, score : -15126.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1154\n",
            "n_episode :1154, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1155\n",
            "n_episode :1155, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1156\n",
            "n_episode :1156, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1157\n",
            "n_episode :1157, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1158\n",
            "n_episode :1158, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1159\n",
            "n_episode :1159, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1160\n",
            "n_episode :1160, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1161\n",
            "n_episode :1161, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1162\n",
            "n_episode :1162, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1163\n",
            "n_episode :1163, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1164\n",
            "n_episode :1164, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1165\n",
            "n_episode :1165, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1166\n",
            "n_episode :1166, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1167\n",
            "n_episode :1167, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1168\n",
            "n_episode :1168, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1169\n",
            "n_episode :1169, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1170\n",
            "n_episode :1170, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1171\n",
            "n_episode :1171, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1172\n",
            "n_episode :1172, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1173\n",
            "n_episode :1173, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1174\n",
            "n_episode :1174, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1175\n",
            "n_episode :1175, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1176\n",
            "n_episode :1176, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1177\n",
            "n_episode :1177, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1178\n",
            "n_episode :1178, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1179\n",
            "n_episode :1179, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1180\n",
            "n_episode :1180, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1181\n",
            "n_episode :1181, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1182\n",
            "n_episode :1182, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1183\n",
            "n_episode :1183, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1184\n",
            "n_episode :1184, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1185\n",
            "n_episode :1185, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1186\n",
            "n_episode :1186, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1187\n",
            "n_episode :1187, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1188\n",
            "n_episode :1188, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1189\n",
            "n_episode :1189, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1190\n",
            "n_episode :1190, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1191\n",
            "n_episode :1191, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1192\n",
            "n_episode :1192, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1193\n",
            "n_episode :1193, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1194\n",
            "n_episode :1194, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1195\n",
            "n_episode :1195, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1196\n",
            "n_episode :1196, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1197\n",
            "n_episode :1197, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1198\n",
            "n_episode :1198, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1199\n",
            "n_episode :1199, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1200\n",
            "n_episode :1200, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1201\n",
            "n_episode :1201, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1202\n",
            "n_episode :1202, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1203\n",
            "n_episode :1203, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1204\n",
            "n_episode :1204, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1205\n",
            "n_episode :1205, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1206\n",
            "n_episode :1206, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1207\n",
            "n_episode :1207, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1208\n",
            "n_episode :1208, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1209\n",
            "n_episode :1209, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1210\n",
            "n_episode :1210, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1211\n",
            "n_episode :1211, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1212\n",
            "n_episode :1212, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1213\n",
            "n_episode :1213, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1214\n",
            "n_episode :1214, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1215\n",
            "n_episode :1215, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1216\n",
            "n_episode :1216, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1217\n",
            "n_episode :1217, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1218\n",
            "n_episode :1218, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1219\n",
            "n_episode :1219, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1220\n",
            "n_episode :1220, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1221\n",
            "n_episode :1221, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1222\n",
            "n_episode :1222, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1223\n",
            "n_episode :1223, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1224\n",
            "n_episode :1224, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1225\n",
            "n_episode :1225, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1226\n",
            "n_episode :1226, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1227\n",
            "n_episode :1227, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1228\n",
            "n_episode :1228, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1229\n",
            "n_episode :1229, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1230\n",
            "n_episode :1230, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1231\n",
            "n_episode :1231, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1232\n",
            "n_episode :1232, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1233\n",
            "n_episode :1233, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1234\n",
            "n_episode :1234, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1235\n",
            "n_episode :1235, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1236\n",
            "n_episode :1236, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1237\n",
            "n_episode :1237, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1238\n",
            "n_episode :1238, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1239\n",
            "n_episode :1239, score : -15108.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1240\n",
            "n_episode :1240, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1241\n",
            "n_episode :1241, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1242\n",
            "n_episode :1242, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1243\n",
            "n_episode :1243, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1244\n",
            "n_episode :1244, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1245\n",
            "n_episode :1245, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1246\n",
            "n_episode :1246, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1247\n",
            "n_episode :1247, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1248\n",
            "n_episode :1248, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1249\n",
            "n_episode :1249, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1250\n",
            "n_episode :1250, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1251\n",
            "n_episode :1251, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1252\n",
            "n_episode :1252, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1253\n",
            "n_episode :1253, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1254\n",
            "n_episode :1254, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1255\n",
            "n_episode :1255, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1256\n",
            "n_episode :1256, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1257\n",
            "n_episode :1257, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1258\n",
            "n_episode :1258, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1259\n",
            "n_episode :1259, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1260\n",
            "n_episode :1260, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1261\n",
            "n_episode :1261, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1262\n",
            "n_episode :1262, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1263\n",
            "n_episode :1263, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1264\n",
            "n_episode :1264, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1265\n",
            "n_episode :1265, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1266\n",
            "n_episode :1266, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1267\n",
            "n_episode :1267, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1268\n",
            "n_episode :1268, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1269\n",
            "n_episode :1269, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1270\n",
            "n_episode :1270, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1271\n",
            "n_episode :1271, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1272\n",
            "n_episode :1272, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1273\n",
            "n_episode :1273, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1274\n",
            "n_episode :1274, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1275\n",
            "n_episode :1275, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1276\n",
            "n_episode :1276, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1277\n",
            "n_episode :1277, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1278\n",
            "n_episode :1278, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1279\n",
            "n_episode :1279, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1280\n",
            "n_episode :1280, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1281\n",
            "n_episode :1281, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1282\n",
            "n_episode :1282, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1283\n",
            "n_episode :1283, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1284\n",
            "n_episode :1284, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1285\n",
            "n_episode :1285, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1286\n",
            "n_episode :1286, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1287\n",
            "n_episode :1287, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1288\n",
            "n_episode :1288, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1289\n",
            "n_episode :1289, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1290\n",
            "n_episode :1290, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1291\n",
            "n_episode :1291, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1292\n",
            "n_episode :1292, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1293\n",
            "n_episode :1293, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1294\n",
            "n_episode :1294, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1295\n",
            "n_episode :1295, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1296\n",
            "n_episode :1296, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1297\n",
            "n_episode :1297, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1298\n",
            "n_episode :1298, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1299\n",
            "n_episode :1299, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1300\n",
            "n_episode :1300, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1301\n",
            "n_episode :1301, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1302\n",
            "n_episode :1302, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1303\n",
            "n_episode :1303, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1304\n",
            "n_episode :1304, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1305\n",
            "n_episode :1305, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1306\n",
            "n_episode :1306, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1307\n",
            "n_episode :1307, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1308\n",
            "n_episode :1308, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1309\n",
            "n_episode :1309, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1310\n",
            "n_episode :1310, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1311\n",
            "n_episode :1311, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1312\n",
            "n_episode :1312, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1313\n",
            "n_episode :1313, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1314\n",
            "n_episode :1314, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1315\n",
            "n_episode :1315, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1316\n",
            "n_episode :1316, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1317\n",
            "n_episode :1317, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1318\n",
            "n_episode :1318, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1319\n",
            "n_episode :1319, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1320\n",
            "n_episode :1320, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1321\n",
            "n_episode :1321, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1322\n",
            "n_episode :1322, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1323\n",
            "n_episode :1323, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1324\n",
            "n_episode :1324, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1325\n",
            "n_episode :1325, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1326\n",
            "n_episode :1326, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1327\n",
            "n_episode :1327, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1328\n",
            "n_episode :1328, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1329\n",
            "n_episode :1329, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1330\n",
            "n_episode :1330, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1331\n",
            "n_episode :1331, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1332\n",
            "n_episode :1332, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1333\n",
            "n_episode :1333, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1334\n",
            "n_episode :1334, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1335\n",
            "n_episode :1335, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1336\n",
            "n_episode :1336, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1337\n",
            "n_episode :1337, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1338\n",
            "n_episode :1338, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1339\n",
            "n_episode :1339, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1340\n",
            "n_episode :1340, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1341\n",
            "n_episode :1341, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1342\n",
            "n_episode :1342, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1343\n",
            "n_episode :1343, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1344\n",
            "n_episode :1344, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1345\n",
            "n_episode :1345, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1346\n",
            "n_episode :1346, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1347\n",
            "n_episode :1347, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1348\n",
            "n_episode :1348, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1349\n",
            "n_episode :1349, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1350\n",
            "n_episode :1350, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1351\n",
            "n_episode :1351, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1352\n",
            "n_episode :1352, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1353\n",
            "n_episode :1353, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1354\n",
            "n_episode :1354, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1355\n",
            "n_episode :1355, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1356\n",
            "n_episode :1356, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1357\n",
            "n_episode :1357, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1358\n",
            "n_episode :1358, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1359\n",
            "n_episode :1359, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1360\n",
            "n_episode :1360, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1361\n",
            "n_episode :1361, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1362\n",
            "n_episode :1362, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1363\n",
            "n_episode :1363, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1364\n",
            "n_episode :1364, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1365\n",
            "n_episode :1365, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1366\n",
            "n_episode :1366, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1367\n",
            "n_episode :1367, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1368\n",
            "n_episode :1368, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1369\n",
            "n_episode :1369, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1370\n",
            "n_episode :1370, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1371\n",
            "n_episode :1371, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1372\n",
            "n_episode :1372, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1373\n",
            "n_episode :1373, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1374\n",
            "n_episode :1374, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1375\n",
            "n_episode :1375, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1376\n",
            "n_episode :1376, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1377\n",
            "n_episode :1377, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1378\n",
            "n_episode :1378, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1379\n",
            "n_episode :1379, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1380\n",
            "n_episode :1380, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1381\n",
            "n_episode :1381, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1382\n",
            "n_episode :1382, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1383\n",
            "n_episode :1383, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1384\n",
            "n_episode :1384, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1385\n",
            "n_episode :1385, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1386\n",
            "n_episode :1386, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1387\n",
            "n_episode :1387, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1388\n",
            "n_episode :1388, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1389\n",
            "n_episode :1389, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1390\n",
            "n_episode :1390, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1391\n",
            "n_episode :1391, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1392\n",
            "n_episode :1392, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1393\n",
            "n_episode :1393, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1394\n",
            "n_episode :1394, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1395\n",
            "n_episode :1395, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1396\n",
            "n_episode :1396, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1397\n",
            "n_episode :1397, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1398\n",
            "n_episode :1398, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1399\n",
            "n_episode :1399, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1400\n",
            "n_episode :1400, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1401\n",
            "n_episode :1401, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1402\n",
            "n_episode :1402, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1403\n",
            "n_episode :1403, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1404\n",
            "n_episode :1404, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1405\n",
            "n_episode :1405, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1406\n",
            "n_episode :1406, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1407\n",
            "n_episode :1407, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1408\n",
            "n_episode :1408, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1409\n",
            "n_episode :1409, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1410\n",
            "n_episode :1410, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1411\n",
            "n_episode :1411, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1412\n",
            "n_episode :1412, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1413\n",
            "n_episode :1413, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1414\n",
            "n_episode :1414, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1415\n",
            "n_episode :1415, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1416\n",
            "n_episode :1416, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1417\n",
            "n_episode :1417, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1418\n",
            "n_episode :1418, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1419\n",
            "n_episode :1419, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1420\n",
            "n_episode :1420, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1421\n",
            "n_episode :1421, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1422\n",
            "n_episode :1422, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1423\n",
            "n_episode :1423, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1424\n",
            "n_episode :1424, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1425\n",
            "n_episode :1425, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1426\n",
            "n_episode :1426, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1427\n",
            "n_episode :1427, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1428\n",
            "n_episode :1428, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1429\n",
            "n_episode :1429, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1430\n",
            "n_episode :1430, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1431\n",
            "n_episode :1431, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1432\n",
            "n_episode :1432, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1433\n",
            "n_episode :1433, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1434\n",
            "n_episode :1434, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1435\n",
            "n_episode :1435, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1436\n",
            "n_episode :1436, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1437\n",
            "n_episode :1437, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1438\n",
            "n_episode :1438, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1439\n",
            "n_episode :1439, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1440\n",
            "n_episode :1440, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1441\n",
            "n_episode :1441, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1442\n",
            "n_episode :1442, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1443\n",
            "n_episode :1443, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1444\n",
            "n_episode :1444, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1445\n",
            "n_episode :1445, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1446\n",
            "n_episode :1446, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1447\n",
            "n_episode :1447, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1448\n",
            "n_episode :1448, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1449\n",
            "n_episode :1449, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1450\n",
            "n_episode :1450, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1451\n",
            "n_episode :1451, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1452\n",
            "n_episode :1452, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1453\n",
            "n_episode :1453, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1454\n",
            "n_episode :1454, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1455\n",
            "n_episode :1455, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1456\n",
            "n_episode :1456, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1457\n",
            "n_episode :1457, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1458\n",
            "n_episode :1458, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1459\n",
            "n_episode :1459, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1460\n",
            "n_episode :1460, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1461\n",
            "n_episode :1461, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1462\n",
            "n_episode :1462, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1463\n",
            "n_episode :1463, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1464\n",
            "n_episode :1464, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1465\n",
            "n_episode :1465, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1466\n",
            "n_episode :1466, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1467\n",
            "n_episode :1467, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1468\n",
            "n_episode :1468, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1469\n",
            "n_episode :1469, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1470\n",
            "n_episode :1470, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1471\n",
            "n_episode :1471, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1472\n",
            "n_episode :1472, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1473\n",
            "n_episode :1473, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1474\n",
            "n_episode :1474, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1475\n",
            "n_episode :1475, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1476\n",
            "n_episode :1476, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1477\n",
            "n_episode :1477, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1478\n",
            "n_episode :1478, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1479\n",
            "n_episode :1479, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1480\n",
            "n_episode :1480, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1481\n",
            "n_episode :1481, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1482\n",
            "n_episode :1482, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1483\n",
            "n_episode :1483, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1484\n",
            "n_episode :1484, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1485\n",
            "n_episode :1485, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1486\n",
            "n_episode :1486, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1487\n",
            "n_episode :1487, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1488\n",
            "n_episode :1488, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1489\n",
            "n_episode :1489, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1490\n",
            "n_episode :1490, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1491\n",
            "n_episode :1491, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1492\n",
            "n_episode :1492, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1493\n",
            "n_episode :1493, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1494\n",
            "n_episode :1494, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1495\n",
            "n_episode :1495, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1496\n",
            "n_episode :1496, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1497\n",
            "n_episode :1497, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1498\n",
            "n_episode :1498, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1499\n",
            "n_episode :1499, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1500\n",
            "n_episode :1500, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1501\n",
            "n_episode :1501, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1502\n",
            "n_episode :1502, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1503\n",
            "n_episode :1503, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1504\n",
            "n_episode :1504, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1505\n",
            "n_episode :1505, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1506\n",
            "n_episode :1506, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1507\n",
            "n_episode :1507, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1508\n",
            "n_episode :1508, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1509\n",
            "n_episode :1509, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1510\n",
            "n_episode :1510, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1511\n",
            "n_episode :1511, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1512\n",
            "n_episode :1512, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1513\n",
            "n_episode :1513, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1514\n",
            "n_episode :1514, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1515\n",
            "n_episode :1515, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1516\n",
            "n_episode :1516, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1517\n",
            "n_episode :1517, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1518\n",
            "n_episode :1518, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1519\n",
            "n_episode :1519, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1520\n",
            "n_episode :1520, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1521\n",
            "n_episode :1521, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1522\n",
            "n_episode :1522, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1523\n",
            "n_episode :1523, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1524\n",
            "n_episode :1524, score : -15045.0, n_buffer : 200000, eps : 0.1%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6e3754f9eb9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-6e3754f9eb9d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;31m# a = random.randint(0,5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0ms_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 단계 진행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0;31m# print(s_prime, a, done)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;31m# print(s, a, s_prime, r, done)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/envs/toy_text/taxi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mtransitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/envs/toy_text/utils.py\u001b[0m in \u001b[0;36mcategorical_sample\u001b[0;34m(prob_n, np_random)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"\"\"Sample from categorical distribution where each row specifies class probabilities.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprob_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcsprob_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsprob_n\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcumsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mcumsum\u001b[0;34m(a, axis, dtype, out)\u001b[0m\n\u001b[1;32m   2530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2531\u001b[0m     \"\"\"\n\u001b[0;32m-> 2532\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cumsum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "env = gym.make('Taxi-v3').unwrapped\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
        "\n",
        "LEARNING_RATE = 0.01\n",
        "NUM_EPISODES = 5000\n",
        "MEMORY = 200000\n",
        "BATCH_SIZE = 1024\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.001\n",
        "MAX_EPISODE = 150\n",
        "TARGET_UPDATE = 3\n",
        "HIDDEN_DIM = 64\n",
        "N_ACTIONS = env.action_space.n\n",
        "N_STATES = env.observation_space.n\n",
        "\n",
        "SOLVE_TAXI_MESSAGE = \"\"\"Task : \\n\n",
        "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
        "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
        "3) Take the passenger to the PINK(drop location) using the shortest path\n",
        "4) Perform a \"dropoff\" action\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "\n",
        "class DQN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
        "\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
        "            torch.nn.BatchNorm1d(hidden_dim),\n",
        "            torch.nn.PReLU()\n",
        "        )\n",
        "\n",
        "        self.layer2 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
        "            torch.nn.BatchNorm1d(hidden_dim),\n",
        "            torch.nn.PReLU()\n",
        "        )\n",
        "\n",
        "        self.final = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)[:,0,:]\n",
        "        x = self.layer1(x)\n",
        "        # x = self.layer2(x)\n",
        "        x = self.final(x)\n",
        "        return x\n",
        "\n",
        "policy_net = DQN(N_STATES,N_ACTIONS,HIDDEN_DIM).to(device)\n",
        "# policy_net.load_state_dict(torch.load('/content/double_dqn_model_last.pth'))\n",
        "target_net = DQN(N_STATES,N_ACTIONS,HIDDEN_DIM).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "optim = torch.optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "memory = ReplayMemory(MEMORY)\n",
        "\n",
        "\n",
        "def to_variable(x, type=torch.long):\n",
        "    return torch.autograd.Variable(torch.tensor(x, dtype=type).to(device))\n",
        "\n",
        "def get_Q(net, state):\n",
        "    state = to_variable(np.array(state).reshape(-1, 1))\n",
        "    net.train(mode=False)\n",
        "    return net(state)\n",
        "\n",
        "\n",
        "def get_action(state, eps):\n",
        "    if np.random.rand() < eps:\n",
        "        return np.random.choice(N_ACTIONS)\n",
        "    else:\n",
        "        policy_net.train(mode=False)\n",
        "        scores = get_Q(policy_net, state)\n",
        "        _, argmax = torch.max(scores.data, 1)\n",
        "        return int(argmax.cpu().numpy())\n",
        "\n",
        "\n",
        "\n",
        "episode_durations = []\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if is_ipython:\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "\n",
        "\n",
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "\n",
        "    states = np.vstack([x.state for x in transitions])\n",
        "    actions = np.array([x.action for x in transitions])\n",
        "    rewards = np.array([x.reward for x in transitions])\n",
        "    next_states = np.vstack([x.next_state for x in transitions])\n",
        "    done = np.array([x.done for x in transitions])\n",
        "\n",
        "    Q_predict = get_Q(policy_net, states)\n",
        "    \n",
        "    Q_target = Q_predict.clone().data.cpu().numpy()\n",
        "\n",
        "    # For DQN\n",
        "    # Q_target[np.arange(len(Q_target)), actions] = rewards + GAMMA * np.max(get_Q(target_net, next_states).data.cpu().numpy(), axis=1) * ~done\n",
        "\n",
        "    # For Double DQN\n",
        "    Q_next_state = np.argmax(get_Q(policy_net, next_states).data.cpu().numpy(), axis=1).reshape(-1)\n",
        "    Q_target[np.arange(len(Q_target)), actions] = rewards + GAMMA * np.choose(Q_next_state, get_Q(target_net, next_states).data.cpu().numpy().T) * ~done\n",
        "\n",
        "    Q_target = to_variable(Q_target, type=torch.float)\n",
        "\n",
        "\n",
        "    policy_net.train(mode=True)\n",
        "    optim.zero_grad()\n",
        "    loss = loss_fn(Q_predict, Q_target)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "def epsilon_annealing(episode, max_episode, min_eps, max_eps):\n",
        "    if max_episode == 0:\n",
        "        return min_eps\n",
        "    slope = (min_eps - max_eps) / max_episode\n",
        "    return max(slope * episode + max_eps, min_eps)\n",
        "\n",
        "def clear_screen(delay=1):\n",
        "    time.sleep(delay)\n",
        "    os.system('clear')\n",
        "\n",
        "def log_progress(env, reward=0, total_reward=0, delay=None, message=None, eps=0):\n",
        "    if type(message) is str:\n",
        "        print(message)\n",
        "    # env.render()\n",
        "    print('Reward:', reward)\n",
        "    print('Cumulative reward', total_reward)\n",
        "    print('ε-greedy probability', eps)\n",
        "    clear_screen(delay)\n",
        "\n",
        "def init_message(attempt, perf):\n",
        "    return 'Initial State : {}'.format(perf_message(attempt=attempt, perf=perf))\n",
        "\n",
        "def perf_message(attempt, perf):\n",
        "    return '{}\\nAttempt: {} | Average reward (until last episode): {:.2f}'.format(\n",
        "        SOLVE_TAXI_MESSAGE, \n",
        "        attempt + 1, \n",
        "        perf\n",
        "    )\n",
        "\n",
        "\n",
        "perf = 0\n",
        "perf_list = []\n",
        "score  = 0\n",
        "for i_episode in range(NUM_EPISODES):\n",
        "    clear_screen(0)\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    eps = epsilon_annealing(i_episode, MAX_EPISODE, EPS_END, EPS_START)\n",
        "    done = False\n",
        "    t = 0\n",
        "    log_progress(env, delay=0.5, message=init_message(i_episode, perf), eps=eps)\n",
        "    while not done:\n",
        "        action = get_action(state, eps)\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        if i_episode >= 250:\n",
        "            log_progress(env, reward=reward, total_reward=total_reward, delay=0.5,message=perf_message(i_episode, perf), eps=eps)\n",
        "\n",
        "        memory.push(state, action, next_state, reward, done)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        optimize_model()\n",
        "        t += 1\n",
        "\n",
        "        if done:\n",
        "            episode_durations.append(t)\n",
        "            # plot_durations()\n",
        "\n",
        "    score += total_reward\n",
        "    perf = score/(i_episode + 1)\n",
        "    perf_list.append(perf)\n",
        "    pd.DataFrame(perf_list).to_csv('record.csv')\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "        torch.save(policy_net.state_dict(), 'double_dqn_model_temp.pth')\n",
        "\n",
        "torch.save(policy_net.state_dict(), 'double_dqn_model_last.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bryRk0EVazzc",
        "outputId": "4d2290e5-517e-4d40-fbef-5f7282658162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4968 | Average reward (until last episode): -31.08\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4968 | Average reward (until last episode): -31.08\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4968 | Average reward (until last episode): -31.08\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4968 | Average reward (until last episode): -31.08\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4968 | Average reward (until last episode): -31.08\n",
            "Reward: 20\n",
            "Cumulative reward 9\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4969 | Average reward (until last episode): -31.07\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4969 | Average reward (until last episode): -31.07\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4969 | Average reward (until last episode): -31.07\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4969 | Average reward (until last episode): -31.07\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4969 | Average reward (until last episode): -31.07\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4969 | Average reward (until last episode): -31.07\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4969 | Average reward (until last episode): -31.07\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4969 | Average reward (until last episode): -31.07\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4969 | Average reward (until last episode): -31.07\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4969 | Average reward (until last episode): -31.07\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4969 | Average reward (until last episode): -31.07\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4969 | Average reward (until last episode): -31.07\n",
            "Reward: 20\n",
            "Cumulative reward 10\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4970 | Average reward (until last episode): -31.06\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4970 | Average reward (until last episode): -31.06\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4970 | Average reward (until last episode): -31.06\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4970 | Average reward (until last episode): -31.06\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4970 | Average reward (until last episode): -31.06\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4970 | Average reward (until last episode): -31.06\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4970 | Average reward (until last episode): -31.06\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4970 | Average reward (until last episode): -31.06\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4970 | Average reward (until last episode): -31.06\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4970 | Average reward (until last episode): -31.06\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4970 | Average reward (until last episode): -31.06\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4970 | Average reward (until last episode): -31.06\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4970 | Average reward (until last episode): -31.06\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4970 | Average reward (until last episode): -31.06\n",
            "Reward: -1\n",
            "Cumulative reward -13\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4970 | Average reward (until last episode): -31.06\n",
            "Reward: -1\n",
            "Cumulative reward -14\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4970 | Average reward (until last episode): -31.06\n",
            "Reward: 20\n",
            "Cumulative reward 6\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4971 | Average reward (until last episode): -31.05\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4971 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4971 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4971 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4971 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4971 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4971 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4971 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4971 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4971 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4971 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4971 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4971 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4971 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -13\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4971 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -14\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4971 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -15\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4971 | Average reward (until last episode): -31.05\n",
            "Reward: 20\n",
            "Cumulative reward 5\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4972 | Average reward (until last episode): -31.05\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4972 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4972 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4972 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4972 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4972 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4972 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4972 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4972 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4972 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4972 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4972 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4972 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4972 | Average reward (until last episode): -31.05\n",
            "Reward: -1\n",
            "Cumulative reward -13\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4972 | Average reward (until last episode): -31.05\n",
            "Reward: 20\n",
            "Cumulative reward 7\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4973 | Average reward (until last episode): -31.04\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4973 | Average reward (until last episode): -31.04\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4973 | Average reward (until last episode): -31.04\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4973 | Average reward (until last episode): -31.04\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4973 | Average reward (until last episode): -31.04\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4973 | Average reward (until last episode): -31.04\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4973 | Average reward (until last episode): -31.04\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4973 | Average reward (until last episode): -31.04\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4973 | Average reward (until last episode): -31.04\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4973 | Average reward (until last episode): -31.04\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4973 | Average reward (until last episode): -31.04\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4973 | Average reward (until last episode): -31.04\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4973 | Average reward (until last episode): -31.04\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4973 | Average reward (until last episode): -31.04\n",
            "Reward: -1\n",
            "Cumulative reward -13\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4973 | Average reward (until last episode): -31.04\n",
            "Reward: 20\n",
            "Cumulative reward 7\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4974 | Average reward (until last episode): -31.03\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4974 | Average reward (until last episode): -31.03\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4974 | Average reward (until last episode): -31.03\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4974 | Average reward (until last episode): -31.03\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4974 | Average reward (until last episode): -31.03\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4974 | Average reward (until last episode): -31.03\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4974 | Average reward (until last episode): -31.03\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4974 | Average reward (until last episode): -31.03\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4974 | Average reward (until last episode): -31.03\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4974 | Average reward (until last episode): -31.03\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4974 | Average reward (until last episode): -31.03\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4974 | Average reward (until last episode): -31.03\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4974 | Average reward (until last episode): -31.03\n",
            "Reward: 20\n",
            "Cumulative reward 9\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4975 | Average reward (until last episode): -31.02\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4975 | Average reward (until last episode): -31.02\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4975 | Average reward (until last episode): -31.02\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4975 | Average reward (until last episode): -31.02\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4975 | Average reward (until last episode): -31.02\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4975 | Average reward (until last episode): -31.02\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4975 | Average reward (until last episode): -31.02\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4975 | Average reward (until last episode): -31.02\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4975 | Average reward (until last episode): -31.02\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4975 | Average reward (until last episode): -31.02\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4975 | Average reward (until last episode): -31.02\n",
            "Reward: 20\n",
            "Cumulative reward 11\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4976 | Average reward (until last episode): -31.02\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4976 | Average reward (until last episode): -31.02\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4976 | Average reward (until last episode): -31.02\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4976 | Average reward (until last episode): -31.02\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4976 | Average reward (until last episode): -31.02\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4976 | Average reward (until last episode): -31.02\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4976 | Average reward (until last episode): -31.02\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4976 | Average reward (until last episode): -31.02\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4976 | Average reward (until last episode): -31.02\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4976 | Average reward (until last episode): -31.02\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4976 | Average reward (until last episode): -31.02\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4976 | Average reward (until last episode): -31.02\n",
            "Reward: 20\n",
            "Cumulative reward 10\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4977 | Average reward (until last episode): -31.01\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4977 | Average reward (until last episode): -31.01\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4977 | Average reward (until last episode): -31.01\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4977 | Average reward (until last episode): -31.01\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4977 | Average reward (until last episode): -31.01\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4977 | Average reward (until last episode): -31.01\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4977 | Average reward (until last episode): -31.01\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4977 | Average reward (until last episode): -31.01\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4977 | Average reward (until last episode): -31.01\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4977 | Average reward (until last episode): -31.01\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4977 | Average reward (until last episode): -31.01\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4977 | Average reward (until last episode): -31.01\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4977 | Average reward (until last episode): -31.01\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4977 | Average reward (until last episode): -31.01\n",
            "Reward: -1\n",
            "Cumulative reward -13\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4977 | Average reward (until last episode): -31.01\n",
            "Reward: -1\n",
            "Cumulative reward -14\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4977 | Average reward (until last episode): -31.01\n",
            "Reward: -1\n",
            "Cumulative reward -15\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4977 | Average reward (until last episode): -31.01\n",
            "Reward: -1\n",
            "Cumulative reward -16\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4977 | Average reward (until last episode): -31.01\n",
            "Reward: 20\n",
            "Cumulative reward 4\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4978 | Average reward (until last episode): -31.00\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4978 | Average reward (until last episode): -31.00\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4978 | Average reward (until last episode): -31.00\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4978 | Average reward (until last episode): -31.00\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4978 | Average reward (until last episode): -31.00\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4978 | Average reward (until last episode): -31.00\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4978 | Average reward (until last episode): -31.00\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4978 | Average reward (until last episode): -31.00\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4978 | Average reward (until last episode): -31.00\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4978 | Average reward (until last episode): -31.00\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4978 | Average reward (until last episode): -31.00\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4978 | Average reward (until last episode): -31.00\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4978 | Average reward (until last episode): -31.00\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4978 | Average reward (until last episode): -31.00\n",
            "Reward: 20\n",
            "Cumulative reward 8\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4979 | Average reward (until last episode): -30.99\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4979 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4979 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4979 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4979 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4979 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4979 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4979 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4979 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4979 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4979 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4979 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4979 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4979 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -13\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4979 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -14\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4979 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -15\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4979 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -16\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4979 | Average reward (until last episode): -30.99\n",
            "Reward: 20\n",
            "Cumulative reward 4\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4980 | Average reward (until last episode): -30.99\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4980 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4980 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4980 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4980 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4980 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4980 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4980 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4980 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4980 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4980 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4980 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4980 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4980 | Average reward (until last episode): -30.99\n",
            "Reward: -1\n",
            "Cumulative reward -13\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4980 | Average reward (until last episode): -30.99\n",
            "Reward: 20\n",
            "Cumulative reward 7\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4981 | Average reward (until last episode): -30.98\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4981 | Average reward (until last episode): -30.98\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4981 | Average reward (until last episode): -30.98\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4981 | Average reward (until last episode): -30.98\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4981 | Average reward (until last episode): -30.98\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4981 | Average reward (until last episode): -30.98\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4981 | Average reward (until last episode): -30.98\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4981 | Average reward (until last episode): -30.98\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4981 | Average reward (until last episode): -30.98\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4981 | Average reward (until last episode): -30.98\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4981 | Average reward (until last episode): -30.98\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4981 | Average reward (until last episode): -30.98\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4981 | Average reward (until last episode): -30.98\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4981 | Average reward (until last episode): -30.98\n",
            "Reward: -1\n",
            "Cumulative reward -13\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4981 | Average reward (until last episode): -30.98\n",
            "Reward: -1\n",
            "Cumulative reward -14\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4981 | Average reward (until last episode): -30.98\n",
            "Reward: -1\n",
            "Cumulative reward -15\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4981 | Average reward (until last episode): -30.98\n",
            "Reward: -1\n",
            "Cumulative reward -16\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4981 | Average reward (until last episode): -30.98\n",
            "Reward: 20\n",
            "Cumulative reward 4\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4982 | Average reward (until last episode): -30.97\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4982 | Average reward (until last episode): -30.97\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4982 | Average reward (until last episode): -30.97\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4982 | Average reward (until last episode): -30.97\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4982 | Average reward (until last episode): -30.97\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4982 | Average reward (until last episode): -30.97\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4982 | Average reward (until last episode): -30.97\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4982 | Average reward (until last episode): -30.97\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4982 | Average reward (until last episode): -30.97\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4982 | Average reward (until last episode): -30.97\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4982 | Average reward (until last episode): -30.97\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4982 | Average reward (until last episode): -30.97\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4982 | Average reward (until last episode): -30.97\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4982 | Average reward (until last episode): -30.97\n",
            "Reward: -1\n",
            "Cumulative reward -13\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4982 | Average reward (until last episode): -30.97\n",
            "Reward: 20\n",
            "Cumulative reward 7\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4983 | Average reward (until last episode): -30.96\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4983 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4983 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4983 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4983 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4983 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4983 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4983 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4983 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4983 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4983 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4983 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4983 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4983 | Average reward (until last episode): -30.96\n",
            "Reward: 20\n",
            "Cumulative reward 8\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4984 | Average reward (until last episode): -30.96\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4984 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4984 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4984 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4984 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4984 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4984 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4984 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4984 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4984 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4984 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4984 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4984 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4984 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -13\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4984 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -14\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4984 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -15\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4984 | Average reward (until last episode): -30.96\n",
            "Reward: -1\n",
            "Cumulative reward -16\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4984 | Average reward (until last episode): -30.96\n",
            "Reward: 20\n",
            "Cumulative reward 4\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4985 | Average reward (until last episode): -30.95\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4985 | Average reward (until last episode): -30.95\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4985 | Average reward (until last episode): -30.95\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4985 | Average reward (until last episode): -30.95\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4985 | Average reward (until last episode): -30.95\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4985 | Average reward (until last episode): -30.95\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4985 | Average reward (until last episode): -30.95\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4985 | Average reward (until last episode): -30.95\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4985 | Average reward (until last episode): -30.95\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4985 | Average reward (until last episode): -30.95\n",
            "Reward: 20\n",
            "Cumulative reward 12\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4986 | Average reward (until last episode): -30.94\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4986 | Average reward (until last episode): -30.94\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4986 | Average reward (until last episode): -30.94\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4986 | Average reward (until last episode): -30.94\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4986 | Average reward (until last episode): -30.94\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4986 | Average reward (until last episode): -30.94\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4986 | Average reward (until last episode): -30.94\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4986 | Average reward (until last episode): -30.94\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4986 | Average reward (until last episode): -30.94\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4986 | Average reward (until last episode): -30.94\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4986 | Average reward (until last episode): -30.94\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4986 | Average reward (until last episode): -30.94\n",
            "Reward: 20\n",
            "Cumulative reward 10\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4987 | Average reward (until last episode): -30.93\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4987 | Average reward (until last episode): -30.93\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4987 | Average reward (until last episode): -30.93\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4987 | Average reward (until last episode): -30.93\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4987 | Average reward (until last episode): -30.93\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4987 | Average reward (until last episode): -30.93\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4987 | Average reward (until last episode): -30.93\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4987 | Average reward (until last episode): -30.93\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4987 | Average reward (until last episode): -30.93\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4987 | Average reward (until last episode): -30.93\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4987 | Average reward (until last episode): -30.93\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4987 | Average reward (until last episode): -30.93\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4987 | Average reward (until last episode): -30.93\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4987 | Average reward (until last episode): -30.93\n",
            "Reward: -1\n",
            "Cumulative reward -13\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4987 | Average reward (until last episode): -30.93\n",
            "Reward: -1\n",
            "Cumulative reward -14\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4987 | Average reward (until last episode): -30.93\n",
            "Reward: 20\n",
            "Cumulative reward 6\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4988 | Average reward (until last episode): -30.92\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4988 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4988 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4988 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4988 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4988 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4988 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4988 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4988 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4988 | Average reward (until last episode): -30.92\n",
            "Reward: 20\n",
            "Cumulative reward 12\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4989 | Average reward (until last episode): -30.92\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4989 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4989 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4989 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4989 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4989 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4989 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4989 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4989 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4989 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4989 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4989 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4989 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4989 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -13\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4989 | Average reward (until last episode): -30.92\n",
            "Reward: -1\n",
            "Cumulative reward -14\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4989 | Average reward (until last episode): -30.92\n",
            "Reward: 20\n",
            "Cumulative reward 6\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4990 | Average reward (until last episode): -30.91\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4990 | Average reward (until last episode): -30.91\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4990 | Average reward (until last episode): -30.91\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4990 | Average reward (until last episode): -30.91\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4990 | Average reward (until last episode): -30.91\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4990 | Average reward (until last episode): -30.91\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4990 | Average reward (until last episode): -30.91\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4990 | Average reward (until last episode): -30.91\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4990 | Average reward (until last episode): -30.91\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4990 | Average reward (until last episode): -30.91\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4990 | Average reward (until last episode): -30.91\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4990 | Average reward (until last episode): -30.91\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4990 | Average reward (until last episode): -30.91\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4990 | Average reward (until last episode): -30.91\n",
            "Reward: -1\n",
            "Cumulative reward -13\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4990 | Average reward (until last episode): -30.91\n",
            "Reward: -1\n",
            "Cumulative reward -14\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4990 | Average reward (until last episode): -30.91\n",
            "Reward: -1\n",
            "Cumulative reward -15\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4990 | Average reward (until last episode): -30.91\n",
            "Reward: -1\n",
            "Cumulative reward -16\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4990 | Average reward (until last episode): -30.91\n",
            "Reward: 20\n",
            "Cumulative reward 4\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4991 | Average reward (until last episode): -30.90\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4991 | Average reward (until last episode): -30.90\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4991 | Average reward (until last episode): -30.90\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4991 | Average reward (until last episode): -30.90\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4991 | Average reward (until last episode): -30.90\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4991 | Average reward (until last episode): -30.90\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4991 | Average reward (until last episode): -30.90\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4991 | Average reward (until last episode): -30.90\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4991 | Average reward (until last episode): -30.90\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4991 | Average reward (until last episode): -30.90\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4991 | Average reward (until last episode): -30.90\n",
            "Reward: 20\n",
            "Cumulative reward 11\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4992 | Average reward (until last episode): -30.89\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4992 | Average reward (until last episode): -30.89\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4992 | Average reward (until last episode): -30.89\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4992 | Average reward (until last episode): -30.89\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4992 | Average reward (until last episode): -30.89\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4992 | Average reward (until last episode): -30.89\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4992 | Average reward (until last episode): -30.89\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4992 | Average reward (until last episode): -30.89\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4992 | Average reward (until last episode): -30.89\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4992 | Average reward (until last episode): -30.89\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4992 | Average reward (until last episode): -30.89\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4992 | Average reward (until last episode): -30.89\n",
            "Reward: 20\n",
            "Cumulative reward 10\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4993 | Average reward (until last episode): -30.88\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4993 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4993 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4993 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4993 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4993 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4993 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4993 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4993 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4993 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4993 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4993 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4993 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4993 | Average reward (until last episode): -30.88\n",
            "Reward: 20\n",
            "Cumulative reward 8\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4994 | Average reward (until last episode): -30.88\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4994 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4994 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4994 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4994 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4994 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4994 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4994 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4994 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4994 | Average reward (until last episode): -30.88\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4994 | Average reward (until last episode): -30.88\n",
            "Reward: 20\n",
            "Cumulative reward 11\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4995 | Average reward (until last episode): -30.87\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4995 | Average reward (until last episode): -30.87\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4995 | Average reward (until last episode): -30.87\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4995 | Average reward (until last episode): -30.87\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4995 | Average reward (until last episode): -30.87\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4995 | Average reward (until last episode): -30.87\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4995 | Average reward (until last episode): -30.87\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4995 | Average reward (until last episode): -30.87\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4995 | Average reward (until last episode): -30.87\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4995 | Average reward (until last episode): -30.87\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4995 | Average reward (until last episode): -30.87\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4995 | Average reward (until last episode): -30.87\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4995 | Average reward (until last episode): -30.87\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4995 | Average reward (until last episode): -30.87\n",
            "Reward: -1\n",
            "Cumulative reward -13\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4995 | Average reward (until last episode): -30.87\n",
            "Reward: -1\n",
            "Cumulative reward -14\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4995 | Average reward (until last episode): -30.87\n",
            "Reward: 20\n",
            "Cumulative reward 6\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4996 | Average reward (until last episode): -30.86\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4996 | Average reward (until last episode): -30.86\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4996 | Average reward (until last episode): -30.86\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4996 | Average reward (until last episode): -30.86\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4996 | Average reward (until last episode): -30.86\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4996 | Average reward (until last episode): -30.86\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4996 | Average reward (until last episode): -30.86\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4996 | Average reward (until last episode): -30.86\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4996 | Average reward (until last episode): -30.86\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4996 | Average reward (until last episode): -30.86\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4996 | Average reward (until last episode): -30.86\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4996 | Average reward (until last episode): -30.86\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4996 | Average reward (until last episode): -30.86\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4996 | Average reward (until last episode): -30.86\n",
            "Reward: -1\n",
            "Cumulative reward -13\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4996 | Average reward (until last episode): -30.86\n",
            "Reward: 20\n",
            "Cumulative reward 7\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4997 | Average reward (until last episode): -30.85\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4997 | Average reward (until last episode): -30.85\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4997 | Average reward (until last episode): -30.85\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4997 | Average reward (until last episode): -30.85\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4997 | Average reward (until last episode): -30.85\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4997 | Average reward (until last episode): -30.85\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4997 | Average reward (until last episode): -30.85\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4997 | Average reward (until last episode): -30.85\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4997 | Average reward (until last episode): -30.85\n",
            "Reward: 20\n",
            "Cumulative reward 13\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4998 | Average reward (until last episode): -30.84\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4998 | Average reward (until last episode): -30.84\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4998 | Average reward (until last episode): -30.84\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4998 | Average reward (until last episode): -30.84\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4998 | Average reward (until last episode): -30.84\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4998 | Average reward (until last episode): -30.84\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4998 | Average reward (until last episode): -30.84\n",
            "Reward: 20\n",
            "Cumulative reward 15\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4999 | Average reward (until last episode): -30.84\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4999 | Average reward (until last episode): -30.84\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4999 | Average reward (until last episode): -30.84\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4999 | Average reward (until last episode): -30.84\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4999 | Average reward (until last episode): -30.84\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4999 | Average reward (until last episode): -30.84\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4999 | Average reward (until last episode): -30.84\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4999 | Average reward (until last episode): -30.84\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4999 | Average reward (until last episode): -30.84\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4999 | Average reward (until last episode): -30.84\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4999 | Average reward (until last episode): -30.84\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4999 | Average reward (until last episode): -30.84\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4999 | Average reward (until last episode): -30.84\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 4999 | Average reward (until last episode): -30.84\n",
            "Reward: 20\n",
            "Cumulative reward 8\n",
            "ε-greedy probability 0.001\n",
            "Initial State : Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 5000 | Average reward (until last episode): -30.83\n",
            "Reward: 0\n",
            "Cumulative reward 0\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 5000 | Average reward (until last episode): -30.83\n",
            "Reward: -1\n",
            "Cumulative reward -1\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 5000 | Average reward (until last episode): -30.83\n",
            "Reward: -1\n",
            "Cumulative reward -2\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 5000 | Average reward (until last episode): -30.83\n",
            "Reward: -1\n",
            "Cumulative reward -3\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 5000 | Average reward (until last episode): -30.83\n",
            "Reward: -1\n",
            "Cumulative reward -4\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 5000 | Average reward (until last episode): -30.83\n",
            "Reward: -1\n",
            "Cumulative reward -5\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 5000 | Average reward (until last episode): -30.83\n",
            "Reward: -1\n",
            "Cumulative reward -6\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 5000 | Average reward (until last episode): -30.83\n",
            "Reward: -1\n",
            "Cumulative reward -7\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 5000 | Average reward (until last episode): -30.83\n",
            "Reward: -1\n",
            "Cumulative reward -8\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 5000 | Average reward (until last episode): -30.83\n",
            "Reward: -1\n",
            "Cumulative reward -9\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 5000 | Average reward (until last episode): -30.83\n",
            "Reward: -1\n",
            "Cumulative reward -10\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 5000 | Average reward (until last episode): -30.83\n",
            "Reward: -1\n",
            "Cumulative reward -11\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 5000 | Average reward (until last episode): -30.83\n",
            "Reward: -1\n",
            "Cumulative reward -12\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 5000 | Average reward (until last episode): -30.83\n",
            "Reward: -1\n",
            "Cumulative reward -13\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 5000 | Average reward (until last episode): -30.83\n",
            "Reward: -1\n",
            "Cumulative reward -14\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 5000 | Average reward (until last episode): -30.83\n",
            "Reward: -1\n",
            "Cumulative reward -15\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 5000 | Average reward (until last episode): -30.83\n",
            "Reward: -1\n",
            "Cumulative reward -16\n",
            "ε-greedy probability 0.001\n",
            "Task : \n",
            "\n",
            "1) The cab(YELLOW) should find the shortest path to BLUE(passenger) \n",
            "2) Perform a \"pickup\" action to board the passenger which turns the cab(GREEN)\n",
            "3) Take the passenger to the PINK(drop location) using the shortest path\n",
            "4) Perform a \"dropoff\" action\n",
            "\n",
            "Attempt: 5000 | Average reward (until last episode): -30.83\n",
            "Reward: 20\n",
            "Cumulative reward 4\n",
            "ε-greedy probability 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# libraries\n",
        "import gym\n",
        "import collections\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# pytorch library is used for deep learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.002\n",
        "gamma = 0.98\n",
        "buffer_limit = 200000        # size of replay buffer, 버퍼에 저장할 데이터 개수\n",
        "batch_size = 1024\n",
        "\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.001\n",
        "MAX_EPISODE = 500\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)    # double-ended queue(deque)를 저장소로 선택,\n",
        "                                                                # buffer_size는 50000으로 설정,\n",
        "                                                                # 50000개가 차면 그 이후부터는 가장 오래된 데이터를 삭제하고 새 데이터를 보관\n",
        "    \n",
        "    def put(self, transition):  # transition 데이터 넣기 메소드\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, n):     # transition 데이터 추출 메소드 (train할떄 batch만들듯이 random으로 데이터 추출해감)\n",
        "        mini_batch = random.sample(self.buffer, n)  # n개만큼 데이터 추출\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], [] # (s,a,r,s')과 종료여부\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done_mask = transition  # buffer에서 추출된 mini_batch에서 transition 데이터 추출\n",
        "            s_lst.append(s)\n",
        "            a_lst.append([a])\n",
        "            r_lst.append([r])\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask_lst.append([done_mask])\n",
        "\n",
        "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
        "               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
        "               torch.tensor(done_mask_lst)\n",
        "\n",
        "    def size(self):  # buffer의 사이즈 확인하는 용도의 메소드\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "class Qnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Qnet, self).__init__() # 초기화\n",
        "        self.embedding = torch.nn.Embedding(500, 500)\n",
        "        self.fc1 = nn.Linear(500, 128)  # 가중치 3개(히든 레이어 2개 적용) 입력값(state)은 4개 (cart position, cart 속도, pole 각도, pole 속도)\n",
        "        self.fc2 = nn.Linear(128, 64) # 가중치 3개(히든 레이어 2개 적용)\n",
        "        self.fc3 = nn.Linear(64, 6) # 가중치 3개(히든 레이어 2개 적용), output(Q(s,a))은 2개\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x.long())\n",
        "        x= x.squeeze(0)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "      \n",
        "    def sample_action(self, obs, epsilon):  # epsilon-greedy로 action 선택하는 함수,  obs : state input 값\n",
        "        out = self.forward(obs)  # forward를 진행 (신경망 통과), out은 tensor 형태\n",
        "        # print(out, out.argmax())\n",
        "        coin = random.random()\n",
        "        if coin < epsilon:\n",
        "            return random.randint(0,5)\n",
        "        else : \n",
        "            return out.argmax().item()   \n",
        "\n",
        "def train(q, q_target, memory, optimizer):\n",
        "    for i in range(10):\n",
        "        s,a,r,s_prime,done_mask = memory.sample(batch_size)  # memory에서 데이터 sampling(추출하기), 배치 사이즈만큼\n",
        "\n",
        "        q_out = q(s)  # q_out.shape = (batch_size, 2)  // 2는 액션의 개수\n",
        "        q_a = q_out.gather(1,a)  # action_list인 a를 활용하여, Q테이블에서 각 state에 맞는 action 값을 취함.\n",
        "                                  # => 그에 대한 value 값을 q_a에 텐서로 저장\n",
        "\n",
        "        # DQN\n",
        "        # max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)  # 타겟넷에 대한 output 값에 대해 dim=1의 max값을 취함\n",
        "\n",
        "        # Double DQN\n",
        "        argmax_Q = q(s_prime).max(1)[1].unsqueeze(1)\n",
        "        max_q_prime = q_target(s_prime).gather(1, argmax_Q)\n",
        "\n",
        "        target = r + gamma * max_q_prime * done_mask # terminate stae면 done_mask가 0이므로 뒤의 항 없어짐\n",
        "        \n",
        "        # MSE Loss\n",
        "        loss = F.mse_loss(q_a, target)\n",
        "\n",
        "        # Smooth L1 Loss\n",
        "        #loss = F.smooth_l1_loss(q_a, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def epsilon_annealing(episode, max_episode, min_eps, max_eps):\n",
        "    if max_episode == 0:\n",
        "        return min_eps\n",
        "    slope = (min_eps - max_eps) / max_episode\n",
        "    return max(slope * episode + max_eps, min_eps)\n",
        "\n",
        "def main():\n",
        "    env = gym.make('Taxi-v3').unwrapped\n",
        "    q = Qnet()            # 메인 넷\n",
        "    q_target = Qnet()     # 타겟 넷  , 별개로 학습\n",
        "    q_target.load_state_dict(q.state_dict())\n",
        "    memory = ReplayBuffer()  ## transition을 담고 있는 테이블에 대한 객체 선언\n",
        "\n",
        "    print_interval = 20   #\n",
        "    score = 0.0           #\n",
        "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
        "    # boost = 1 \n",
        "\n",
        "    return_list = []\n",
        "    for n_epi in range(5000):\n",
        "        # epsilon = max(0.01, 0.5 - 0.02*(n_epi/200)*boost) #Linear annealing from 8% to 1%, epsilon 값을 0.01이 되기 전까지 점점 줄여가도록 함.\n",
        "        epsilon = epsilon_annealing(n_epi, MAX_EPISODE, EPS_END, EPS_START)\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "\n",
        "        if n_epi > 500:\n",
        "          boost = 5\n",
        "        \n",
        "        for i in range(15000):  # 200step으로 제한\n",
        "        # while not done:\n",
        "            a = q.sample_action(torch.LongTensor([s]), epsilon)  # action을 policy(eplsilon-grredy)에 따라 선택\n",
        "\n",
        "            # a = random.randint(0,5)\n",
        "            s_prime, r, done, info, _ = env.step(a)  # 단계 진행\n",
        "            # print(s_prime, a, done)\n",
        "            # print(s, a, s_prime, r, done)\n",
        "            done_mask = 0.0 if done else 1.0\n",
        "            memory.put((s,a,r,s_prime, done_mask))  # r값이 너무 크면, train이 잘 안되서 줄엿음 (왜? 튜닝/테크닉의 영역이라는 교수님의 말씀)\n",
        "                                                          # replace_buffer 객체에 transition 데이터 저장\n",
        "            s = s_prime\n",
        "\n",
        "            score += r\n",
        "            if done:\n",
        "                return_list.append(score)\n",
        "                break\n",
        "\n",
        "        if memory.size()>20000:  # 메모리 사이즈가 2000까지 차기 전에는 train(x), 데이터가 적을때는 훈련이 무의미하므로\n",
        "            print('train', n_epi)\n",
        "            train(q, q_target, memory, optimizer)\n",
        "\n",
        "        print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(n_epi, score, memory.size(), epsilon*100))\n",
        "        score = 0.0\n",
        "\n",
        "        # if n_epi%print_interval==0 and n_epi!=0:  # 20번쨰 episode마다 메인넷(q)의 가중치값을 타겟넷(q_target)으로 카피함.\n",
        "        #     q_target.load_state_dict(q.state_dict())\n",
        "        #     print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n",
        "        #                                                     n_epi, score/print_interval, memory.size(), epsilon*100))\n",
        "        #     score = 0.0\n",
        "\n",
        "        \n",
        "        pd.DataFrame(return_list).to_csv('/content/drive/MyDrive/강화학습/Double_DQN_model_record2.csv')\n",
        "        torch.save(q.state_dict(), '/content/drive/MyDrive/강화학습/Double_DQN_model_last2.pth')\n",
        "\n",
        "    env.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d5fYQC1K94tf",
        "outputId": "9d09f398-eade-4dab-a3b0-429dec720915"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "train 81\n",
            "n_episode :81, score : -3668.0, n_buffer : 200000, eps : 75.4%\n",
            "train 82\n",
            "n_episode :82, score : -31869.0, n_buffer : 200000, eps : 75.3%\n",
            "train 83\n",
            "n_episode :83, score : -21058.0, n_buffer : 200000, eps : 75.1%\n",
            "train 84\n",
            "n_episode :84, score : -6937.0, n_buffer : 200000, eps : 74.9%\n",
            "train 85\n",
            "n_episode :85, score : -48705.0, n_buffer : 200000, eps : 74.7%\n",
            "train 86\n",
            "n_episode :86, score : -24988.0, n_buffer : 200000, eps : 74.5%\n",
            "train 87\n",
            "n_episode :87, score : -29115.0, n_buffer : 200000, eps : 74.4%\n",
            "train 88\n",
            "n_episode :88, score : -482.0, n_buffer : 200000, eps : 74.2%\n",
            "train 89\n",
            "n_episode :89, score : -46689.0, n_buffer : 200000, eps : 74.0%\n",
            "train 90\n",
            "n_episode :90, score : -48138.0, n_buffer : 200000, eps : 73.8%\n",
            "train 91\n",
            "n_episode :91, score : -14802.0, n_buffer : 200000, eps : 73.6%\n",
            "train 92\n",
            "n_episode :92, score : -11513.0, n_buffer : 200000, eps : 73.5%\n",
            "train 93\n",
            "n_episode :93, score : -4813.0, n_buffer : 200000, eps : 73.3%\n",
            "train 94\n",
            "n_episode :94, score : -4252.0, n_buffer : 200000, eps : 73.1%\n",
            "train 95\n",
            "n_episode :95, score : -3761.0, n_buffer : 200000, eps : 72.9%\n",
            "train 96\n",
            "n_episode :96, score : -7554.0, n_buffer : 200000, eps : 72.7%\n",
            "train 97\n",
            "n_episode :97, score : -20116.0, n_buffer : 200000, eps : 72.6%\n",
            "train 98\n",
            "n_episode :98, score : -15071.0, n_buffer : 200000, eps : 72.4%\n",
            "train 99\n",
            "n_episode :99, score : -4067.0, n_buffer : 200000, eps : 72.2%\n",
            "train 100\n",
            "n_episode :100, score : -13011.0, n_buffer : 200000, eps : 72.0%\n",
            "train 101\n",
            "n_episode :101, score : -28034.0, n_buffer : 200000, eps : 71.8%\n",
            "train 102\n",
            "n_episode :102, score : -28096.0, n_buffer : 200000, eps : 71.7%\n",
            "train 103\n",
            "n_episode :103, score : -6756.0, n_buffer : 200000, eps : 71.5%\n",
            "train 104\n",
            "n_episode :104, score : -20970.0, n_buffer : 200000, eps : 71.3%\n",
            "train 105\n",
            "n_episode :105, score : -46590.0, n_buffer : 200000, eps : 71.1%\n",
            "train 106\n",
            "n_episode :106, score : -29597.0, n_buffer : 200000, eps : 70.9%\n",
            "train 107\n",
            "n_episode :107, score : -27003.0, n_buffer : 200000, eps : 70.8%\n",
            "train 108\n",
            "n_episode :108, score : -166.0, n_buffer : 200000, eps : 70.6%\n",
            "train 109\n",
            "n_episode :109, score : -1954.0, n_buffer : 200000, eps : 70.4%\n",
            "train 110\n",
            "n_episode :110, score : -14289.0, n_buffer : 200000, eps : 70.2%\n",
            "train 111\n",
            "n_episode :111, score : -4339.0, n_buffer : 200000, eps : 70.0%\n",
            "train 112\n",
            "n_episode :112, score : -36281.0, n_buffer : 200000, eps : 69.9%\n",
            "train 113\n",
            "n_episode :113, score : -13762.0, n_buffer : 200000, eps : 69.7%\n",
            "train 114\n",
            "n_episode :114, score : -14891.0, n_buffer : 200000, eps : 69.5%\n",
            "train 115\n",
            "n_episode :115, score : -5968.0, n_buffer : 200000, eps : 69.3%\n",
            "train 116\n",
            "n_episode :116, score : -46059.0, n_buffer : 200000, eps : 69.1%\n",
            "train 117\n",
            "n_episode :117, score : -46239.0, n_buffer : 200000, eps : 69.0%\n",
            "train 118\n",
            "n_episode :118, score : -625.0, n_buffer : 200000, eps : 68.8%\n",
            "train 119\n",
            "n_episode :119, score : -45393.0, n_buffer : 200000, eps : 68.6%\n",
            "train 120\n",
            "n_episode :120, score : -42167.0, n_buffer : 200000, eps : 68.4%\n",
            "train 121\n",
            "n_episode :121, score : -2515.0, n_buffer : 200000, eps : 68.2%\n",
            "train 122\n",
            "n_episode :122, score : -42549.0, n_buffer : 200000, eps : 68.1%\n",
            "train 123\n",
            "n_episode :123, score : -45807.0, n_buffer : 200000, eps : 67.9%\n",
            "train 124\n",
            "n_episode :124, score : -43350.0, n_buffer : 200000, eps : 67.7%\n",
            "train 125\n",
            "n_episode :125, score : -45591.0, n_buffer : 200000, eps : 67.5%\n",
            "train 126\n",
            "n_episode :126, score : -45456.0, n_buffer : 200000, eps : 67.3%\n",
            "train 127\n",
            "n_episode :127, score : -43566.0, n_buffer : 200000, eps : 67.2%\n",
            "train 128\n",
            "n_episode :128, score : -30305.0, n_buffer : 200000, eps : 67.0%\n",
            "train 129\n",
            "n_episode :129, score : -4863.0, n_buffer : 200000, eps : 66.8%\n",
            "train 130\n",
            "n_episode :130, score : -5390.0, n_buffer : 200000, eps : 66.6%\n",
            "train 131\n",
            "n_episode :131, score : -44511.0, n_buffer : 200000, eps : 66.4%\n",
            "train 132\n",
            "n_episode :132, score : -42747.0, n_buffer : 200000, eps : 66.3%\n",
            "train 133\n",
            "n_episode :133, score : -9074.0, n_buffer : 200000, eps : 66.1%\n",
            "train 134\n",
            "n_episode :134, score : -1453.0, n_buffer : 200000, eps : 65.9%\n",
            "train 135\n",
            "n_episode :135, score : -41568.0, n_buffer : 200000, eps : 65.7%\n",
            "train 136\n",
            "n_episode :136, score : -27443.0, n_buffer : 200000, eps : 65.5%\n",
            "train 137\n",
            "n_episode :137, score : -41550.0, n_buffer : 200000, eps : 65.4%\n",
            "train 138\n",
            "n_episode :138, score : -219.0, n_buffer : 200000, eps : 65.2%\n",
            "train 139\n",
            "n_episode :139, score : -1336.0, n_buffer : 200000, eps : 65.0%\n",
            "train 140\n",
            "n_episode :140, score : -44124.0, n_buffer : 200000, eps : 64.8%\n",
            "train 141\n",
            "n_episode :141, score : -43071.0, n_buffer : 200000, eps : 64.6%\n",
            "train 142\n",
            "n_episode :142, score : -43215.0, n_buffer : 200000, eps : 64.5%\n",
            "train 143\n",
            "n_episode :143, score : -698.0, n_buffer : 200000, eps : 64.3%\n",
            "train 144\n",
            "n_episode :144, score : -42243.0, n_buffer : 200000, eps : 64.1%\n",
            "train 145\n",
            "n_episode :145, score : -42198.0, n_buffer : 200000, eps : 63.9%\n",
            "train 146\n",
            "n_episode :146, score : -42207.0, n_buffer : 200000, eps : 63.7%\n",
            "train 147\n",
            "n_episode :147, score : -41397.0, n_buffer : 200000, eps : 63.6%\n",
            "train 148\n",
            "n_episode :148, score : -5228.0, n_buffer : 200000, eps : 63.4%\n",
            "train 149\n",
            "n_episode :149, score : -1042.0, n_buffer : 200000, eps : 63.2%\n",
            "train 150\n",
            "n_episode :150, score : -2745.0, n_buffer : 200000, eps : 63.0%\n",
            "train 151\n",
            "n_episode :151, score : -43098.0, n_buffer : 200000, eps : 62.9%\n",
            "train 152\n",
            "n_episode :152, score : -43377.0, n_buffer : 200000, eps : 62.7%\n",
            "train 153\n",
            "n_episode :153, score : -40137.0, n_buffer : 200000, eps : 62.5%\n",
            "train 154\n",
            "n_episode :154, score : -43206.0, n_buffer : 200000, eps : 62.3%\n",
            "train 155\n",
            "n_episode :155, score : -14440.0, n_buffer : 200000, eps : 62.1%\n",
            "train 156\n",
            "n_episode :156, score : -34114.0, n_buffer : 200000, eps : 62.0%\n",
            "train 157\n",
            "n_episode :157, score : -42540.0, n_buffer : 200000, eps : 61.8%\n",
            "train 158\n",
            "n_episode :158, score : -145.0, n_buffer : 200000, eps : 61.6%\n",
            "train 159\n",
            "n_episode :159, score : -590.0, n_buffer : 200000, eps : 61.4%\n",
            "train 160\n",
            "n_episode :160, score : -42648.0, n_buffer : 200000, eps : 61.2%\n",
            "train 161\n",
            "n_episode :161, score : -4844.0, n_buffer : 200000, eps : 61.1%\n",
            "train 162\n",
            "n_episode :162, score : -373.0, n_buffer : 200000, eps : 60.9%\n",
            "train 163\n",
            "n_episode :163, score : -42243.0, n_buffer : 200000, eps : 60.7%\n",
            "train 164\n",
            "n_episode :164, score : -40290.0, n_buffer : 200000, eps : 60.5%\n",
            "train 165\n",
            "n_episode :165, score : -40200.0, n_buffer : 200000, eps : 60.3%\n",
            "train 166\n",
            "n_episode :166, score : -41712.0, n_buffer : 200000, eps : 60.2%\n",
            "train 167\n",
            "n_episode :167, score : -41523.0, n_buffer : 200000, eps : 60.0%\n",
            "train 168\n",
            "n_episode :168, score : -12354.0, n_buffer : 200000, eps : 59.8%\n",
            "train 169\n",
            "n_episode :169, score : -6713.0, n_buffer : 200000, eps : 59.6%\n",
            "train 170\n",
            "n_episode :170, score : -42000.0, n_buffer : 200000, eps : 59.4%\n",
            "train 171\n",
            "n_episode :171, score : -17224.0, n_buffer : 200000, eps : 59.3%\n",
            "train 172\n",
            "n_episode :172, score : -37428.0, n_buffer : 200000, eps : 59.1%\n",
            "train 173\n",
            "n_episode :173, score : -38094.0, n_buffer : 200000, eps : 58.9%\n",
            "train 174\n",
            "n_episode :174, score : -6480.0, n_buffer : 200000, eps : 58.7%\n",
            "train 175\n",
            "n_episode :175, score : -949.0, n_buffer : 200000, eps : 58.5%\n",
            "train 176\n",
            "n_episode :176, score : -40146.0, n_buffer : 200000, eps : 58.4%\n",
            "train 177\n",
            "n_episode :177, score : -41280.0, n_buffer : 200000, eps : 58.2%\n",
            "train 178\n",
            "n_episode :178, score : -8133.0, n_buffer : 200000, eps : 58.0%\n",
            "train 179\n",
            "n_episode :179, score : -40659.0, n_buffer : 200000, eps : 57.8%\n",
            "train 180\n",
            "n_episode :180, score : -6476.0, n_buffer : 200000, eps : 57.6%\n",
            "train 181\n",
            "n_episode :181, score : -41109.0, n_buffer : 200000, eps : 57.5%\n",
            "train 182\n",
            "n_episode :182, score : -40830.0, n_buffer : 200000, eps : 57.3%\n",
            "train 183\n",
            "n_episode :183, score : -40362.0, n_buffer : 200000, eps : 57.1%\n",
            "train 184\n",
            "n_episode :184, score : -40137.0, n_buffer : 200000, eps : 56.9%\n",
            "train 185\n",
            "n_episode :185, score : -39228.0, n_buffer : 200000, eps : 56.7%\n",
            "train 186\n",
            "n_episode :186, score : -39903.0, n_buffer : 200000, eps : 56.6%\n",
            "train 187\n",
            "n_episode :187, score : -1621.0, n_buffer : 200000, eps : 56.4%\n",
            "train 188\n",
            "n_episode :188, score : -40101.0, n_buffer : 200000, eps : 56.2%\n",
            "train 189\n",
            "n_episode :189, score : -40236.0, n_buffer : 200000, eps : 56.0%\n",
            "train 190\n",
            "n_episode :190, score : -6970.0, n_buffer : 200000, eps : 55.8%\n",
            "train 191\n",
            "n_episode :191, score : -11241.0, n_buffer : 200000, eps : 55.7%\n",
            "train 192\n",
            "n_episode :192, score : -41028.0, n_buffer : 200000, eps : 55.5%\n",
            "train 193\n",
            "n_episode :193, score : -39660.0, n_buffer : 200000, eps : 55.3%\n",
            "train 194\n",
            "n_episode :194, score : -40263.0, n_buffer : 200000, eps : 55.1%\n",
            "train 195\n",
            "n_episode :195, score : -39867.0, n_buffer : 200000, eps : 54.9%\n",
            "train 196\n",
            "n_episode :196, score : -38760.0, n_buffer : 200000, eps : 54.8%\n",
            "train 197\n",
            "n_episode :197, score : -2594.0, n_buffer : 200000, eps : 54.6%\n",
            "train 198\n",
            "n_episode :198, score : -38535.0, n_buffer : 200000, eps : 54.4%\n",
            "train 199\n",
            "n_episode :199, score : -6770.0, n_buffer : 200000, eps : 54.2%\n",
            "train 200\n",
            "n_episode :200, score : -34584.0, n_buffer : 200000, eps : 54.0%\n",
            "train 201\n",
            "n_episode :201, score : -28.0, n_buffer : 200000, eps : 53.9%\n",
            "train 202\n",
            "n_episode :202, score : -31428.0, n_buffer : 200000, eps : 53.7%\n",
            "train 203\n",
            "n_episode :203, score : -38346.0, n_buffer : 200000, eps : 53.5%\n",
            "train 204\n",
            "n_episode :204, score : -786.0, n_buffer : 200000, eps : 53.3%\n",
            "train 205\n",
            "n_episode :205, score : -6541.0, n_buffer : 200000, eps : 53.1%\n",
            "train 206\n",
            "n_episode :206, score : -15329.0, n_buffer : 200000, eps : 53.0%\n",
            "train 207\n",
            "n_episode :207, score : -4359.0, n_buffer : 200000, eps : 52.8%\n",
            "train 208\n",
            "n_episode :208, score : -1279.0, n_buffer : 200000, eps : 52.6%\n",
            "train 209\n",
            "n_episode :209, score : -24075.0, n_buffer : 200000, eps : 52.4%\n",
            "train 210\n",
            "n_episode :210, score : -10482.0, n_buffer : 200000, eps : 52.2%\n",
            "train 211\n",
            "n_episode :211, score : -1639.0, n_buffer : 200000, eps : 52.1%\n",
            "train 212\n",
            "n_episode :212, score : -37959.0, n_buffer : 200000, eps : 51.9%\n",
            "train 213\n",
            "n_episode :213, score : -35547.0, n_buffer : 200000, eps : 51.7%\n",
            "train 214\n",
            "n_episode :214, score : -38004.0, n_buffer : 200000, eps : 51.5%\n",
            "train 215\n",
            "n_episode :215, score : -7525.0, n_buffer : 200000, eps : 51.3%\n",
            "train 216\n",
            "n_episode :216, score : -10896.0, n_buffer : 200000, eps : 51.2%\n",
            "train 217\n",
            "n_episode :217, score : -37473.0, n_buffer : 200000, eps : 51.0%\n",
            "train 218\n",
            "n_episode :218, score : -38067.0, n_buffer : 200000, eps : 50.8%\n",
            "train 219\n",
            "n_episode :219, score : -38148.0, n_buffer : 200000, eps : 50.6%\n",
            "train 220\n",
            "n_episode :220, score : -1466.0, n_buffer : 200000, eps : 50.4%\n",
            "train 221\n",
            "n_episode :221, score : -36150.0, n_buffer : 200000, eps : 50.3%\n",
            "train 222\n",
            "n_episode :222, score : -8011.0, n_buffer : 200000, eps : 50.1%\n",
            "train 223\n",
            "n_episode :223, score : -37239.0, n_buffer : 200000, eps : 49.9%\n",
            "train 224\n",
            "n_episode :224, score : -37383.0, n_buffer : 200000, eps : 49.7%\n",
            "train 225\n",
            "n_episode :225, score : -37203.0, n_buffer : 200000, eps : 49.5%\n",
            "train 226\n",
            "n_episode :226, score : -36060.0, n_buffer : 200000, eps : 49.4%\n",
            "train 227\n",
            "n_episode :227, score : -36825.0, n_buffer : 200000, eps : 49.2%\n",
            "train 228\n",
            "n_episode :228, score : -34278.0, n_buffer : 200000, eps : 49.0%\n",
            "train 229\n",
            "n_episode :229, score : -36267.0, n_buffer : 200000, eps : 48.8%\n",
            "train 230\n",
            "n_episode :230, score : -35160.0, n_buffer : 200000, eps : 48.6%\n",
            "train 231\n",
            "n_episode :231, score : -35943.0, n_buffer : 200000, eps : 48.5%\n",
            "train 232\n",
            "n_episode :232, score : -37041.0, n_buffer : 200000, eps : 48.3%\n",
            "train 233\n",
            "n_episode :233, score : -36510.0, n_buffer : 200000, eps : 48.1%\n",
            "train 234\n",
            "n_episode :234, score : -2473.0, n_buffer : 200000, eps : 47.9%\n",
            "train 235\n",
            "n_episode :235, score : -36231.0, n_buffer : 200000, eps : 47.7%\n",
            "train 236\n",
            "n_episode :236, score : -5742.0, n_buffer : 200000, eps : 47.6%\n",
            "train 237\n",
            "n_episode :237, score : -35826.0, n_buffer : 200000, eps : 47.4%\n",
            "train 238\n",
            "n_episode :238, score : -6117.0, n_buffer : 200000, eps : 47.2%\n",
            "train 239\n",
            "n_episode :239, score : -36654.0, n_buffer : 200000, eps : 47.0%\n",
            "train 240\n",
            "n_episode :240, score : -35754.0, n_buffer : 200000, eps : 46.8%\n",
            "train 241\n",
            "n_episode :241, score : -35880.0, n_buffer : 200000, eps : 46.7%\n",
            "train 242\n",
            "n_episode :242, score : -36726.0, n_buffer : 200000, eps : 46.5%\n",
            "train 243\n",
            "n_episode :243, score : -32937.0, n_buffer : 200000, eps : 46.3%\n",
            "train 244\n",
            "n_episode :244, score : -317.0, n_buffer : 200000, eps : 46.1%\n",
            "train 245\n",
            "n_episode :245, score : -35331.0, n_buffer : 200000, eps : 45.9%\n",
            "train 246\n",
            "n_episode :246, score : -21681.0, n_buffer : 200000, eps : 45.8%\n",
            "train 247\n",
            "n_episode :247, score : -2329.0, n_buffer : 200000, eps : 45.6%\n",
            "train 248\n",
            "n_episode :248, score : -1241.0, n_buffer : 200000, eps : 45.4%\n",
            "train 249\n",
            "n_episode :249, score : -3651.0, n_buffer : 200000, eps : 45.2%\n",
            "train 250\n",
            "n_episode :250, score : -35052.0, n_buffer : 200000, eps : 45.1%\n",
            "train 251\n",
            "n_episode :251, score : -34699.0, n_buffer : 200000, eps : 44.9%\n",
            "train 252\n",
            "n_episode :252, score : -35367.0, n_buffer : 200000, eps : 44.7%\n",
            "train 253\n",
            "n_episode :253, score : -14521.0, n_buffer : 200000, eps : 44.5%\n",
            "train 254\n",
            "n_episode :254, score : -20225.0, n_buffer : 200000, eps : 44.3%\n",
            "train 255\n",
            "n_episode :255, score : -5858.0, n_buffer : 200000, eps : 44.2%\n",
            "train 256\n",
            "n_episode :256, score : -34728.0, n_buffer : 200000, eps : 44.0%\n",
            "train 257\n",
            "n_episode :257, score : -34296.0, n_buffer : 200000, eps : 43.8%\n",
            "train 258\n",
            "n_episode :258, score : -33342.0, n_buffer : 200000, eps : 43.6%\n",
            "train 259\n",
            "n_episode :259, score : -9177.0, n_buffer : 200000, eps : 43.4%\n",
            "train 260\n",
            "n_episode :260, score : -34467.0, n_buffer : 200000, eps : 43.3%\n",
            "train 261\n",
            "n_episode :261, score : -19743.0, n_buffer : 200000, eps : 43.1%\n",
            "train 262\n",
            "n_episode :262, score : -33405.0, n_buffer : 200000, eps : 42.9%\n",
            "train 263\n",
            "n_episode :263, score : -34377.0, n_buffer : 200000, eps : 42.7%\n",
            "train 264\n",
            "n_episode :264, score : -34377.0, n_buffer : 200000, eps : 42.5%\n",
            "train 265\n",
            "n_episode :265, score : -11042.0, n_buffer : 200000, eps : 42.4%\n",
            "train 266\n",
            "n_episode :266, score : -33981.0, n_buffer : 200000, eps : 42.2%\n",
            "train 267\n",
            "n_episode :267, score : -33567.0, n_buffer : 200000, eps : 42.0%\n",
            "train 268\n",
            "n_episode :268, score : -19388.0, n_buffer : 200000, eps : 41.8%\n",
            "train 269\n",
            "n_episode :269, score : -32406.0, n_buffer : 200000, eps : 41.6%\n",
            "train 270\n",
            "n_episode :270, score : -33126.0, n_buffer : 200000, eps : 41.5%\n",
            "train 271\n",
            "n_episode :271, score : -31002.0, n_buffer : 200000, eps : 41.3%\n",
            "train 272\n",
            "n_episode :272, score : -8724.0, n_buffer : 200000, eps : 41.1%\n",
            "train 273\n",
            "n_episode :273, score : -33621.0, n_buffer : 200000, eps : 40.9%\n",
            "train 274\n",
            "n_episode :274, score : -33666.0, n_buffer : 200000, eps : 40.7%\n",
            "train 275\n",
            "n_episode :275, score : -33558.0, n_buffer : 200000, eps : 40.6%\n",
            "train 276\n",
            "n_episode :276, score : -1304.0, n_buffer : 200000, eps : 40.4%\n",
            "train 277\n",
            "n_episode :277, score : -31731.0, n_buffer : 200000, eps : 40.2%\n",
            "train 278\n",
            "n_episode :278, score : -33081.0, n_buffer : 200000, eps : 40.0%\n",
            "train 279\n",
            "n_episode :279, score : -32604.0, n_buffer : 200000, eps : 39.8%\n",
            "train 280\n",
            "n_episode :280, score : -31623.0, n_buffer : 200000, eps : 39.7%\n",
            "train 281\n",
            "n_episode :281, score : -33036.0, n_buffer : 200000, eps : 39.5%\n",
            "train 282\n",
            "n_episode :282, score : -7053.0, n_buffer : 200000, eps : 39.3%\n",
            "train 283\n",
            "n_episode :283, score : -15579.0, n_buffer : 200000, eps : 39.1%\n",
            "train 284\n",
            "n_episode :284, score : -32172.0, n_buffer : 200000, eps : 38.9%\n",
            "train 285\n",
            "n_episode :285, score : -31938.0, n_buffer : 200000, eps : 38.8%\n",
            "train 286\n",
            "n_episode :286, score : -16194.0, n_buffer : 200000, eps : 38.6%\n",
            "train 287\n",
            "n_episode :287, score : -31731.0, n_buffer : 200000, eps : 38.4%\n",
            "train 288\n",
            "n_episode :288, score : -31812.0, n_buffer : 200000, eps : 38.2%\n",
            "train 289\n",
            "n_episode :289, score : -21576.0, n_buffer : 200000, eps : 38.0%\n",
            "train 290\n",
            "n_episode :290, score : -31398.0, n_buffer : 200000, eps : 37.9%\n",
            "train 291\n",
            "n_episode :291, score : -30174.0, n_buffer : 200000, eps : 37.7%\n",
            "train 292\n",
            "n_episode :292, score : -515.0, n_buffer : 200000, eps : 37.5%\n",
            "train 293\n",
            "n_episode :293, score : -4778.0, n_buffer : 200000, eps : 37.3%\n",
            "train 294\n",
            "n_episode :294, score : -30912.0, n_buffer : 200000, eps : 37.1%\n",
            "train 295\n",
            "n_episode :295, score : -1283.0, n_buffer : 200000, eps : 37.0%\n",
            "train 296\n",
            "n_episode :296, score : -28212.0, n_buffer : 200000, eps : 36.8%\n",
            "train 297\n",
            "n_episode :297, score : -31380.0, n_buffer : 200000, eps : 36.6%\n",
            "train 298\n",
            "n_episode :298, score : -31245.0, n_buffer : 200000, eps : 36.4%\n",
            "train 299\n",
            "n_episode :299, score : -31155.0, n_buffer : 200000, eps : 36.2%\n",
            "train 300\n",
            "n_episode :300, score : -30741.0, n_buffer : 200000, eps : 36.1%\n",
            "train 301\n",
            "n_episode :301, score : -31164.0, n_buffer : 200000, eps : 35.9%\n",
            "train 302\n",
            "n_episode :302, score : -31533.0, n_buffer : 200000, eps : 35.7%\n",
            "train 303\n",
            "n_episode :303, score : -30903.0, n_buffer : 200000, eps : 35.5%\n",
            "train 304\n",
            "n_episode :304, score : -31353.0, n_buffer : 200000, eps : 35.3%\n",
            "train 305\n",
            "n_episode :305, score : -30282.0, n_buffer : 200000, eps : 35.2%\n",
            "train 306\n",
            "n_episode :306, score : -24828.0, n_buffer : 200000, eps : 35.0%\n",
            "train 307\n",
            "n_episode :307, score : -30615.0, n_buffer : 200000, eps : 34.8%\n",
            "train 308\n",
            "n_episode :308, score : -8953.0, n_buffer : 200000, eps : 34.6%\n",
            "train 309\n",
            "n_episode :309, score : -11398.0, n_buffer : 200000, eps : 34.4%\n",
            "train 310\n",
            "n_episode :310, score : -27384.0, n_buffer : 200000, eps : 34.3%\n",
            "train 311\n",
            "n_episode :311, score : -29994.0, n_buffer : 200000, eps : 34.1%\n",
            "train 312\n",
            "n_episode :312, score : -28455.0, n_buffer : 200000, eps : 33.9%\n",
            "train 313\n",
            "n_episode :313, score : -1049.0, n_buffer : 200000, eps : 33.7%\n",
            "train 314\n",
            "n_episode :314, score : -29760.0, n_buffer : 200000, eps : 33.5%\n",
            "train 315\n",
            "n_episode :315, score : -29679.0, n_buffer : 200000, eps : 33.4%\n",
            "train 316\n",
            "n_episode :316, score : -29040.0, n_buffer : 200000, eps : 33.2%\n",
            "train 317\n",
            "n_episode :317, score : -30462.0, n_buffer : 200000, eps : 33.0%\n",
            "train 318\n",
            "n_episode :318, score : -29364.0, n_buffer : 200000, eps : 32.8%\n",
            "train 319\n",
            "n_episode :319, score : -29877.0, n_buffer : 200000, eps : 32.6%\n",
            "train 320\n",
            "n_episode :320, score : -29553.0, n_buffer : 200000, eps : 32.5%\n",
            "train 321\n",
            "n_episode :321, score : -29139.0, n_buffer : 200000, eps : 32.3%\n",
            "train 322\n",
            "n_episode :322, score : -28905.0, n_buffer : 200000, eps : 32.1%\n",
            "train 323\n",
            "n_episode :323, score : -29400.0, n_buffer : 200000, eps : 31.9%\n",
            "train 324\n",
            "n_episode :324, score : -28932.0, n_buffer : 200000, eps : 31.7%\n",
            "train 325\n",
            "n_episode :325, score : -25665.0, n_buffer : 200000, eps : 31.6%\n",
            "train 326\n",
            "n_episode :326, score : -29103.0, n_buffer : 200000, eps : 31.4%\n",
            "train 327\n",
            "n_episode :327, score : -29202.0, n_buffer : 200000, eps : 31.2%\n",
            "train 328\n",
            "n_episode :328, score : -28932.0, n_buffer : 200000, eps : 31.0%\n",
            "train 329\n",
            "n_episode :329, score : -28860.0, n_buffer : 200000, eps : 30.8%\n",
            "train 330\n",
            "n_episode :330, score : -15484.0, n_buffer : 200000, eps : 30.7%\n",
            "train 331\n",
            "n_episode :331, score : -28905.0, n_buffer : 200000, eps : 30.5%\n",
            "train 332\n",
            "n_episode :332, score : -29157.0, n_buffer : 200000, eps : 30.3%\n",
            "train 333\n",
            "n_episode :333, score : -28455.0, n_buffer : 200000, eps : 30.1%\n",
            "train 334\n",
            "n_episode :334, score : -27546.0, n_buffer : 200000, eps : 29.9%\n",
            "train 335\n",
            "n_episode :335, score : -28590.0, n_buffer : 200000, eps : 29.8%\n",
            "train 336\n",
            "n_episode :336, score : -28167.0, n_buffer : 200000, eps : 29.6%\n",
            "train 337\n",
            "n_episode :337, score : -28410.0, n_buffer : 200000, eps : 29.4%\n",
            "train 338\n",
            "n_episode :338, score : -27951.0, n_buffer : 200000, eps : 29.2%\n",
            "train 339\n",
            "n_episode :339, score : -27078.0, n_buffer : 200000, eps : 29.0%\n",
            "train 340\n",
            "n_episode :340, score : -27879.0, n_buffer : 200000, eps : 28.9%\n",
            "train 341\n",
            "n_episode :341, score : -5387.0, n_buffer : 200000, eps : 28.7%\n",
            "train 342\n",
            "n_episode :342, score : -27843.0, n_buffer : 200000, eps : 28.5%\n",
            "train 343\n",
            "n_episode :343, score : -28086.0, n_buffer : 200000, eps : 28.3%\n",
            "train 344\n",
            "n_episode :344, score : -27870.0, n_buffer : 200000, eps : 28.1%\n",
            "train 345\n",
            "n_episode :345, score : -5496.0, n_buffer : 200000, eps : 28.0%\n",
            "train 346\n",
            "n_episode :346, score : -27078.0, n_buffer : 200000, eps : 27.8%\n",
            "train 347\n",
            "n_episode :347, score : -2334.0, n_buffer : 200000, eps : 27.6%\n",
            "train 348\n",
            "n_episode :348, score : -27645.0, n_buffer : 200000, eps : 27.4%\n",
            "train 349\n",
            "n_episode :349, score : -26412.0, n_buffer : 200000, eps : 27.2%\n",
            "train 350\n",
            "n_episode :350, score : -27150.0, n_buffer : 200000, eps : 27.1%\n",
            "train 351\n",
            "n_episode :351, score : -26916.0, n_buffer : 200000, eps : 26.9%\n",
            "train 352\n",
            "n_episode :352, score : -27249.0, n_buffer : 200000, eps : 26.7%\n",
            "train 353\n",
            "n_episode :353, score : -27303.0, n_buffer : 200000, eps : 26.5%\n",
            "train 354\n",
            "n_episode :354, score : -26592.0, n_buffer : 200000, eps : 26.4%\n",
            "train 355\n",
            "n_episode :355, score : -25827.0, n_buffer : 200000, eps : 26.2%\n",
            "train 356\n",
            "n_episode :356, score : -25404.0, n_buffer : 200000, eps : 26.0%\n",
            "train 357\n",
            "n_episode :357, score : -26637.0, n_buffer : 200000, eps : 25.8%\n",
            "train 358\n",
            "n_episode :358, score : -25080.0, n_buffer : 200000, eps : 25.6%\n",
            "train 359\n",
            "n_episode :359, score : -1117.0, n_buffer : 200000, eps : 25.5%\n",
            "train 360\n",
            "n_episode :360, score : -26637.0, n_buffer : 200000, eps : 25.3%\n",
            "train 361\n",
            "n_episode :361, score : -25854.0, n_buffer : 200000, eps : 25.1%\n",
            "train 362\n",
            "n_episode :362, score : -26475.0, n_buffer : 200000, eps : 24.9%\n",
            "train 363\n",
            "n_episode :363, score : -25899.0, n_buffer : 200000, eps : 24.7%\n",
            "train 364\n",
            "n_episode :364, score : -25575.0, n_buffer : 200000, eps : 24.6%\n",
            "train 365\n",
            "n_episode :365, score : -25539.0, n_buffer : 200000, eps : 24.4%\n",
            "train 366\n",
            "n_episode :366, score : -25386.0, n_buffer : 200000, eps : 24.2%\n",
            "train 367\n",
            "n_episode :367, score : -25791.0, n_buffer : 200000, eps : 24.0%\n",
            "train 368\n",
            "n_episode :368, score : -25170.0, n_buffer : 200000, eps : 23.8%\n",
            "train 369\n",
            "n_episode :369, score : -25242.0, n_buffer : 200000, eps : 23.7%\n",
            "train 370\n",
            "n_episode :370, score : -25773.0, n_buffer : 200000, eps : 23.5%\n",
            "train 371\n",
            "n_episode :371, score : -24999.0, n_buffer : 200000, eps : 23.3%\n",
            "train 372\n",
            "n_episode :372, score : -25368.0, n_buffer : 200000, eps : 23.1%\n",
            "train 373\n",
            "n_episode :373, score : -25206.0, n_buffer : 200000, eps : 22.9%\n",
            "train 374\n",
            "n_episode :374, score : -25611.0, n_buffer : 200000, eps : 22.8%\n",
            "train 375\n",
            "n_episode :375, score : -24549.0, n_buffer : 200000, eps : 22.6%\n",
            "train 376\n",
            "n_episode :376, score : -25035.0, n_buffer : 200000, eps : 22.4%\n",
            "train 377\n",
            "n_episode :377, score : -14206.0, n_buffer : 200000, eps : 22.2%\n",
            "train 378\n",
            "n_episode :378, score : -24702.0, n_buffer : 200000, eps : 22.0%\n",
            "train 379\n",
            "n_episode :379, score : -23118.0, n_buffer : 200000, eps : 21.9%\n",
            "train 380\n",
            "n_episode :380, score : -24873.0, n_buffer : 200000, eps : 21.7%\n",
            "train 381\n",
            "n_episode :381, score : -23001.0, n_buffer : 200000, eps : 21.5%\n",
            "train 382\n",
            "n_episode :382, score : -24747.0, n_buffer : 200000, eps : 21.3%\n",
            "train 383\n",
            "n_episode :383, score : -24351.0, n_buffer : 200000, eps : 21.1%\n",
            "train 384\n",
            "n_episode :384, score : -24117.0, n_buffer : 200000, eps : 21.0%\n",
            "train 385\n",
            "n_episode :385, score : -2888.0, n_buffer : 200000, eps : 20.8%\n",
            "train 386\n",
            "n_episode :386, score : -24351.0, n_buffer : 200000, eps : 20.6%\n",
            "train 387\n",
            "n_episode :387, score : -24081.0, n_buffer : 200000, eps : 20.4%\n",
            "train 388\n",
            "n_episode :388, score : -23361.0, n_buffer : 200000, eps : 20.2%\n",
            "train 389\n",
            "n_episode :389, score : -23784.0, n_buffer : 200000, eps : 20.1%\n",
            "train 390\n",
            "n_episode :390, score : -23433.0, n_buffer : 200000, eps : 19.9%\n",
            "train 391\n",
            "n_episode :391, score : -7060.0, n_buffer : 200000, eps : 19.7%\n",
            "train 392\n",
            "n_episode :392, score : -23703.0, n_buffer : 200000, eps : 19.5%\n",
            "train 393\n",
            "n_episode :393, score : -23811.0, n_buffer : 200000, eps : 19.3%\n",
            "train 394\n",
            "n_episode :394, score : -23712.0, n_buffer : 200000, eps : 19.2%\n",
            "train 395\n",
            "n_episode :395, score : -23631.0, n_buffer : 200000, eps : 19.0%\n",
            "train 396\n",
            "n_episode :396, score : -3324.0, n_buffer : 200000, eps : 18.8%\n",
            "train 397\n",
            "n_episode :397, score : -23442.0, n_buffer : 200000, eps : 18.6%\n",
            "train 398\n",
            "n_episode :398, score : -23145.0, n_buffer : 200000, eps : 18.4%\n",
            "train 399\n",
            "n_episode :399, score : -22686.0, n_buffer : 200000, eps : 18.3%\n",
            "train 400\n",
            "n_episode :400, score : -23469.0, n_buffer : 200000, eps : 18.1%\n",
            "train 401\n",
            "n_episode :401, score : -23271.0, n_buffer : 200000, eps : 17.9%\n",
            "train 402\n",
            "n_episode :402, score : -22533.0, n_buffer : 200000, eps : 17.7%\n",
            "train 403\n",
            "n_episode :403, score : -3986.0, n_buffer : 200000, eps : 17.5%\n",
            "train 404\n",
            "n_episode :404, score : -23019.0, n_buffer : 200000, eps : 17.4%\n",
            "train 405\n",
            "n_episode :405, score : -23109.0, n_buffer : 200000, eps : 17.2%\n",
            "train 406\n",
            "n_episode :406, score : -22884.0, n_buffer : 200000, eps : 17.0%\n",
            "train 407\n",
            "n_episode :407, score : -22209.0, n_buffer : 200000, eps : 16.8%\n",
            "train 408\n",
            "n_episode :408, score : -22281.0, n_buffer : 200000, eps : 16.6%\n",
            "train 409\n",
            "n_episode :409, score : -22857.0, n_buffer : 200000, eps : 16.5%\n",
            "train 410\n",
            "n_episode :410, score : -22218.0, n_buffer : 200000, eps : 16.3%\n",
            "train 411\n",
            "n_episode :411, score : -22218.0, n_buffer : 200000, eps : 16.1%\n",
            "train 412\n",
            "n_episode :412, score : -21984.0, n_buffer : 200000, eps : 15.9%\n",
            "train 413\n",
            "n_episode :413, score : -1317.0, n_buffer : 200000, eps : 15.7%\n",
            "train 414\n",
            "n_episode :414, score : -21939.0, n_buffer : 200000, eps : 15.6%\n",
            "train 415\n",
            "n_episode :415, score : -20967.0, n_buffer : 200000, eps : 15.4%\n",
            "train 416\n",
            "n_episode :416, score : -21894.0, n_buffer : 200000, eps : 15.2%\n",
            "train 417\n",
            "n_episode :417, score : -21966.0, n_buffer : 200000, eps : 15.0%\n",
            "train 418\n",
            "n_episode :418, score : -21354.0, n_buffer : 200000, eps : 14.8%\n",
            "train 419\n",
            "n_episode :419, score : -21372.0, n_buffer : 200000, eps : 14.7%\n",
            "train 420\n",
            "n_episode :420, score : -21300.0, n_buffer : 200000, eps : 14.5%\n",
            "train 421\n",
            "n_episode :421, score : -20031.0, n_buffer : 200000, eps : 14.3%\n",
            "train 422\n",
            "n_episode :422, score : -21228.0, n_buffer : 200000, eps : 14.1%\n",
            "train 423\n",
            "n_episode :423, score : -21444.0, n_buffer : 200000, eps : 13.9%\n",
            "train 424\n",
            "n_episode :424, score : -13134.0, n_buffer : 200000, eps : 13.8%\n",
            "train 425\n",
            "n_episode :425, score : -20922.0, n_buffer : 200000, eps : 13.6%\n",
            "train 426\n",
            "n_episode :426, score : -20976.0, n_buffer : 200000, eps : 13.4%\n",
            "train 427\n",
            "n_episode :427, score : -20778.0, n_buffer : 200000, eps : 13.2%\n",
            "train 428\n",
            "n_episode :428, score : -749.0, n_buffer : 200000, eps : 13.0%\n",
            "train 429\n",
            "n_episode :429, score : -20940.0, n_buffer : 200000, eps : 12.9%\n",
            "train 430\n",
            "n_episode :430, score : -20886.0, n_buffer : 200000, eps : 12.7%\n",
            "train 431\n",
            "n_episode :431, score : -20382.0, n_buffer : 200000, eps : 12.5%\n",
            "train 432\n",
            "n_episode :432, score : -20679.0, n_buffer : 200000, eps : 12.3%\n",
            "train 433\n",
            "n_episode :433, score : -17511.0, n_buffer : 200000, eps : 12.1%\n",
            "train 434\n",
            "n_episode :434, score : -20310.0, n_buffer : 200000, eps : 12.0%\n",
            "train 435\n",
            "n_episode :435, score : -20328.0, n_buffer : 200000, eps : 11.8%\n",
            "train 436\n",
            "n_episode :436, score : -20490.0, n_buffer : 200000, eps : 11.6%\n",
            "train 437\n",
            "n_episode :437, score : -379.0, n_buffer : 200000, eps : 11.4%\n",
            "train 438\n",
            "n_episode :438, score : -19752.0, n_buffer : 200000, eps : 11.2%\n",
            "train 439\n",
            "n_episode :439, score : -20031.0, n_buffer : 200000, eps : 11.1%\n",
            "train 440\n",
            "n_episode :440, score : -20247.0, n_buffer : 200000, eps : 10.9%\n",
            "train 441\n",
            "n_episode :441, score : -19806.0, n_buffer : 200000, eps : 10.7%\n",
            "train 442\n",
            "n_episode :442, score : -19518.0, n_buffer : 200000, eps : 10.5%\n",
            "train 443\n",
            "n_episode :443, score : -19275.0, n_buffer : 200000, eps : 10.3%\n",
            "train 444\n",
            "n_episode :444, score : -19500.0, n_buffer : 200000, eps : 10.2%\n",
            "train 445\n",
            "n_episode :445, score : -19644.0, n_buffer : 200000, eps : 10.0%\n",
            "train 446\n",
            "n_episode :446, score : -19248.0, n_buffer : 200000, eps : 9.8%\n",
            "train 447\n",
            "n_episode :447, score : -19374.0, n_buffer : 200000, eps : 9.6%\n",
            "train 448\n",
            "n_episode :448, score : -19365.0, n_buffer : 200000, eps : 9.4%\n",
            "train 449\n",
            "n_episode :449, score : -6090.0, n_buffer : 200000, eps : 9.3%\n",
            "train 450\n",
            "n_episode :450, score : -19014.0, n_buffer : 200000, eps : 9.1%\n",
            "train 451\n",
            "n_episode :451, score : -19059.0, n_buffer : 200000, eps : 8.9%\n",
            "train 452\n",
            "n_episode :452, score : -19068.0, n_buffer : 200000, eps : 8.7%\n",
            "train 453\n",
            "n_episode :453, score : -18960.0, n_buffer : 200000, eps : 8.6%\n",
            "train 454\n",
            "n_episode :454, score : -18528.0, n_buffer : 200000, eps : 8.4%\n",
            "train 455\n",
            "n_episode :455, score : -3616.0, n_buffer : 200000, eps : 8.2%\n",
            "train 456\n",
            "n_episode :456, score : -18744.0, n_buffer : 200000, eps : 8.0%\n",
            "train 457\n",
            "n_episode :457, score : -18285.0, n_buffer : 200000, eps : 7.8%\n",
            "train 458\n",
            "n_episode :458, score : -18294.0, n_buffer : 200000, eps : 7.7%\n",
            "train 459\n",
            "n_episode :459, score : -18384.0, n_buffer : 200000, eps : 7.5%\n",
            "train 460\n",
            "n_episode :460, score : -18375.0, n_buffer : 200000, eps : 7.3%\n",
            "train 461\n",
            "n_episode :461, score : -18123.0, n_buffer : 200000, eps : 7.1%\n",
            "train 462\n",
            "n_episode :462, score : -18105.0, n_buffer : 200000, eps : 6.9%\n",
            "train 463\n",
            "n_episode :463, score : -18033.0, n_buffer : 200000, eps : 6.8%\n",
            "train 464\n",
            "n_episode :464, score : -18024.0, n_buffer : 200000, eps : 6.6%\n",
            "train 465\n",
            "n_episode :465, score : -17349.0, n_buffer : 200000, eps : 6.4%\n",
            "train 466\n",
            "n_episode :466, score : -17646.0, n_buffer : 200000, eps : 6.2%\n",
            "train 467\n",
            "n_episode :467, score : -17844.0, n_buffer : 200000, eps : 6.0%\n",
            "train 468\n",
            "n_episode :468, score : -17538.0, n_buffer : 200000, eps : 5.9%\n",
            "train 469\n",
            "n_episode :469, score : -17484.0, n_buffer : 200000, eps : 5.7%\n",
            "train 470\n",
            "n_episode :470, score : -17565.0, n_buffer : 200000, eps : 5.5%\n",
            "train 471\n",
            "n_episode :471, score : -17277.0, n_buffer : 200000, eps : 5.3%\n",
            "train 472\n",
            "n_episode :472, score : -17232.0, n_buffer : 200000, eps : 5.1%\n",
            "train 473\n",
            "n_episode :473, score : -17214.0, n_buffer : 200000, eps : 5.0%\n",
            "train 474\n",
            "n_episode :474, score : -17331.0, n_buffer : 200000, eps : 4.8%\n",
            "train 475\n",
            "n_episode :475, score : -17106.0, n_buffer : 200000, eps : 4.6%\n",
            "train 476\n",
            "n_episode :476, score : -16944.0, n_buffer : 200000, eps : 4.4%\n",
            "train 477\n",
            "n_episode :477, score : -16854.0, n_buffer : 200000, eps : 4.2%\n",
            "train 478\n",
            "n_episode :478, score : -16368.0, n_buffer : 200000, eps : 4.1%\n",
            "train 479\n",
            "n_episode :479, score : -16800.0, n_buffer : 200000, eps : 3.9%\n",
            "train 480\n",
            "n_episode :480, score : -16548.0, n_buffer : 200000, eps : 3.7%\n",
            "train 481\n",
            "n_episode :481, score : -16701.0, n_buffer : 200000, eps : 3.5%\n",
            "train 482\n",
            "n_episode :482, score : -16368.0, n_buffer : 200000, eps : 3.3%\n",
            "train 483\n",
            "n_episode :483, score : -16323.0, n_buffer : 200000, eps : 3.2%\n",
            "train 484\n",
            "n_episode :484, score : -16539.0, n_buffer : 200000, eps : 3.0%\n",
            "train 485\n",
            "n_episode :485, score : -16377.0, n_buffer : 200000, eps : 2.8%\n",
            "train 486\n",
            "n_episode :486, score : -16161.0, n_buffer : 200000, eps : 2.6%\n",
            "train 487\n",
            "n_episode :487, score : -16098.0, n_buffer : 200000, eps : 2.4%\n",
            "train 488\n",
            "n_episode :488, score : -1076.0, n_buffer : 200000, eps : 2.3%\n",
            "train 489\n",
            "n_episode :489, score : -16017.0, n_buffer : 200000, eps : 2.1%\n",
            "train 490\n",
            "n_episode :490, score : -15792.0, n_buffer : 200000, eps : 1.9%\n",
            "train 491\n",
            "n_episode :491, score : -15738.0, n_buffer : 200000, eps : 1.7%\n",
            "train 492\n",
            "n_episode :492, score : -15549.0, n_buffer : 200000, eps : 1.5%\n",
            "train 493\n",
            "n_episode :493, score : -15522.0, n_buffer : 200000, eps : 1.4%\n",
            "train 494\n",
            "n_episode :494, score : -15513.0, n_buffer : 200000, eps : 1.2%\n",
            "train 495\n",
            "n_episode :495, score : -15486.0, n_buffer : 200000, eps : 1.0%\n",
            "train 496\n",
            "n_episode :496, score : -15369.0, n_buffer : 200000, eps : 0.8%\n",
            "train 497\n",
            "n_episode :497, score : -15360.0, n_buffer : 200000, eps : 0.6%\n",
            "train 498\n",
            "n_episode :498, score : -15153.0, n_buffer : 200000, eps : 0.5%\n",
            "train 499\n",
            "n_episode :499, score : -15117.0, n_buffer : 200000, eps : 0.3%\n",
            "train 500\n",
            "n_episode :500, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 501\n",
            "n_episode :501, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 502\n",
            "n_episode :502, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 503\n",
            "n_episode :503, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 504\n",
            "n_episode :504, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 505\n",
            "n_episode :505, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 506\n",
            "n_episode :506, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 507\n",
            "n_episode :507, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 508\n",
            "n_episode :508, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 509\n",
            "n_episode :509, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 510\n",
            "n_episode :510, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 511\n",
            "n_episode :511, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 512\n",
            "n_episode :512, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 513\n",
            "n_episode :513, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 514\n",
            "n_episode :514, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 515\n",
            "n_episode :515, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 516\n",
            "n_episode :516, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 517\n",
            "n_episode :517, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 518\n",
            "n_episode :518, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 519\n",
            "n_episode :519, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 520\n",
            "n_episode :520, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 521\n",
            "n_episode :521, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 522\n",
            "n_episode :522, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 523\n",
            "n_episode :523, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 524\n",
            "n_episode :524, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 525\n",
            "n_episode :525, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 526\n",
            "n_episode :526, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 527\n",
            "n_episode :527, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 528\n",
            "n_episode :528, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 529\n",
            "n_episode :529, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 530\n",
            "n_episode :530, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 531\n",
            "n_episode :531, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 532\n",
            "n_episode :532, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 533\n",
            "n_episode :533, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 534\n",
            "n_episode :534, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 535\n",
            "n_episode :535, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 536\n",
            "n_episode :536, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 537\n",
            "n_episode :537, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 538\n",
            "n_episode :538, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 539\n",
            "n_episode :539, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 540\n",
            "n_episode :540, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 541\n",
            "n_episode :541, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 542\n",
            "n_episode :542, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 543\n",
            "n_episode :543, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 544\n",
            "n_episode :544, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 545\n",
            "n_episode :545, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 546\n",
            "n_episode :546, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 547\n",
            "n_episode :547, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 548\n",
            "n_episode :548, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 549\n",
            "n_episode :549, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 550\n",
            "n_episode :550, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 551\n",
            "n_episode :551, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 552\n",
            "n_episode :552, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 553\n",
            "n_episode :553, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 554\n",
            "n_episode :554, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 555\n",
            "n_episode :555, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 556\n",
            "n_episode :556, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 557\n",
            "n_episode :557, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 558\n",
            "n_episode :558, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 559\n",
            "n_episode :559, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 560\n",
            "n_episode :560, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 561\n",
            "n_episode :561, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 562\n",
            "n_episode :562, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 563\n",
            "n_episode :563, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 564\n",
            "n_episode :564, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 565\n",
            "n_episode :565, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 566\n",
            "n_episode :566, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 567\n",
            "n_episode :567, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 568\n",
            "n_episode :568, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 569\n",
            "n_episode :569, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 570\n",
            "n_episode :570, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 571\n",
            "n_episode :571, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 572\n",
            "n_episode :572, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 573\n",
            "n_episode :573, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 574\n",
            "n_episode :574, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 575\n",
            "n_episode :575, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 576\n",
            "n_episode :576, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 577\n",
            "n_episode :577, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 578\n",
            "n_episode :578, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 579\n",
            "n_episode :579, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 580\n",
            "n_episode :580, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 581\n",
            "n_episode :581, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 582\n",
            "n_episode :582, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 583\n",
            "n_episode :583, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 584\n",
            "n_episode :584, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 585\n",
            "n_episode :585, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 586\n",
            "n_episode :586, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 587\n",
            "n_episode :587, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 588\n",
            "n_episode :588, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 589\n",
            "n_episode :589, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 590\n",
            "n_episode :590, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 591\n",
            "n_episode :591, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 592\n",
            "n_episode :592, score : -10829.0, n_buffer : 200000, eps : 0.1%\n",
            "train 593\n",
            "n_episode :593, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 594\n",
            "n_episode :594, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 595\n",
            "n_episode :595, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 596\n",
            "n_episode :596, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 597\n",
            "n_episode :597, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 598\n",
            "n_episode :598, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 599\n",
            "n_episode :599, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 600\n",
            "n_episode :600, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 601\n",
            "n_episode :601, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 602\n",
            "n_episode :602, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 603\n",
            "n_episode :603, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 604\n",
            "n_episode :604, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 605\n",
            "n_episode :605, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 606\n",
            "n_episode :606, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 607\n",
            "n_episode :607, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 608\n",
            "n_episode :608, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 609\n",
            "n_episode :609, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 610\n",
            "n_episode :610, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 611\n",
            "n_episode :611, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 612\n",
            "n_episode :612, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 613\n",
            "n_episode :613, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 614\n",
            "n_episode :614, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 615\n",
            "n_episode :615, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 616\n",
            "n_episode :616, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 617\n",
            "n_episode :617, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 618\n",
            "n_episode :618, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 619\n",
            "n_episode :619, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 620\n",
            "n_episode :620, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 621\n",
            "n_episode :621, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 622\n",
            "n_episode :622, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 623\n",
            "n_episode :623, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 624\n",
            "n_episode :624, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 625\n",
            "n_episode :625, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 626\n",
            "n_episode :626, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 627\n",
            "n_episode :627, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 628\n",
            "n_episode :628, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 629\n",
            "n_episode :629, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 630\n",
            "n_episode :630, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 631\n",
            "n_episode :631, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 632\n",
            "n_episode :632, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 633\n",
            "n_episode :633, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 634\n",
            "n_episode :634, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 635\n",
            "n_episode :635, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 636\n",
            "n_episode :636, score : -9086.0, n_buffer : 200000, eps : 0.1%\n",
            "train 637\n",
            "n_episode :637, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 638\n",
            "n_episode :638, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 639\n",
            "n_episode :639, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 640\n",
            "n_episode :640, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 641\n",
            "n_episode :641, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 642\n",
            "n_episode :642, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 643\n",
            "n_episode :643, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 644\n",
            "n_episode :644, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 645\n",
            "n_episode :645, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 646\n",
            "n_episode :646, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 647\n",
            "n_episode :647, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 648\n",
            "n_episode :648, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 649\n",
            "n_episode :649, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 650\n",
            "n_episode :650, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 651\n",
            "n_episode :651, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 652\n",
            "n_episode :652, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 653\n",
            "n_episode :653, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 654\n",
            "n_episode :654, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 655\n",
            "n_episode :655, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 656\n",
            "n_episode :656, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 657\n",
            "n_episode :657, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 658\n",
            "n_episode :658, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 659\n",
            "n_episode :659, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 660\n",
            "n_episode :660, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 661\n",
            "n_episode :661, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 662\n",
            "n_episode :662, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 663\n",
            "n_episode :663, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 664\n",
            "n_episode :664, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 665\n",
            "n_episode :665, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 666\n",
            "n_episode :666, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 667\n",
            "n_episode :667, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 668\n",
            "n_episode :668, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 669\n",
            "n_episode :669, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 670\n",
            "n_episode :670, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 671\n",
            "n_episode :671, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 672\n",
            "n_episode :672, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 673\n",
            "n_episode :673, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 674\n",
            "n_episode :674, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 675\n",
            "n_episode :675, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 676\n",
            "n_episode :676, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 677\n",
            "n_episode :677, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 678\n",
            "n_episode :678, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 679\n",
            "n_episode :679, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 680\n",
            "n_episode :680, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 681\n",
            "n_episode :681, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 682\n",
            "n_episode :682, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 683\n",
            "n_episode :683, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 684\n",
            "n_episode :684, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 685\n",
            "n_episode :685, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 686\n",
            "n_episode :686, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 687\n",
            "n_episode :687, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 688\n",
            "n_episode :688, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 689\n",
            "n_episode :689, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 690\n",
            "n_episode :690, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 691\n",
            "n_episode :691, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 692\n",
            "n_episode :692, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 693\n",
            "n_episode :693, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 694\n",
            "n_episode :694, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 695\n",
            "n_episode :695, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 696\n",
            "n_episode :696, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 697\n",
            "n_episode :697, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 698\n",
            "n_episode :698, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 699\n",
            "n_episode :699, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 700\n",
            "n_episode :700, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 701\n",
            "n_episode :701, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 702\n",
            "n_episode :702, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 703\n",
            "n_episode :703, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 704\n",
            "n_episode :704, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 705\n",
            "n_episode :705, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 706\n",
            "n_episode :706, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 707\n",
            "n_episode :707, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 708\n",
            "n_episode :708, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 709\n",
            "n_episode :709, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 710\n",
            "n_episode :710, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 711\n",
            "n_episode :711, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 712\n",
            "n_episode :712, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 713\n",
            "n_episode :713, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 714\n",
            "n_episode :714, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 715\n",
            "n_episode :715, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 716\n",
            "n_episode :716, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 717\n",
            "n_episode :717, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 718\n",
            "n_episode :718, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 719\n",
            "n_episode :719, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 720\n",
            "n_episode :720, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 721\n",
            "n_episode :721, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 722\n",
            "n_episode :722, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 723\n",
            "n_episode :723, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 724\n",
            "n_episode :724, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 725\n",
            "n_episode :725, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 726\n",
            "n_episode :726, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 727\n",
            "n_episode :727, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 728\n",
            "n_episode :728, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 729\n",
            "n_episode :729, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 730\n",
            "n_episode :730, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 731\n",
            "n_episode :731, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 732\n",
            "n_episode :732, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 733\n",
            "n_episode :733, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 734\n",
            "n_episode :734, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 735\n",
            "n_episode :735, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 736\n",
            "n_episode :736, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 737\n",
            "n_episode :737, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 738\n",
            "n_episode :738, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 739\n",
            "n_episode :739, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 740\n",
            "n_episode :740, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 741\n",
            "n_episode :741, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 742\n",
            "n_episode :742, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 743\n",
            "n_episode :743, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 744\n",
            "n_episode :744, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 745\n",
            "n_episode :745, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 746\n",
            "n_episode :746, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 747\n",
            "n_episode :747, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 748\n",
            "n_episode :748, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 749\n",
            "n_episode :749, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 750\n",
            "n_episode :750, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 751\n",
            "n_episode :751, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 752\n",
            "n_episode :752, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 753\n",
            "n_episode :753, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 754\n",
            "n_episode :754, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 755\n",
            "n_episode :755, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 756\n",
            "n_episode :756, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 757\n",
            "n_episode :757, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 758\n",
            "n_episode :758, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 759\n",
            "n_episode :759, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 760\n",
            "n_episode :760, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 761\n",
            "n_episode :761, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 762\n",
            "n_episode :762, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 763\n",
            "n_episode :763, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 764\n",
            "n_episode :764, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 765\n",
            "n_episode :765, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 766\n",
            "n_episode :766, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 767\n",
            "n_episode :767, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 768\n",
            "n_episode :768, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 769\n",
            "n_episode :769, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 770\n",
            "n_episode :770, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 771\n",
            "n_episode :771, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 772\n",
            "n_episode :772, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 773\n",
            "n_episode :773, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 774\n",
            "n_episode :774, score : -15117.0, n_buffer : 200000, eps : 0.1%\n",
            "train 775\n",
            "n_episode :775, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 776\n",
            "n_episode :776, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 777\n",
            "n_episode :777, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 778\n",
            "n_episode :778, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 779\n",
            "n_episode :779, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 780\n",
            "n_episode :780, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 781\n",
            "n_episode :781, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 782\n",
            "n_episode :782, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 783\n",
            "n_episode :783, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 784\n",
            "n_episode :784, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 785\n",
            "n_episode :785, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 786\n",
            "n_episode :786, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 787\n",
            "n_episode :787, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 788\n",
            "n_episode :788, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 789\n",
            "n_episode :789, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 790\n",
            "n_episode :790, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 791\n",
            "n_episode :791, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 792\n",
            "n_episode :792, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 793\n",
            "n_episode :793, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 794\n",
            "n_episode :794, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 795\n",
            "n_episode :795, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 796\n",
            "n_episode :796, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 797\n",
            "n_episode :797, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 798\n",
            "n_episode :798, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 799\n",
            "n_episode :799, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 800\n",
            "n_episode :800, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 801\n",
            "n_episode :801, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 802\n",
            "n_episode :802, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 803\n",
            "n_episode :803, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 804\n",
            "n_episode :804, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 805\n",
            "n_episode :805, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 806\n",
            "n_episode :806, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 807\n",
            "n_episode :807, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 808\n",
            "n_episode :808, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 809\n",
            "n_episode :809, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 810\n",
            "n_episode :810, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 811\n",
            "n_episode :811, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 812\n",
            "n_episode :812, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 813\n",
            "n_episode :813, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 814\n",
            "n_episode :814, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 815\n",
            "n_episode :815, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 816\n",
            "n_episode :816, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 817\n",
            "n_episode :817, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 818\n",
            "n_episode :818, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 819\n",
            "n_episode :819, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 820\n",
            "n_episode :820, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 821\n",
            "n_episode :821, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 822\n",
            "n_episode :822, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 823\n",
            "n_episode :823, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 824\n",
            "n_episode :824, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 825\n",
            "n_episode :825, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 826\n",
            "n_episode :826, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 827\n",
            "n_episode :827, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 828\n",
            "n_episode :828, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 829\n",
            "n_episode :829, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 830\n",
            "n_episode :830, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 831\n",
            "n_episode :831, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 832\n",
            "n_episode :832, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 833\n",
            "n_episode :833, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 834\n",
            "n_episode :834, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 835\n",
            "n_episode :835, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 836\n",
            "n_episode :836, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 837\n",
            "n_episode :837, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 838\n",
            "n_episode :838, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 839\n",
            "n_episode :839, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 840\n",
            "n_episode :840, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 841\n",
            "n_episode :841, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 842\n",
            "n_episode :842, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 843\n",
            "n_episode :843, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 844\n",
            "n_episode :844, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 845\n",
            "n_episode :845, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 846\n",
            "n_episode :846, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 847\n",
            "n_episode :847, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 848\n",
            "n_episode :848, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 849\n",
            "n_episode :849, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 850\n",
            "n_episode :850, score : -15108.0, n_buffer : 200000, eps : 0.1%\n",
            "train 851\n",
            "n_episode :851, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 852\n",
            "n_episode :852, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 853\n",
            "n_episode :853, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 854\n",
            "n_episode :854, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 855\n",
            "n_episode :855, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 856\n",
            "n_episode :856, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 857\n",
            "n_episode :857, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 858\n",
            "n_episode :858, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 859\n",
            "n_episode :859, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 860\n",
            "n_episode :860, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 861\n",
            "n_episode :861, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 862\n",
            "n_episode :862, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 863\n",
            "n_episode :863, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 864\n",
            "n_episode :864, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 865\n",
            "n_episode :865, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 866\n",
            "n_episode :866, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 867\n",
            "n_episode :867, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 868\n",
            "n_episode :868, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 869\n",
            "n_episode :869, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 870\n",
            "n_episode :870, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 871\n",
            "n_episode :871, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 872\n",
            "n_episode :872, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 873\n",
            "n_episode :873, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 874\n",
            "n_episode :874, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 875\n",
            "n_episode :875, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 876\n",
            "n_episode :876, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 877\n",
            "n_episode :877, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 878\n",
            "n_episode :878, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 879\n",
            "n_episode :879, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 880\n",
            "n_episode :880, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 881\n",
            "n_episode :881, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 882\n",
            "n_episode :882, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 883\n",
            "n_episode :883, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 884\n",
            "n_episode :884, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 885\n",
            "n_episode :885, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 886\n",
            "n_episode :886, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 887\n",
            "n_episode :887, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 888\n",
            "n_episode :888, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 889\n",
            "n_episode :889, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 890\n",
            "n_episode :890, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 891\n",
            "n_episode :891, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 892\n",
            "n_episode :892, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 893\n",
            "n_episode :893, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 894\n",
            "n_episode :894, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 895\n",
            "n_episode :895, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 896\n",
            "n_episode :896, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 897\n",
            "n_episode :897, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 898\n",
            "n_episode :898, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 899\n",
            "n_episode :899, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 900\n",
            "n_episode :900, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 901\n",
            "n_episode :901, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 902\n",
            "n_episode :902, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 903\n",
            "n_episode :903, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 904\n",
            "n_episode :904, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 905\n",
            "n_episode :905, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 906\n",
            "n_episode :906, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 907\n",
            "n_episode :907, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 908\n",
            "n_episode :908, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 909\n",
            "n_episode :909, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 910\n",
            "n_episode :910, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 911\n",
            "n_episode :911, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 912\n",
            "n_episode :912, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 913\n",
            "n_episode :913, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 914\n",
            "n_episode :914, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 915\n",
            "n_episode :915, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 916\n",
            "n_episode :916, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 917\n",
            "n_episode :917, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 918\n",
            "n_episode :918, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 919\n",
            "n_episode :919, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 920\n",
            "n_episode :920, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 921\n",
            "n_episode :921, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 922\n",
            "n_episode :922, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 923\n",
            "n_episode :923, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 924\n",
            "n_episode :924, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 925\n",
            "n_episode :925, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 926\n",
            "n_episode :926, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 927\n",
            "n_episode :927, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 928\n",
            "n_episode :928, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 929\n",
            "n_episode :929, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 930\n",
            "n_episode :930, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 931\n",
            "n_episode :931, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 932\n",
            "n_episode :932, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 933\n",
            "n_episode :933, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 934\n",
            "n_episode :934, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 935\n",
            "n_episode :935, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 936\n",
            "n_episode :936, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 937\n",
            "n_episode :937, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 938\n",
            "n_episode :938, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 939\n",
            "n_episode :939, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 940\n",
            "n_episode :940, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 941\n",
            "n_episode :941, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 942\n",
            "n_episode :942, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 943\n",
            "n_episode :943, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 944\n",
            "n_episode :944, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 945\n",
            "n_episode :945, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 946\n",
            "n_episode :946, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 947\n",
            "n_episode :947, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 948\n",
            "n_episode :948, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 949\n",
            "n_episode :949, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 950\n",
            "n_episode :950, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 951\n",
            "n_episode :951, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 952\n",
            "n_episode :952, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 953\n",
            "n_episode :953, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 954\n",
            "n_episode :954, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 955\n",
            "n_episode :955, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 956\n",
            "n_episode :956, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 957\n",
            "n_episode :957, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 958\n",
            "n_episode :958, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 959\n",
            "n_episode :959, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 960\n",
            "n_episode :960, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 961\n",
            "n_episode :961, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 962\n",
            "n_episode :962, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 963\n",
            "n_episode :963, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 964\n",
            "n_episode :964, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 965\n",
            "n_episode :965, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 966\n",
            "n_episode :966, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 967\n",
            "n_episode :967, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 968\n",
            "n_episode :968, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 969\n",
            "n_episode :969, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 970\n",
            "n_episode :970, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 971\n",
            "n_episode :971, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 972\n",
            "n_episode :972, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 973\n",
            "n_episode :973, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 974\n",
            "n_episode :974, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 975\n",
            "n_episode :975, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 976\n",
            "n_episode :976, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 977\n",
            "n_episode :977, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 978\n",
            "n_episode :978, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 979\n",
            "n_episode :979, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 980\n",
            "n_episode :980, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 981\n",
            "n_episode :981, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 982\n",
            "n_episode :982, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 983\n",
            "n_episode :983, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 984\n",
            "n_episode :984, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 985\n",
            "n_episode :985, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 986\n",
            "n_episode :986, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 987\n",
            "n_episode :987, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 988\n",
            "n_episode :988, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 989\n",
            "n_episode :989, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 990\n",
            "n_episode :990, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 991\n",
            "n_episode :991, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 992\n",
            "n_episode :992, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 993\n",
            "n_episode :993, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 994\n",
            "n_episode :994, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 995\n",
            "n_episode :995, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 996\n",
            "n_episode :996, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 997\n",
            "n_episode :997, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 998\n",
            "n_episode :998, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 999\n",
            "n_episode :999, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1000\n",
            "n_episode :1000, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1001\n",
            "n_episode :1001, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1002\n",
            "n_episode :1002, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1003\n",
            "n_episode :1003, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1004\n",
            "n_episode :1004, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1005\n",
            "n_episode :1005, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1006\n",
            "n_episode :1006, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1007\n",
            "n_episode :1007, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1008\n",
            "n_episode :1008, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1009\n",
            "n_episode :1009, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1010\n",
            "n_episode :1010, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1011\n",
            "n_episode :1011, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1012\n",
            "n_episode :1012, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1013\n",
            "n_episode :1013, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1014\n",
            "n_episode :1014, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1015\n",
            "n_episode :1015, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1016\n",
            "n_episode :1016, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1017\n",
            "n_episode :1017, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1018\n",
            "n_episode :1018, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1019\n",
            "n_episode :1019, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1020\n",
            "n_episode :1020, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1021\n",
            "n_episode :1021, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1022\n",
            "n_episode :1022, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1023\n",
            "n_episode :1023, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1024\n",
            "n_episode :1024, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1025\n",
            "n_episode :1025, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1026\n",
            "n_episode :1026, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1027\n",
            "n_episode :1027, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1028\n",
            "n_episode :1028, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1029\n",
            "n_episode :1029, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1030\n",
            "n_episode :1030, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1031\n",
            "n_episode :1031, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1032\n",
            "n_episode :1032, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1033\n",
            "n_episode :1033, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1034\n",
            "n_episode :1034, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1035\n",
            "n_episode :1035, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1036\n",
            "n_episode :1036, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1037\n",
            "n_episode :1037, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1038\n",
            "n_episode :1038, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1039\n",
            "n_episode :1039, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1040\n",
            "n_episode :1040, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1041\n",
            "n_episode :1041, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1042\n",
            "n_episode :1042, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1043\n",
            "n_episode :1043, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1044\n",
            "n_episode :1044, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1045\n",
            "n_episode :1045, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1046\n",
            "n_episode :1046, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1047\n",
            "n_episode :1047, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1048\n",
            "n_episode :1048, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1049\n",
            "n_episode :1049, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1050\n",
            "n_episode :1050, score : -15108.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1051\n",
            "n_episode :1051, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1052\n",
            "n_episode :1052, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1053\n",
            "n_episode :1053, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1054\n",
            "n_episode :1054, score : -15108.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1055\n",
            "n_episode :1055, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1056\n",
            "n_episode :1056, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1057\n",
            "n_episode :1057, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1058\n",
            "n_episode :1058, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1059\n",
            "n_episode :1059, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1060\n",
            "n_episode :1060, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1061\n",
            "n_episode :1061, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1062\n",
            "n_episode :1062, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1063\n",
            "n_episode :1063, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1064\n",
            "n_episode :1064, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1065\n",
            "n_episode :1065, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1066\n",
            "n_episode :1066, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1067\n",
            "n_episode :1067, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1068\n",
            "n_episode :1068, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1069\n",
            "n_episode :1069, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1070\n",
            "n_episode :1070, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1071\n",
            "n_episode :1071, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1072\n",
            "n_episode :1072, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1073\n",
            "n_episode :1073, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1074\n",
            "n_episode :1074, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1075\n",
            "n_episode :1075, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1076\n",
            "n_episode :1076, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1077\n",
            "n_episode :1077, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1078\n",
            "n_episode :1078, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1079\n",
            "n_episode :1079, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1080\n",
            "n_episode :1080, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1081\n",
            "n_episode :1081, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1082\n",
            "n_episode :1082, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1083\n",
            "n_episode :1083, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1084\n",
            "n_episode :1084, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1085\n",
            "n_episode :1085, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1086\n",
            "n_episode :1086, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1087\n",
            "n_episode :1087, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1088\n",
            "n_episode :1088, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1089\n",
            "n_episode :1089, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1090\n",
            "n_episode :1090, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1091\n",
            "n_episode :1091, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1092\n",
            "n_episode :1092, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1093\n",
            "n_episode :1093, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1094\n",
            "n_episode :1094, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1095\n",
            "n_episode :1095, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1096\n",
            "n_episode :1096, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1097\n",
            "n_episode :1097, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1098\n",
            "n_episode :1098, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1099\n",
            "n_episode :1099, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1100\n",
            "n_episode :1100, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1101\n",
            "n_episode :1101, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1102\n",
            "n_episode :1102, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1103\n",
            "n_episode :1103, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1104\n",
            "n_episode :1104, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1105\n",
            "n_episode :1105, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1106\n",
            "n_episode :1106, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1107\n",
            "n_episode :1107, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1108\n",
            "n_episode :1108, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1109\n",
            "n_episode :1109, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1110\n",
            "n_episode :1110, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1111\n",
            "n_episode :1111, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1112\n",
            "n_episode :1112, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1113\n",
            "n_episode :1113, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1114\n",
            "n_episode :1114, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1115\n",
            "n_episode :1115, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1116\n",
            "n_episode :1116, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1117\n",
            "n_episode :1117, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1118\n",
            "n_episode :1118, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1119\n",
            "n_episode :1119, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1120\n",
            "n_episode :1120, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1121\n",
            "n_episode :1121, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1122\n",
            "n_episode :1122, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1123\n",
            "n_episode :1123, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1124\n",
            "n_episode :1124, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1125\n",
            "n_episode :1125, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1126\n",
            "n_episode :1126, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1127\n",
            "n_episode :1127, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1128\n",
            "n_episode :1128, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1129\n",
            "n_episode :1129, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1130\n",
            "n_episode :1130, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1131\n",
            "n_episode :1131, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1132\n",
            "n_episode :1132, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1133\n",
            "n_episode :1133, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1134\n",
            "n_episode :1134, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1135\n",
            "n_episode :1135, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1136\n",
            "n_episode :1136, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1137\n",
            "n_episode :1137, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1138\n",
            "n_episode :1138, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1139\n",
            "n_episode :1139, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1140\n",
            "n_episode :1140, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1141\n",
            "n_episode :1141, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1142\n",
            "n_episode :1142, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1143\n",
            "n_episode :1143, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1144\n",
            "n_episode :1144, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1145\n",
            "n_episode :1145, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1146\n",
            "n_episode :1146, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1147\n",
            "n_episode :1147, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1148\n",
            "n_episode :1148, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1149\n",
            "n_episode :1149, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1150\n",
            "n_episode :1150, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1151\n",
            "n_episode :1151, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1152\n",
            "n_episode :1152, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1153\n",
            "n_episode :1153, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1154\n",
            "n_episode :1154, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1155\n",
            "n_episode :1155, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1156\n",
            "n_episode :1156, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1157\n",
            "n_episode :1157, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1158\n",
            "n_episode :1158, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1159\n",
            "n_episode :1159, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1160\n",
            "n_episode :1160, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1161\n",
            "n_episode :1161, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1162\n",
            "n_episode :1162, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1163\n",
            "n_episode :1163, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1164\n",
            "n_episode :1164, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1165\n",
            "n_episode :1165, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1166\n",
            "n_episode :1166, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1167\n",
            "n_episode :1167, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1168\n",
            "n_episode :1168, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1169\n",
            "n_episode :1169, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1170\n",
            "n_episode :1170, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1171\n",
            "n_episode :1171, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1172\n",
            "n_episode :1172, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1173\n",
            "n_episode :1173, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1174\n",
            "n_episode :1174, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1175\n",
            "n_episode :1175, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1176\n",
            "n_episode :1176, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1177\n",
            "n_episode :1177, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1178\n",
            "n_episode :1178, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1179\n",
            "n_episode :1179, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1180\n",
            "n_episode :1180, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1181\n",
            "n_episode :1181, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1182\n",
            "n_episode :1182, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1183\n",
            "n_episode :1183, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1184\n",
            "n_episode :1184, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1185\n",
            "n_episode :1185, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1186\n",
            "n_episode :1186, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1187\n",
            "n_episode :1187, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1188\n",
            "n_episode :1188, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1189\n",
            "n_episode :1189, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1190\n",
            "n_episode :1190, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1191\n",
            "n_episode :1191, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1192\n",
            "n_episode :1192, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1193\n",
            "n_episode :1193, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1194\n",
            "n_episode :1194, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1195\n",
            "n_episode :1195, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1196\n",
            "n_episode :1196, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1197\n",
            "n_episode :1197, score : -15108.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1198\n",
            "n_episode :1198, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1199\n",
            "n_episode :1199, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1200\n",
            "n_episode :1200, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1201\n",
            "n_episode :1201, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1202\n",
            "n_episode :1202, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1203\n",
            "n_episode :1203, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1204\n",
            "n_episode :1204, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1205\n",
            "n_episode :1205, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1206\n",
            "n_episode :1206, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1207\n",
            "n_episode :1207, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1208\n",
            "n_episode :1208, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1209\n",
            "n_episode :1209, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1210\n",
            "n_episode :1210, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1211\n",
            "n_episode :1211, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1212\n",
            "n_episode :1212, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1213\n",
            "n_episode :1213, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1214\n",
            "n_episode :1214, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1215\n",
            "n_episode :1215, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1216\n",
            "n_episode :1216, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1217\n",
            "n_episode :1217, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1218\n",
            "n_episode :1218, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1219\n",
            "n_episode :1219, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1220\n",
            "n_episode :1220, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1221\n",
            "n_episode :1221, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1222\n",
            "n_episode :1222, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1223\n",
            "n_episode :1223, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1224\n",
            "n_episode :1224, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1225\n",
            "n_episode :1225, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1226\n",
            "n_episode :1226, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1227\n",
            "n_episode :1227, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1228\n",
            "n_episode :1228, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1229\n",
            "n_episode :1229, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1230\n",
            "n_episode :1230, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1231\n",
            "n_episode :1231, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1232\n",
            "n_episode :1232, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1233\n",
            "n_episode :1233, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1234\n",
            "n_episode :1234, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1235\n",
            "n_episode :1235, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1236\n",
            "n_episode :1236, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1237\n",
            "n_episode :1237, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1238\n",
            "n_episode :1238, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1239\n",
            "n_episode :1239, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1240\n",
            "n_episode :1240, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1241\n",
            "n_episode :1241, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1242\n",
            "n_episode :1242, score : -15108.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1243\n",
            "n_episode :1243, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1244\n",
            "n_episode :1244, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1245\n",
            "n_episode :1245, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1246\n",
            "n_episode :1246, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1247\n",
            "n_episode :1247, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1248\n",
            "n_episode :1248, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1249\n",
            "n_episode :1249, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1250\n",
            "n_episode :1250, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1251\n",
            "n_episode :1251, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1252\n",
            "n_episode :1252, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1253\n",
            "n_episode :1253, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1254\n",
            "n_episode :1254, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1255\n",
            "n_episode :1255, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1256\n",
            "n_episode :1256, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1257\n",
            "n_episode :1257, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1258\n",
            "n_episode :1258, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1259\n",
            "n_episode :1259, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1260\n",
            "n_episode :1260, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1261\n",
            "n_episode :1261, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1262\n",
            "n_episode :1262, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1263\n",
            "n_episode :1263, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1264\n",
            "n_episode :1264, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1265\n",
            "n_episode :1265, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1266\n",
            "n_episode :1266, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1267\n",
            "n_episode :1267, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1268\n",
            "n_episode :1268, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1269\n",
            "n_episode :1269, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1270\n",
            "n_episode :1270, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1271\n",
            "n_episode :1271, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1272\n",
            "n_episode :1272, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1273\n",
            "n_episode :1273, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1274\n",
            "n_episode :1274, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1275\n",
            "n_episode :1275, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1276\n",
            "n_episode :1276, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1277\n",
            "n_episode :1277, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1278\n",
            "n_episode :1278, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1279\n",
            "n_episode :1279, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1280\n",
            "n_episode :1280, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1281\n",
            "n_episode :1281, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1282\n",
            "n_episode :1282, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1283\n",
            "n_episode :1283, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1284\n",
            "n_episode :1284, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1285\n",
            "n_episode :1285, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1286\n",
            "n_episode :1286, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1287\n",
            "n_episode :1287, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1288\n",
            "n_episode :1288, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1289\n",
            "n_episode :1289, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1290\n",
            "n_episode :1290, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1291\n",
            "n_episode :1291, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1292\n",
            "n_episode :1292, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1293\n",
            "n_episode :1293, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1294\n",
            "n_episode :1294, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1295\n",
            "n_episode :1295, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1296\n",
            "n_episode :1296, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1297\n",
            "n_episode :1297, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1298\n",
            "n_episode :1298, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1299\n",
            "n_episode :1299, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1300\n",
            "n_episode :1300, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1301\n",
            "n_episode :1301, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1302\n",
            "n_episode :1302, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1303\n",
            "n_episode :1303, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1304\n",
            "n_episode :1304, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1305\n",
            "n_episode :1305, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1306\n",
            "n_episode :1306, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1307\n",
            "n_episode :1307, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1308\n",
            "n_episode :1308, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1309\n",
            "n_episode :1309, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1310\n",
            "n_episode :1310, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1311\n",
            "n_episode :1311, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1312\n",
            "n_episode :1312, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1313\n",
            "n_episode :1313, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1314\n",
            "n_episode :1314, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1315\n",
            "n_episode :1315, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1316\n",
            "n_episode :1316, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1317\n",
            "n_episode :1317, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1318\n",
            "n_episode :1318, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1319\n",
            "n_episode :1319, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1320\n",
            "n_episode :1320, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1321\n",
            "n_episode :1321, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1322\n",
            "n_episode :1322, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1323\n",
            "n_episode :1323, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1324\n",
            "n_episode :1324, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1325\n",
            "n_episode :1325, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1326\n",
            "n_episode :1326, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1327\n",
            "n_episode :1327, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1328\n",
            "n_episode :1328, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1329\n",
            "n_episode :1329, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1330\n",
            "n_episode :1330, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1331\n",
            "n_episode :1331, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1332\n",
            "n_episode :1332, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1333\n",
            "n_episode :1333, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1334\n",
            "n_episode :1334, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1335\n",
            "n_episode :1335, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1336\n",
            "n_episode :1336, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1337\n",
            "n_episode :1337, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1338\n",
            "n_episode :1338, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1339\n",
            "n_episode :1339, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1340\n",
            "n_episode :1340, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1341\n",
            "n_episode :1341, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1342\n",
            "n_episode :1342, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1343\n",
            "n_episode :1343, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1344\n",
            "n_episode :1344, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1345\n",
            "n_episode :1345, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1346\n",
            "n_episode :1346, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1347\n",
            "n_episode :1347, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1348\n",
            "n_episode :1348, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1349\n",
            "n_episode :1349, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1350\n",
            "n_episode :1350, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1351\n",
            "n_episode :1351, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1352\n",
            "n_episode :1352, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1353\n",
            "n_episode :1353, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1354\n",
            "n_episode :1354, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1355\n",
            "n_episode :1355, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1356\n",
            "n_episode :1356, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1357\n",
            "n_episode :1357, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1358\n",
            "n_episode :1358, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1359\n",
            "n_episode :1359, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1360\n",
            "n_episode :1360, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1361\n",
            "n_episode :1361, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1362\n",
            "n_episode :1362, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1363\n",
            "n_episode :1363, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1364\n",
            "n_episode :1364, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1365\n",
            "n_episode :1365, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1366\n",
            "n_episode :1366, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1367\n",
            "n_episode :1367, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1368\n",
            "n_episode :1368, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1369\n",
            "n_episode :1369, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1370\n",
            "n_episode :1370, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1371\n",
            "n_episode :1371, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1372\n",
            "n_episode :1372, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1373\n",
            "n_episode :1373, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1374\n",
            "n_episode :1374, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1375\n",
            "n_episode :1375, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1376\n",
            "n_episode :1376, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1377\n",
            "n_episode :1377, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1378\n",
            "n_episode :1378, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1379\n",
            "n_episode :1379, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1380\n",
            "n_episode :1380, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1381\n",
            "n_episode :1381, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1382\n",
            "n_episode :1382, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1383\n",
            "n_episode :1383, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1384\n",
            "n_episode :1384, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1385\n",
            "n_episode :1385, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1386\n",
            "n_episode :1386, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1387\n",
            "n_episode :1387, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1388\n",
            "n_episode :1388, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1389\n",
            "n_episode :1389, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1390\n",
            "n_episode :1390, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1391\n",
            "n_episode :1391, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1392\n",
            "n_episode :1392, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1393\n",
            "n_episode :1393, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1394\n",
            "n_episode :1394, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1395\n",
            "n_episode :1395, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1396\n",
            "n_episode :1396, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1397\n",
            "n_episode :1397, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1398\n",
            "n_episode :1398, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1399\n",
            "n_episode :1399, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1400\n",
            "n_episode :1400, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1401\n",
            "n_episode :1401, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1402\n",
            "n_episode :1402, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1403\n",
            "n_episode :1403, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1404\n",
            "n_episode :1404, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1405\n",
            "n_episode :1405, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1406\n",
            "n_episode :1406, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1407\n",
            "n_episode :1407, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1408\n",
            "n_episode :1408, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1409\n",
            "n_episode :1409, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1410\n",
            "n_episode :1410, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1411\n",
            "n_episode :1411, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1412\n",
            "n_episode :1412, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1413\n",
            "n_episode :1413, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1414\n",
            "n_episode :1414, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1415\n",
            "n_episode :1415, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1416\n",
            "n_episode :1416, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1417\n",
            "n_episode :1417, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1418\n",
            "n_episode :1418, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1419\n",
            "n_episode :1419, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1420\n",
            "n_episode :1420, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1421\n",
            "n_episode :1421, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1422\n",
            "n_episode :1422, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1423\n",
            "n_episode :1423, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1424\n",
            "n_episode :1424, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1425\n",
            "n_episode :1425, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1426\n",
            "n_episode :1426, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1427\n",
            "n_episode :1427, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1428\n",
            "n_episode :1428, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1429\n",
            "n_episode :1429, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1430\n",
            "n_episode :1430, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1431\n",
            "n_episode :1431, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1432\n",
            "n_episode :1432, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1433\n",
            "n_episode :1433, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1434\n",
            "n_episode :1434, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1435\n",
            "n_episode :1435, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1436\n",
            "n_episode :1436, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1437\n",
            "n_episode :1437, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1438\n",
            "n_episode :1438, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1439\n",
            "n_episode :1439, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1440\n",
            "n_episode :1440, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1441\n",
            "n_episode :1441, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1442\n",
            "n_episode :1442, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1443\n",
            "n_episode :1443, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1444\n",
            "n_episode :1444, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1445\n",
            "n_episode :1445, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1446\n",
            "n_episode :1446, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1447\n",
            "n_episode :1447, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1448\n",
            "n_episode :1448, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1449\n",
            "n_episode :1449, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1450\n",
            "n_episode :1450, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1451\n",
            "n_episode :1451, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1452\n",
            "n_episode :1452, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1453\n",
            "n_episode :1453, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1454\n",
            "n_episode :1454, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1455\n",
            "n_episode :1455, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1456\n",
            "n_episode :1456, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1457\n",
            "n_episode :1457, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1458\n",
            "n_episode :1458, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1459\n",
            "n_episode :1459, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1460\n",
            "n_episode :1460, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1461\n",
            "n_episode :1461, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1462\n",
            "n_episode :1462, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1463\n",
            "n_episode :1463, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1464\n",
            "n_episode :1464, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1465\n",
            "n_episode :1465, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1466\n",
            "n_episode :1466, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1467\n",
            "n_episode :1467, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1468\n",
            "n_episode :1468, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1469\n",
            "n_episode :1469, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1470\n",
            "n_episode :1470, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1471\n",
            "n_episode :1471, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1472\n",
            "n_episode :1472, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1473\n",
            "n_episode :1473, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1474\n",
            "n_episode :1474, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1475\n",
            "n_episode :1475, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1476\n",
            "n_episode :1476, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1477\n",
            "n_episode :1477, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1478\n",
            "n_episode :1478, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1479\n",
            "n_episode :1479, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1480\n",
            "n_episode :1480, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1481\n",
            "n_episode :1481, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1482\n",
            "n_episode :1482, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1483\n",
            "n_episode :1483, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1484\n",
            "n_episode :1484, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1485\n",
            "n_episode :1485, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1486\n",
            "n_episode :1486, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1487\n",
            "n_episode :1487, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1488\n",
            "n_episode :1488, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1489\n",
            "n_episode :1489, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1490\n",
            "n_episode :1490, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1491\n",
            "n_episode :1491, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1492\n",
            "n_episode :1492, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1493\n",
            "n_episode :1493, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1494\n",
            "n_episode :1494, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1495\n",
            "n_episode :1495, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1496\n",
            "n_episode :1496, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1497\n",
            "n_episode :1497, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1498\n",
            "n_episode :1498, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1499\n",
            "n_episode :1499, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1500\n",
            "n_episode :1500, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1501\n",
            "n_episode :1501, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1502\n",
            "n_episode :1502, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1503\n",
            "n_episode :1503, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1504\n",
            "n_episode :1504, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1505\n",
            "n_episode :1505, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1506\n",
            "n_episode :1506, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1507\n",
            "n_episode :1507, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1508\n",
            "n_episode :1508, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1509\n",
            "n_episode :1509, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1510\n",
            "n_episode :1510, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1511\n",
            "n_episode :1511, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1512\n",
            "n_episode :1512, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1513\n",
            "n_episode :1513, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1514\n",
            "n_episode :1514, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1515\n",
            "n_episode :1515, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1516\n",
            "n_episode :1516, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1517\n",
            "n_episode :1517, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1518\n",
            "n_episode :1518, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1519\n",
            "n_episode :1519, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1520\n",
            "n_episode :1520, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1521\n",
            "n_episode :1521, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1522\n",
            "n_episode :1522, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1523\n",
            "n_episode :1523, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1524\n",
            "n_episode :1524, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1525\n",
            "n_episode :1525, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1526\n",
            "n_episode :1526, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1527\n",
            "n_episode :1527, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1528\n",
            "n_episode :1528, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1529\n",
            "n_episode :1529, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1530\n",
            "n_episode :1530, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1531\n",
            "n_episode :1531, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1532\n",
            "n_episode :1532, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1533\n",
            "n_episode :1533, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1534\n",
            "n_episode :1534, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1535\n",
            "n_episode :1535, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1536\n",
            "n_episode :1536, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1537\n",
            "n_episode :1537, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1538\n",
            "n_episode :1538, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1539\n",
            "n_episode :1539, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1540\n",
            "n_episode :1540, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1541\n",
            "n_episode :1541, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1542\n",
            "n_episode :1542, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1543\n",
            "n_episode :1543, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1544\n",
            "n_episode :1544, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1545\n",
            "n_episode :1545, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1546\n",
            "n_episode :1546, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1547\n",
            "n_episode :1547, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1548\n",
            "n_episode :1548, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1549\n",
            "n_episode :1549, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1550\n",
            "n_episode :1550, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1551\n",
            "n_episode :1551, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1552\n",
            "n_episode :1552, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1553\n",
            "n_episode :1553, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1554\n",
            "n_episode :1554, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1555\n",
            "n_episode :1555, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1556\n",
            "n_episode :1556, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1557\n",
            "n_episode :1557, score : -15108.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1558\n",
            "n_episode :1558, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1559\n",
            "n_episode :1559, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1560\n",
            "n_episode :1560, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1561\n",
            "n_episode :1561, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1562\n",
            "n_episode :1562, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1563\n",
            "n_episode :1563, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1564\n",
            "n_episode :1564, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1565\n",
            "n_episode :1565, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1566\n",
            "n_episode :1566, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1567\n",
            "n_episode :1567, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1568\n",
            "n_episode :1568, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1569\n",
            "n_episode :1569, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1570\n",
            "n_episode :1570, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1571\n",
            "n_episode :1571, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1572\n",
            "n_episode :1572, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1573\n",
            "n_episode :1573, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1574\n",
            "n_episode :1574, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1575\n",
            "n_episode :1575, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1576\n",
            "n_episode :1576, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1577\n",
            "n_episode :1577, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1578\n",
            "n_episode :1578, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1579\n",
            "n_episode :1579, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1580\n",
            "n_episode :1580, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1581\n",
            "n_episode :1581, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1582\n",
            "n_episode :1582, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1583\n",
            "n_episode :1583, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1584\n",
            "n_episode :1584, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1585\n",
            "n_episode :1585, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1586\n",
            "n_episode :1586, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1587\n",
            "n_episode :1587, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1588\n",
            "n_episode :1588, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1589\n",
            "n_episode :1589, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1590\n",
            "n_episode :1590, score : -138.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1591\n",
            "n_episode :1591, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1592\n",
            "n_episode :1592, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1593\n",
            "n_episode :1593, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1594\n",
            "n_episode :1594, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1595\n",
            "n_episode :1595, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1596\n",
            "n_episode :1596, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1597\n",
            "n_episode :1597, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1598\n",
            "n_episode :1598, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1599\n",
            "n_episode :1599, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1600\n",
            "n_episode :1600, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1601\n",
            "n_episode :1601, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1602\n",
            "n_episode :1602, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1603\n",
            "n_episode :1603, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1604\n",
            "n_episode :1604, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1605\n",
            "n_episode :1605, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1606\n",
            "n_episode :1606, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1607\n",
            "n_episode :1607, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1608\n",
            "n_episode :1608, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1609\n",
            "n_episode :1609, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1610\n",
            "n_episode :1610, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1611\n",
            "n_episode :1611, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1612\n",
            "n_episode :1612, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1613\n",
            "n_episode :1613, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1614\n",
            "n_episode :1614, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1615\n",
            "n_episode :1615, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1616\n",
            "n_episode :1616, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1617\n",
            "n_episode :1617, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1618\n",
            "n_episode :1618, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1619\n",
            "n_episode :1619, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1620\n",
            "n_episode :1620, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1621\n",
            "n_episode :1621, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1622\n",
            "n_episode :1622, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1623\n",
            "n_episode :1623, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1624\n",
            "n_episode :1624, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1625\n",
            "n_episode :1625, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1626\n",
            "n_episode :1626, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1627\n",
            "n_episode :1627, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1628\n",
            "n_episode :1628, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1629\n",
            "n_episode :1629, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1630\n",
            "n_episode :1630, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1631\n",
            "n_episode :1631, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1632\n",
            "n_episode :1632, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1633\n",
            "n_episode :1633, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1634\n",
            "n_episode :1634, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1635\n",
            "n_episode :1635, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1636\n",
            "n_episode :1636, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1637\n",
            "n_episode :1637, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1638\n",
            "n_episode :1638, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1639\n",
            "n_episode :1639, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1640\n",
            "n_episode :1640, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1641\n",
            "n_episode :1641, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1642\n",
            "n_episode :1642, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1643\n",
            "n_episode :1643, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1644\n",
            "n_episode :1644, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1645\n",
            "n_episode :1645, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1646\n",
            "n_episode :1646, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1647\n",
            "n_episode :1647, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1648\n",
            "n_episode :1648, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1649\n",
            "n_episode :1649, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1650\n",
            "n_episode :1650, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1651\n",
            "n_episode :1651, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1652\n",
            "n_episode :1652, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1653\n",
            "n_episode :1653, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1654\n",
            "n_episode :1654, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1655\n",
            "n_episode :1655, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1656\n",
            "n_episode :1656, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1657\n",
            "n_episode :1657, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1658\n",
            "n_episode :1658, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1659\n",
            "n_episode :1659, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1660\n",
            "n_episode :1660, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1661\n",
            "n_episode :1661, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1662\n",
            "n_episode :1662, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1663\n",
            "n_episode :1663, score : -15108.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1664\n",
            "n_episode :1664, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1665\n",
            "n_episode :1665, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1666\n",
            "n_episode :1666, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1667\n",
            "n_episode :1667, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1668\n",
            "n_episode :1668, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1669\n",
            "n_episode :1669, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1670\n",
            "n_episode :1670, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1671\n",
            "n_episode :1671, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1672\n",
            "n_episode :1672, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1673\n",
            "n_episode :1673, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1674\n",
            "n_episode :1674, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1675\n",
            "n_episode :1675, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1676\n",
            "n_episode :1676, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1677\n",
            "n_episode :1677, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1678\n",
            "n_episode :1678, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1679\n",
            "n_episode :1679, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1680\n",
            "n_episode :1680, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1681\n",
            "n_episode :1681, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1682\n",
            "n_episode :1682, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1683\n",
            "n_episode :1683, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1684\n",
            "n_episode :1684, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1685\n",
            "n_episode :1685, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1686\n",
            "n_episode :1686, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1687\n",
            "n_episode :1687, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1688\n",
            "n_episode :1688, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1689\n",
            "n_episode :1689, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1690\n",
            "n_episode :1690, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1691\n",
            "n_episode :1691, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1692\n",
            "n_episode :1692, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1693\n",
            "n_episode :1693, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1694\n",
            "n_episode :1694, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1695\n",
            "n_episode :1695, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1696\n",
            "n_episode :1696, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1697\n",
            "n_episode :1697, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1698\n",
            "n_episode :1698, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1699\n",
            "n_episode :1699, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1700\n",
            "n_episode :1700, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1701\n",
            "n_episode :1701, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1702\n",
            "n_episode :1702, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1703\n",
            "n_episode :1703, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1704\n",
            "n_episode :1704, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1705\n",
            "n_episode :1705, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1706\n",
            "n_episode :1706, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1707\n",
            "n_episode :1707, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1708\n",
            "n_episode :1708, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1709\n",
            "n_episode :1709, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1710\n",
            "n_episode :1710, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1711\n",
            "n_episode :1711, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1712\n",
            "n_episode :1712, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1713\n",
            "n_episode :1713, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1714\n",
            "n_episode :1714, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1715\n",
            "n_episode :1715, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1716\n",
            "n_episode :1716, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1717\n",
            "n_episode :1717, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1718\n",
            "n_episode :1718, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1719\n",
            "n_episode :1719, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1720\n",
            "n_episode :1720, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1721\n",
            "n_episode :1721, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1722\n",
            "n_episode :1722, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1723\n",
            "n_episode :1723, score : -15108.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1724\n",
            "n_episode :1724, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1725\n",
            "n_episode :1725, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1726\n",
            "n_episode :1726, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1727\n",
            "n_episode :1727, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1728\n",
            "n_episode :1728, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1729\n",
            "n_episode :1729, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1730\n",
            "n_episode :1730, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1731\n",
            "n_episode :1731, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1732\n",
            "n_episode :1732, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1733\n",
            "n_episode :1733, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1734\n",
            "n_episode :1734, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1735\n",
            "n_episode :1735, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1736\n",
            "n_episode :1736, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1737\n",
            "n_episode :1737, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1738\n",
            "n_episode :1738, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1739\n",
            "n_episode :1739, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1740\n",
            "n_episode :1740, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1741\n",
            "n_episode :1741, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1742\n",
            "n_episode :1742, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1743\n",
            "n_episode :1743, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1744\n",
            "n_episode :1744, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1745\n",
            "n_episode :1745, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1746\n",
            "n_episode :1746, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1747\n",
            "n_episode :1747, score : -10897.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1748\n",
            "n_episode :1748, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1749\n",
            "n_episode :1749, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1750\n",
            "n_episode :1750, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1751\n",
            "n_episode :1751, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1752\n",
            "n_episode :1752, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1753\n",
            "n_episode :1753, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1754\n",
            "n_episode :1754, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1755\n",
            "n_episode :1755, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1756\n",
            "n_episode :1756, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1757\n",
            "n_episode :1757, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1758\n",
            "n_episode :1758, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1759\n",
            "n_episode :1759, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1760\n",
            "n_episode :1760, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1761\n",
            "n_episode :1761, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1762\n",
            "n_episode :1762, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1763\n",
            "n_episode :1763, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1764\n",
            "n_episode :1764, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1765\n",
            "n_episode :1765, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1766\n",
            "n_episode :1766, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1767\n",
            "n_episode :1767, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1768\n",
            "n_episode :1768, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1769\n",
            "n_episode :1769, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1770\n",
            "n_episode :1770, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1771\n",
            "n_episode :1771, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1772\n",
            "n_episode :1772, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1773\n",
            "n_episode :1773, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1774\n",
            "n_episode :1774, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1775\n",
            "n_episode :1775, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1776\n",
            "n_episode :1776, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1777\n",
            "n_episode :1777, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1778\n",
            "n_episode :1778, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1779\n",
            "n_episode :1779, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1780\n",
            "n_episode :1780, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1781\n",
            "n_episode :1781, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1782\n",
            "n_episode :1782, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1783\n",
            "n_episode :1783, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1784\n",
            "n_episode :1784, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1785\n",
            "n_episode :1785, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1786\n",
            "n_episode :1786, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1787\n",
            "n_episode :1787, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1788\n",
            "n_episode :1788, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1789\n",
            "n_episode :1789, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1790\n",
            "n_episode :1790, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1791\n",
            "n_episode :1791, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1792\n",
            "n_episode :1792, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1793\n",
            "n_episode :1793, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1794\n",
            "n_episode :1794, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1795\n",
            "n_episode :1795, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1796\n",
            "n_episode :1796, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1797\n",
            "n_episode :1797, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1798\n",
            "n_episode :1798, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1799\n",
            "n_episode :1799, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1800\n",
            "n_episode :1800, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1801\n",
            "n_episode :1801, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1802\n",
            "n_episode :1802, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1803\n",
            "n_episode :1803, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1804\n",
            "n_episode :1804, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1805\n",
            "n_episode :1805, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1806\n",
            "n_episode :1806, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1807\n",
            "n_episode :1807, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1808\n",
            "n_episode :1808, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1809\n",
            "n_episode :1809, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1810\n",
            "n_episode :1810, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1811\n",
            "n_episode :1811, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1812\n",
            "n_episode :1812, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1813\n",
            "n_episode :1813, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1814\n",
            "n_episode :1814, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1815\n",
            "n_episode :1815, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1816\n",
            "n_episode :1816, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1817\n",
            "n_episode :1817, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1818\n",
            "n_episode :1818, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1819\n",
            "n_episode :1819, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1820\n",
            "n_episode :1820, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1821\n",
            "n_episode :1821, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1822\n",
            "n_episode :1822, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1823\n",
            "n_episode :1823, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1824\n",
            "n_episode :1824, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1825\n",
            "n_episode :1825, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1826\n",
            "n_episode :1826, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1827\n",
            "n_episode :1827, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1828\n",
            "n_episode :1828, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1829\n",
            "n_episode :1829, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1830\n",
            "n_episode :1830, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1831\n",
            "n_episode :1831, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1832\n",
            "n_episode :1832, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1833\n",
            "n_episode :1833, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1834\n",
            "n_episode :1834, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1835\n",
            "n_episode :1835, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1836\n",
            "n_episode :1836, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1837\n",
            "n_episode :1837, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1838\n",
            "n_episode :1838, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1839\n",
            "n_episode :1839, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1840\n",
            "n_episode :1840, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1841\n",
            "n_episode :1841, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1842\n",
            "n_episode :1842, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1843\n",
            "n_episode :1843, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1844\n",
            "n_episode :1844, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1845\n",
            "n_episode :1845, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1846\n",
            "n_episode :1846, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1847\n",
            "n_episode :1847, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1848\n",
            "n_episode :1848, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1849\n",
            "n_episode :1849, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1850\n",
            "n_episode :1850, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1851\n",
            "n_episode :1851, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1852\n",
            "n_episode :1852, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1853\n",
            "n_episode :1853, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1854\n",
            "n_episode :1854, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1855\n",
            "n_episode :1855, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1856\n",
            "n_episode :1856, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1857\n",
            "n_episode :1857, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1858\n",
            "n_episode :1858, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1859\n",
            "n_episode :1859, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1860\n",
            "n_episode :1860, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1861\n",
            "n_episode :1861, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1862\n",
            "n_episode :1862, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1863\n",
            "n_episode :1863, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1864\n",
            "n_episode :1864, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1865\n",
            "n_episode :1865, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1866\n",
            "n_episode :1866, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1867\n",
            "n_episode :1867, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1868\n",
            "n_episode :1868, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1869\n",
            "n_episode :1869, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1870\n",
            "n_episode :1870, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1871\n",
            "n_episode :1871, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1872\n",
            "n_episode :1872, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1873\n",
            "n_episode :1873, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1874\n",
            "n_episode :1874, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1875\n",
            "n_episode :1875, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1876\n",
            "n_episode :1876, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1877\n",
            "n_episode :1877, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1878\n",
            "n_episode :1878, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1879\n",
            "n_episode :1879, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1880\n",
            "n_episode :1880, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1881\n",
            "n_episode :1881, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1882\n",
            "n_episode :1882, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1883\n",
            "n_episode :1883, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1884\n",
            "n_episode :1884, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1885\n",
            "n_episode :1885, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1886\n",
            "n_episode :1886, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1887\n",
            "n_episode :1887, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1888\n",
            "n_episode :1888, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1889\n",
            "n_episode :1889, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1890\n",
            "n_episode :1890, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1891\n",
            "n_episode :1891, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1892\n",
            "n_episode :1892, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1893\n",
            "n_episode :1893, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1894\n",
            "n_episode :1894, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1895\n",
            "n_episode :1895, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1896\n",
            "n_episode :1896, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1897\n",
            "n_episode :1897, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1898\n",
            "n_episode :1898, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1899\n",
            "n_episode :1899, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1900\n",
            "n_episode :1900, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1901\n",
            "n_episode :1901, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1902\n",
            "n_episode :1902, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1903\n",
            "n_episode :1903, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1904\n",
            "n_episode :1904, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1905\n",
            "n_episode :1905, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1906\n",
            "n_episode :1906, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1907\n",
            "n_episode :1907, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1908\n",
            "n_episode :1908, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1909\n",
            "n_episode :1909, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1910\n",
            "n_episode :1910, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1911\n",
            "n_episode :1911, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1912\n",
            "n_episode :1912, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1913\n",
            "n_episode :1913, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1914\n",
            "n_episode :1914, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1915\n",
            "n_episode :1915, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1916\n",
            "n_episode :1916, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1917\n",
            "n_episode :1917, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1918\n",
            "n_episode :1918, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1919\n",
            "n_episode :1919, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1920\n",
            "n_episode :1920, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1921\n",
            "n_episode :1921, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1922\n",
            "n_episode :1922, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1923\n",
            "n_episode :1923, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1924\n",
            "n_episode :1924, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1925\n",
            "n_episode :1925, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1926\n",
            "n_episode :1926, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1927\n",
            "n_episode :1927, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1928\n",
            "n_episode :1928, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1929\n",
            "n_episode :1929, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1930\n",
            "n_episode :1930, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1931\n",
            "n_episode :1931, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1932\n",
            "n_episode :1932, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1933\n",
            "n_episode :1933, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1934\n",
            "n_episode :1934, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1935\n",
            "n_episode :1935, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1936\n",
            "n_episode :1936, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1937\n",
            "n_episode :1937, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1938\n",
            "n_episode :1938, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1939\n",
            "n_episode :1939, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1940\n",
            "n_episode :1940, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1941\n",
            "n_episode :1941, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1942\n",
            "n_episode :1942, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1943\n",
            "n_episode :1943, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1944\n",
            "n_episode :1944, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1945\n",
            "n_episode :1945, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1946\n",
            "n_episode :1946, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1947\n",
            "n_episode :1947, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1948\n",
            "n_episode :1948, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1949\n",
            "n_episode :1949, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1950\n",
            "n_episode :1950, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1951\n",
            "n_episode :1951, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1952\n",
            "n_episode :1952, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1953\n",
            "n_episode :1953, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1954\n",
            "n_episode :1954, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1955\n",
            "n_episode :1955, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1956\n",
            "n_episode :1956, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1957\n",
            "n_episode :1957, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1958\n",
            "n_episode :1958, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1959\n",
            "n_episode :1959, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1960\n",
            "n_episode :1960, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1961\n",
            "n_episode :1961, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1962\n",
            "n_episode :1962, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1963\n",
            "n_episode :1963, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1964\n",
            "n_episode :1964, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1965\n",
            "n_episode :1965, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1966\n",
            "n_episode :1966, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1967\n",
            "n_episode :1967, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1968\n",
            "n_episode :1968, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1969\n",
            "n_episode :1969, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1970\n",
            "n_episode :1970, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1971\n",
            "n_episode :1971, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1972\n",
            "n_episode :1972, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1973\n",
            "n_episode :1973, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1974\n",
            "n_episode :1974, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1975\n",
            "n_episode :1975, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1976\n",
            "n_episode :1976, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1977\n",
            "n_episode :1977, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1978\n",
            "n_episode :1978, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1979\n",
            "n_episode :1979, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1980\n",
            "n_episode :1980, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1981\n",
            "n_episode :1981, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1982\n",
            "n_episode :1982, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1983\n",
            "n_episode :1983, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1984\n",
            "n_episode :1984, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1985\n",
            "n_episode :1985, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1986\n",
            "n_episode :1986, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1987\n",
            "n_episode :1987, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1988\n",
            "n_episode :1988, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1989\n",
            "n_episode :1989, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1990\n",
            "n_episode :1990, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1991\n",
            "n_episode :1991, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1992\n",
            "n_episode :1992, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1993\n",
            "n_episode :1993, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1994\n",
            "n_episode :1994, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1995\n",
            "n_episode :1995, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1996\n",
            "n_episode :1996, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1997\n",
            "n_episode :1997, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1998\n",
            "n_episode :1998, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 1999\n",
            "n_episode :1999, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2000\n",
            "n_episode :2000, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2001\n",
            "n_episode :2001, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2002\n",
            "n_episode :2002, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2003\n",
            "n_episode :2003, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2004\n",
            "n_episode :2004, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2005\n",
            "n_episode :2005, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2006\n",
            "n_episode :2006, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2007\n",
            "n_episode :2007, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2008\n",
            "n_episode :2008, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2009\n",
            "n_episode :2009, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2010\n",
            "n_episode :2010, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2011\n",
            "n_episode :2011, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2012\n",
            "n_episode :2012, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2013\n",
            "n_episode :2013, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2014\n",
            "n_episode :2014, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2015\n",
            "n_episode :2015, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2016\n",
            "n_episode :2016, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2017\n",
            "n_episode :2017, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2018\n",
            "n_episode :2018, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2019\n",
            "n_episode :2019, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2020\n",
            "n_episode :2020, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2021\n",
            "n_episode :2021, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2022\n",
            "n_episode :2022, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2023\n",
            "n_episode :2023, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2024\n",
            "n_episode :2024, score : -15117.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2025\n",
            "n_episode :2025, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2026\n",
            "n_episode :2026, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2027\n",
            "n_episode :2027, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2028\n",
            "n_episode :2028, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2029\n",
            "n_episode :2029, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2030\n",
            "n_episode :2030, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2031\n",
            "n_episode :2031, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2032\n",
            "n_episode :2032, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2033\n",
            "n_episode :2033, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2034\n",
            "n_episode :2034, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2035\n",
            "n_episode :2035, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2036\n",
            "n_episode :2036, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2037\n",
            "n_episode :2037, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2038\n",
            "n_episode :2038, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2039\n",
            "n_episode :2039, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2040\n",
            "n_episode :2040, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2041\n",
            "n_episode :2041, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2042\n",
            "n_episode :2042, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2043\n",
            "n_episode :2043, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2044\n",
            "n_episode :2044, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2045\n",
            "n_episode :2045, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2046\n",
            "n_episode :2046, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2047\n",
            "n_episode :2047, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2048\n",
            "n_episode :2048, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2049\n",
            "n_episode :2049, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2050\n",
            "n_episode :2050, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2051\n",
            "n_episode :2051, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2052\n",
            "n_episode :2052, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2053\n",
            "n_episode :2053, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2054\n",
            "n_episode :2054, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2055\n",
            "n_episode :2055, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2056\n",
            "n_episode :2056, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2057\n",
            "n_episode :2057, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2058\n",
            "n_episode :2058, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2059\n",
            "n_episode :2059, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2060\n",
            "n_episode :2060, score : -15117.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2061\n",
            "n_episode :2061, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2062\n",
            "n_episode :2062, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2063\n",
            "n_episode :2063, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2064\n",
            "n_episode :2064, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2065\n",
            "n_episode :2065, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2066\n",
            "n_episode :2066, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2067\n",
            "n_episode :2067, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2068\n",
            "n_episode :2068, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2069\n",
            "n_episode :2069, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2070\n",
            "n_episode :2070, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2071\n",
            "n_episode :2071, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2072\n",
            "n_episode :2072, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2073\n",
            "n_episode :2073, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2074\n",
            "n_episode :2074, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2075\n",
            "n_episode :2075, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2076\n",
            "n_episode :2076, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2077\n",
            "n_episode :2077, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2078\n",
            "n_episode :2078, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2079\n",
            "n_episode :2079, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2080\n",
            "n_episode :2080, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2081\n",
            "n_episode :2081, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2082\n",
            "n_episode :2082, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2083\n",
            "n_episode :2083, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2084\n",
            "n_episode :2084, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2085\n",
            "n_episode :2085, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2086\n",
            "n_episode :2086, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2087\n",
            "n_episode :2087, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2088\n",
            "n_episode :2088, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2089\n",
            "n_episode :2089, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2090\n",
            "n_episode :2090, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2091\n",
            "n_episode :2091, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2092\n",
            "n_episode :2092, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2093\n",
            "n_episode :2093, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2094\n",
            "n_episode :2094, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2095\n",
            "n_episode :2095, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2096\n",
            "n_episode :2096, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2097\n",
            "n_episode :2097, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2098\n",
            "n_episode :2098, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2099\n",
            "n_episode :2099, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2100\n",
            "n_episode :2100, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2101\n",
            "n_episode :2101, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2102\n",
            "n_episode :2102, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2103\n",
            "n_episode :2103, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2104\n",
            "n_episode :2104, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2105\n",
            "n_episode :2105, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2106\n",
            "n_episode :2106, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2107\n",
            "n_episode :2107, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2108\n",
            "n_episode :2108, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2109\n",
            "n_episode :2109, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2110\n",
            "n_episode :2110, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2111\n",
            "n_episode :2111, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2112\n",
            "n_episode :2112, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2113\n",
            "n_episode :2113, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2114\n",
            "n_episode :2114, score : -15108.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2115\n",
            "n_episode :2115, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2116\n",
            "n_episode :2116, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2117\n",
            "n_episode :2117, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2118\n",
            "n_episode :2118, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2119\n",
            "n_episode :2119, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2120\n",
            "n_episode :2120, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2121\n",
            "n_episode :2121, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2122\n",
            "n_episode :2122, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2123\n",
            "n_episode :2123, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2124\n",
            "n_episode :2124, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2125\n",
            "n_episode :2125, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2126\n",
            "n_episode :2126, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2127\n",
            "n_episode :2127, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2128\n",
            "n_episode :2128, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2129\n",
            "n_episode :2129, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2130\n",
            "n_episode :2130, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2131\n",
            "n_episode :2131, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2132\n",
            "n_episode :2132, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2133\n",
            "n_episode :2133, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2134\n",
            "n_episode :2134, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2135\n",
            "n_episode :2135, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2136\n",
            "n_episode :2136, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2137\n",
            "n_episode :2137, score : -15108.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2138\n",
            "n_episode :2138, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2139\n",
            "n_episode :2139, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2140\n",
            "n_episode :2140, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2141\n",
            "n_episode :2141, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2142\n",
            "n_episode :2142, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2143\n",
            "n_episode :2143, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2144\n",
            "n_episode :2144, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2145\n",
            "n_episode :2145, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2146\n",
            "n_episode :2146, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2147\n",
            "n_episode :2147, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2148\n",
            "n_episode :2148, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2149\n",
            "n_episode :2149, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2150\n",
            "n_episode :2150, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2151\n",
            "n_episode :2151, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2152\n",
            "n_episode :2152, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2153\n",
            "n_episode :2153, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2154\n",
            "n_episode :2154, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2155\n",
            "n_episode :2155, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2156\n",
            "n_episode :2156, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2157\n",
            "n_episode :2157, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2158\n",
            "n_episode :2158, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2159\n",
            "n_episode :2159, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2160\n",
            "n_episode :2160, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2161\n",
            "n_episode :2161, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2162\n",
            "n_episode :2162, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2163\n",
            "n_episode :2163, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2164\n",
            "n_episode :2164, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2165\n",
            "n_episode :2165, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2166\n",
            "n_episode :2166, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2167\n",
            "n_episode :2167, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2168\n",
            "n_episode :2168, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2169\n",
            "n_episode :2169, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2170\n",
            "n_episode :2170, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2171\n",
            "n_episode :2171, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2172\n",
            "n_episode :2172, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2173\n",
            "n_episode :2173, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2174\n",
            "n_episode :2174, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2175\n",
            "n_episode :2175, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2176\n",
            "n_episode :2176, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2177\n",
            "n_episode :2177, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2178\n",
            "n_episode :2178, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2179\n",
            "n_episode :2179, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2180\n",
            "n_episode :2180, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2181\n",
            "n_episode :2181, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2182\n",
            "n_episode :2182, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2183\n",
            "n_episode :2183, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2184\n",
            "n_episode :2184, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2185\n",
            "n_episode :2185, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2186\n",
            "n_episode :2186, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2187\n",
            "n_episode :2187, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2188\n",
            "n_episode :2188, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2189\n",
            "n_episode :2189, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2190\n",
            "n_episode :2190, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2191\n",
            "n_episode :2191, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2192\n",
            "n_episode :2192, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2193\n",
            "n_episode :2193, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2194\n",
            "n_episode :2194, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2195\n",
            "n_episode :2195, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2196\n",
            "n_episode :2196, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2197\n",
            "n_episode :2197, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2198\n",
            "n_episode :2198, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2199\n",
            "n_episode :2199, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2200\n",
            "n_episode :2200, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2201\n",
            "n_episode :2201, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2202\n",
            "n_episode :2202, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2203\n",
            "n_episode :2203, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2204\n",
            "n_episode :2204, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2205\n",
            "n_episode :2205, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2206\n",
            "n_episode :2206, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2207\n",
            "n_episode :2207, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2208\n",
            "n_episode :2208, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2209\n",
            "n_episode :2209, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2210\n",
            "n_episode :2210, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2211\n",
            "n_episode :2211, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2212\n",
            "n_episode :2212, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2213\n",
            "n_episode :2213, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2214\n",
            "n_episode :2214, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2215\n",
            "n_episode :2215, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2216\n",
            "n_episode :2216, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2217\n",
            "n_episode :2217, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2218\n",
            "n_episode :2218, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2219\n",
            "n_episode :2219, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2220\n",
            "n_episode :2220, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2221\n",
            "n_episode :2221, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2222\n",
            "n_episode :2222, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2223\n",
            "n_episode :2223, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2224\n",
            "n_episode :2224, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2225\n",
            "n_episode :2225, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2226\n",
            "n_episode :2226, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2227\n",
            "n_episode :2227, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2228\n",
            "n_episode :2228, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2229\n",
            "n_episode :2229, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2230\n",
            "n_episode :2230, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2231\n",
            "n_episode :2231, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2232\n",
            "n_episode :2232, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2233\n",
            "n_episode :2233, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2234\n",
            "n_episode :2234, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2235\n",
            "n_episode :2235, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2236\n",
            "n_episode :2236, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2237\n",
            "n_episode :2237, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2238\n",
            "n_episode :2238, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2239\n",
            "n_episode :2239, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2240\n",
            "n_episode :2240, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2241\n",
            "n_episode :2241, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2242\n",
            "n_episode :2242, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2243\n",
            "n_episode :2243, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2244\n",
            "n_episode :2244, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2245\n",
            "n_episode :2245, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2246\n",
            "n_episode :2246, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2247\n",
            "n_episode :2247, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2248\n",
            "n_episode :2248, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2249\n",
            "n_episode :2249, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2250\n",
            "n_episode :2250, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2251\n",
            "n_episode :2251, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2252\n",
            "n_episode :2252, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2253\n",
            "n_episode :2253, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2254\n",
            "n_episode :2254, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2255\n",
            "n_episode :2255, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2256\n",
            "n_episode :2256, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2257\n",
            "n_episode :2257, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2258\n",
            "n_episode :2258, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2259\n",
            "n_episode :2259, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2260\n",
            "n_episode :2260, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2261\n",
            "n_episode :2261, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2262\n",
            "n_episode :2262, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2263\n",
            "n_episode :2263, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2264\n",
            "n_episode :2264, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2265\n",
            "n_episode :2265, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2266\n",
            "n_episode :2266, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2267\n",
            "n_episode :2267, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2268\n",
            "n_episode :2268, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2269\n",
            "n_episode :2269, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2270\n",
            "n_episode :2270, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2271\n",
            "n_episode :2271, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2272\n",
            "n_episode :2272, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2273\n",
            "n_episode :2273, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2274\n",
            "n_episode :2274, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2275\n",
            "n_episode :2275, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2276\n",
            "n_episode :2276, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2277\n",
            "n_episode :2277, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2278\n",
            "n_episode :2278, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2279\n",
            "n_episode :2279, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2280\n",
            "n_episode :2280, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2281\n",
            "n_episode :2281, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2282\n",
            "n_episode :2282, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2283\n",
            "n_episode :2283, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2284\n",
            "n_episode :2284, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2285\n",
            "n_episode :2285, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2286\n",
            "n_episode :2286, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2287\n",
            "n_episode :2287, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2288\n",
            "n_episode :2288, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2289\n",
            "n_episode :2289, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2290\n",
            "n_episode :2290, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2291\n",
            "n_episode :2291, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2292\n",
            "n_episode :2292, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2293\n",
            "n_episode :2293, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2294\n",
            "n_episode :2294, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2295\n",
            "n_episode :2295, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2296\n",
            "n_episode :2296, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2297\n",
            "n_episode :2297, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2298\n",
            "n_episode :2298, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2299\n",
            "n_episode :2299, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2300\n",
            "n_episode :2300, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2301\n",
            "n_episode :2301, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2302\n",
            "n_episode :2302, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2303\n",
            "n_episode :2303, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2304\n",
            "n_episode :2304, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2305\n",
            "n_episode :2305, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2306\n",
            "n_episode :2306, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2307\n",
            "n_episode :2307, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2308\n",
            "n_episode :2308, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2309\n",
            "n_episode :2309, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2310\n",
            "n_episode :2310, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2311\n",
            "n_episode :2311, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2312\n",
            "n_episode :2312, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2313\n",
            "n_episode :2313, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2314\n",
            "n_episode :2314, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2315\n",
            "n_episode :2315, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2316\n",
            "n_episode :2316, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2317\n",
            "n_episode :2317, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2318\n",
            "n_episode :2318, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2319\n",
            "n_episode :2319, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2320\n",
            "n_episode :2320, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2321\n",
            "n_episode :2321, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2322\n",
            "n_episode :2322, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2323\n",
            "n_episode :2323, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2324\n",
            "n_episode :2324, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2325\n",
            "n_episode :2325, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2326\n",
            "n_episode :2326, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2327\n",
            "n_episode :2327, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2328\n",
            "n_episode :2328, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2329\n",
            "n_episode :2329, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2330\n",
            "n_episode :2330, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2331\n",
            "n_episode :2331, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2332\n",
            "n_episode :2332, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2333\n",
            "n_episode :2333, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2334\n",
            "n_episode :2334, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2335\n",
            "n_episode :2335, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2336\n",
            "n_episode :2336, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2337\n",
            "n_episode :2337, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2338\n",
            "n_episode :2338, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2339\n",
            "n_episode :2339, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2340\n",
            "n_episode :2340, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2341\n",
            "n_episode :2341, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2342\n",
            "n_episode :2342, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2343\n",
            "n_episode :2343, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2344\n",
            "n_episode :2344, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2345\n",
            "n_episode :2345, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2346\n",
            "n_episode :2346, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2347\n",
            "n_episode :2347, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2348\n",
            "n_episode :2348, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2349\n",
            "n_episode :2349, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2350\n",
            "n_episode :2350, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2351\n",
            "n_episode :2351, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2352\n",
            "n_episode :2352, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2353\n",
            "n_episode :2353, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2354\n",
            "n_episode :2354, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2355\n",
            "n_episode :2355, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2356\n",
            "n_episode :2356, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2357\n",
            "n_episode :2357, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2358\n",
            "n_episode :2358, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2359\n",
            "n_episode :2359, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2360\n",
            "n_episode :2360, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2361\n",
            "n_episode :2361, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2362\n",
            "n_episode :2362, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2363\n",
            "n_episode :2363, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2364\n",
            "n_episode :2364, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2365\n",
            "n_episode :2365, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2366\n",
            "n_episode :2366, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2367\n",
            "n_episode :2367, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2368\n",
            "n_episode :2368, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2369\n",
            "n_episode :2369, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2370\n",
            "n_episode :2370, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2371\n",
            "n_episode :2371, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2372\n",
            "n_episode :2372, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2373\n",
            "n_episode :2373, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2374\n",
            "n_episode :2374, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2375\n",
            "n_episode :2375, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2376\n",
            "n_episode :2376, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2377\n",
            "n_episode :2377, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2378\n",
            "n_episode :2378, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2379\n",
            "n_episode :2379, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2380\n",
            "n_episode :2380, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2381\n",
            "n_episode :2381, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2382\n",
            "n_episode :2382, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2383\n",
            "n_episode :2383, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2384\n",
            "n_episode :2384, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2385\n",
            "n_episode :2385, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2386\n",
            "n_episode :2386, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2387\n",
            "n_episode :2387, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2388\n",
            "n_episode :2388, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2389\n",
            "n_episode :2389, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2390\n",
            "n_episode :2390, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2391\n",
            "n_episode :2391, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2392\n",
            "n_episode :2392, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2393\n",
            "n_episode :2393, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2394\n",
            "n_episode :2394, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2395\n",
            "n_episode :2395, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2396\n",
            "n_episode :2396, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2397\n",
            "n_episode :2397, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2398\n",
            "n_episode :2398, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2399\n",
            "n_episode :2399, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2400\n",
            "n_episode :2400, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2401\n",
            "n_episode :2401, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2402\n",
            "n_episode :2402, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2403\n",
            "n_episode :2403, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2404\n",
            "n_episode :2404, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2405\n",
            "n_episode :2405, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2406\n",
            "n_episode :2406, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2407\n",
            "n_episode :2407, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2408\n",
            "n_episode :2408, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2409\n",
            "n_episode :2409, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2410\n",
            "n_episode :2410, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2411\n",
            "n_episode :2411, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2412\n",
            "n_episode :2412, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2413\n",
            "n_episode :2413, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2414\n",
            "n_episode :2414, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2415\n",
            "n_episode :2415, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2416\n",
            "n_episode :2416, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2417\n",
            "n_episode :2417, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2418\n",
            "n_episode :2418, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2419\n",
            "n_episode :2419, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2420\n",
            "n_episode :2420, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2421\n",
            "n_episode :2421, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2422\n",
            "n_episode :2422, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2423\n",
            "n_episode :2423, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2424\n",
            "n_episode :2424, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2425\n",
            "n_episode :2425, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2426\n",
            "n_episode :2426, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2427\n",
            "n_episode :2427, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2428\n",
            "n_episode :2428, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2429\n",
            "n_episode :2429, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2430\n",
            "n_episode :2430, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2431\n",
            "n_episode :2431, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2432\n",
            "n_episode :2432, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2433\n",
            "n_episode :2433, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2434\n",
            "n_episode :2434, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2435\n",
            "n_episode :2435, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2436\n",
            "n_episode :2436, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2437\n",
            "n_episode :2437, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2438\n",
            "n_episode :2438, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2439\n",
            "n_episode :2439, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2440\n",
            "n_episode :2440, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2441\n",
            "n_episode :2441, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2442\n",
            "n_episode :2442, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2443\n",
            "n_episode :2443, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2444\n",
            "n_episode :2444, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2445\n",
            "n_episode :2445, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2446\n",
            "n_episode :2446, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2447\n",
            "n_episode :2447, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2448\n",
            "n_episode :2448, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2449\n",
            "n_episode :2449, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2450\n",
            "n_episode :2450, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2451\n",
            "n_episode :2451, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2452\n",
            "n_episode :2452, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2453\n",
            "n_episode :2453, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2454\n",
            "n_episode :2454, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2455\n",
            "n_episode :2455, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2456\n",
            "n_episode :2456, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2457\n",
            "n_episode :2457, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2458\n",
            "n_episode :2458, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2459\n",
            "n_episode :2459, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2460\n",
            "n_episode :2460, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2461\n",
            "n_episode :2461, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2462\n",
            "n_episode :2462, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2463\n",
            "n_episode :2463, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2464\n",
            "n_episode :2464, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2465\n",
            "n_episode :2465, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2466\n",
            "n_episode :2466, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2467\n",
            "n_episode :2467, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2468\n",
            "n_episode :2468, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2469\n",
            "n_episode :2469, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2470\n",
            "n_episode :2470, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2471\n",
            "n_episode :2471, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2472\n",
            "n_episode :2472, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2473\n",
            "n_episode :2473, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2474\n",
            "n_episode :2474, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2475\n",
            "n_episode :2475, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2476\n",
            "n_episode :2476, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2477\n",
            "n_episode :2477, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2478\n",
            "n_episode :2478, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2479\n",
            "n_episode :2479, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2480\n",
            "n_episode :2480, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2481\n",
            "n_episode :2481, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2482\n",
            "n_episode :2482, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2483\n",
            "n_episode :2483, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2484\n",
            "n_episode :2484, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2485\n",
            "n_episode :2485, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2486\n",
            "n_episode :2486, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2487\n",
            "n_episode :2487, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2488\n",
            "n_episode :2488, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2489\n",
            "n_episode :2489, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2490\n",
            "n_episode :2490, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2491\n",
            "n_episode :2491, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2492\n",
            "n_episode :2492, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2493\n",
            "n_episode :2493, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2494\n",
            "n_episode :2494, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2495\n",
            "n_episode :2495, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2496\n",
            "n_episode :2496, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2497\n",
            "n_episode :2497, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2498\n",
            "n_episode :2498, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2499\n",
            "n_episode :2499, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2500\n",
            "n_episode :2500, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2501\n",
            "n_episode :2501, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2502\n",
            "n_episode :2502, score : -15000.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2503\n",
            "n_episode :2503, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2504\n",
            "n_episode :2504, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2505\n",
            "n_episode :2505, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2506\n",
            "n_episode :2506, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2507\n",
            "n_episode :2507, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2508\n",
            "n_episode :2508, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2509\n",
            "n_episode :2509, score : -15072.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2510\n",
            "n_episode :2510, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2511\n",
            "n_episode :2511, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2512\n",
            "n_episode :2512, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2513\n",
            "n_episode :2513, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2514\n",
            "n_episode :2514, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2515\n",
            "n_episode :2515, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2516\n",
            "n_episode :2516, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2517\n",
            "n_episode :2517, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2518\n",
            "n_episode :2518, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2519\n",
            "n_episode :2519, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2520\n",
            "n_episode :2520, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2521\n",
            "n_episode :2521, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2522\n",
            "n_episode :2522, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2523\n",
            "n_episode :2523, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2524\n",
            "n_episode :2524, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2525\n",
            "n_episode :2525, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2526\n",
            "n_episode :2526, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2527\n",
            "n_episode :2527, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2528\n",
            "n_episode :2528, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2529\n",
            "n_episode :2529, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2530\n",
            "n_episode :2530, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2531\n",
            "n_episode :2531, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2532\n",
            "n_episode :2532, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2533\n",
            "n_episode :2533, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2534\n",
            "n_episode :2534, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2535\n",
            "n_episode :2535, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2536\n",
            "n_episode :2536, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2537\n",
            "n_episode :2537, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2538\n",
            "n_episode :2538, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2539\n",
            "n_episode :2539, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2540\n",
            "n_episode :2540, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2541\n",
            "n_episode :2541, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2542\n",
            "n_episode :2542, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2543\n",
            "n_episode :2543, score : -15081.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2544\n",
            "n_episode :2544, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2545\n",
            "n_episode :2545, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2546\n",
            "n_episode :2546, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2547\n",
            "n_episode :2547, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2548\n",
            "n_episode :2548, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2549\n",
            "n_episode :2549, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2550\n",
            "n_episode :2550, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2551\n",
            "n_episode :2551, score : -15090.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2552\n",
            "n_episode :2552, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2553\n",
            "n_episode :2553, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2554\n",
            "n_episode :2554, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2555\n",
            "n_episode :2555, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2556\n",
            "n_episode :2556, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2557\n",
            "n_episode :2557, score : -15099.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2558\n",
            "n_episode :2558, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2559\n",
            "n_episode :2559, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2560\n",
            "n_episode :2560, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2561\n",
            "n_episode :2561, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2562\n",
            "n_episode :2562, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2563\n",
            "n_episode :2563, score : -15063.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2564\n",
            "n_episode :2564, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2565\n",
            "n_episode :2565, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2566\n",
            "n_episode :2566, score : -15027.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2567\n",
            "n_episode :2567, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2568\n",
            "n_episode :2568, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2569\n",
            "n_episode :2569, score : -15018.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2570\n",
            "n_episode :2570, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2571\n",
            "n_episode :2571, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2572\n",
            "n_episode :2572, score : -15054.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2573\n",
            "n_episode :2573, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2574\n",
            "n_episode :2574, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2575\n",
            "n_episode :2575, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2576\n",
            "n_episode :2576, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2577\n",
            "n_episode :2577, score : -15009.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2578\n",
            "n_episode :2578, score : -15045.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2579\n",
            "n_episode :2579, score : -15036.0, n_buffer : 200000, eps : 0.1%\n",
            "train 2580\n",
            "n_episode :2580, score : -15063.0, n_buffer : 200000, eps : 0.1%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7e66a50a882a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-7e66a50a882a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 200step으로 제한\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# while not done:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# action을 policy(eplsilon-grredy)에 따라 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# a = random.randint(0,5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-7e66a50a882a>\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(self, obs, epsilon)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# epsilon-greedy로 action 선택하는 함수,  obs : state input 값\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# forward를 진행 (신경망 통과), out은 tensor 형태\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;31m# print(out, out.argmax())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mcoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-7e66a50a882a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# libraries\n",
        "import gym\n",
        "import collections\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# pytorch library is used for deep learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.002\n",
        "gamma = 0.98\n",
        "buffer_limit = 200000        # size of replay buffer, 버퍼에 저장할 데이터 개수\n",
        "batch_size = 1024\n",
        "\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.001\n",
        "MAX_EPISODE = 500\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)    # double-ended queue(deque)를 저장소로 선택,\n",
        "                                                                # buffer_size는 50000으로 설정,\n",
        "                                                                # 50000개가 차면 그 이후부터는 가장 오래된 데이터를 삭제하고 새 데이터를 보관\n",
        "    \n",
        "    def put(self, transition):  # transition 데이터 넣기 메소드\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, n):     # transition 데이터 추출 메소드 (train할떄 batch만들듯이 random으로 데이터 추출해감)\n",
        "        mini_batch = random.sample(self.buffer, n)  # n개만큼 데이터 추출\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], [] # (s,a,r,s')과 종료여부\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done_mask = transition  # buffer에서 추출된 mini_batch에서 transition 데이터 추출\n",
        "            s_lst.append(s)\n",
        "            a_lst.append([a])\n",
        "            r_lst.append([r])\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask_lst.append([done_mask])\n",
        "\n",
        "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
        "               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
        "               torch.tensor(done_mask_lst)\n",
        "\n",
        "    def size(self):  # buffer의 사이즈 확인하는 용도의 메소드\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "class Qnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Qnet, self).__init__() # 초기화\n",
        "        self.embedding = torch.nn.Embedding(500, 500)\n",
        "        self.fc1 = nn.Linear(500, 128)  # 가중치 3개(히든 레이어 2개 적용) 입력값(state)은 4개 (cart position, cart 속도, pole 각도, pole 속도)\n",
        "        self.fc2 = nn.Linear(128, 64) # 가중치 3개(히든 레이어 2개 적용)\n",
        "        self.fc3 = nn.Linear(64, 6) # 가중치 3개(히든 레이어 2개 적용), output(Q(s,a))은 2개\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x.long())\n",
        "        x= x.squeeze(0)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "      \n",
        "    def sample_action(self, obs, epsilon):  # epsilon-greedy로 action 선택하는 함수,  obs : state input 값\n",
        "        out = self.forward(obs)  # forward를 진행 (신경망 통과), out은 tensor 형태\n",
        "        # print(out, out.argmax())\n",
        "        coin = random.random()\n",
        "        if coin < epsilon:\n",
        "            return random.randint(0,5)\n",
        "        else : \n",
        "            return out.argmax().item()   \n",
        "\n",
        "def train(q, q_target, memory, optimizer):\n",
        "    for i in range(10):\n",
        "        s,a,r,s_prime,done_mask = memory.sample(batch_size)  # memory에서 데이터 sampling(추출하기), 배치 사이즈만큼\n",
        "\n",
        "        q_out = q(s)  # q_out.shape = (batch_size, 2)  // 2는 액션의 개수\n",
        "        q_a = q_out.gather(1,a)  # action_list인 a를 활용하여, Q테이블에서 각 state에 맞는 action 값을 취함.\n",
        "                                  # => 그에 대한 value 값을 q_a에 텐서로 저장\n",
        "\n",
        "        # DQN\n",
        "        # max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)  # 타겟넷에 대한 output 값에 대해 dim=1의 max값을 취함\n",
        "\n",
        "        # Double DQN\n",
        "        argmax_Q = q(s_prime).max(1)[1].unsqueeze(1)\n",
        "        max_q_prime = q_target(s_prime).gather(1, argmax_Q)\n",
        "\n",
        "        target = r + gamma * max_q_prime * done_mask # terminate stae면 done_mask가 0이므로 뒤의 항 없어짐\n",
        "        \n",
        "        # MSE Loss\n",
        "        loss = F.mse_loss(q_a, target)\n",
        "\n",
        "        # Smooth L1 Loss\n",
        "        #loss = F.smooth_l1_loss(q_a, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def epsilon_annealing(episode, max_episode, min_eps, max_eps):\n",
        "    if max_episode == 0:\n",
        "        return min_eps\n",
        "    slope = (min_eps - max_eps) / max_episode\n",
        "    return max(slope * episode + max_eps, min_eps)\n",
        "\n",
        "def main():\n",
        "    env = gym.make('Taxi-v3').unwrapped\n",
        "    q = Qnet()            # 메인 넷\n",
        "    q_target = Qnet()     # 타겟 넷  , 별개로 학습\n",
        "    q_target.load_state_dict(q.state_dict())\n",
        "    memory = ReplayBuffer()  ## transition을 담고 있는 테이블에 대한 객체 선언\n",
        "\n",
        "    print_interval = 20   #\n",
        "    score = 0.0           #\n",
        "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
        "    # boost = 1 \n",
        "\n",
        "    return_list = []\n",
        "    for n_epi in range(5000):\n",
        "        # epsilon = max(0.01, 0.5 - 0.02*(n_epi/200)*boost) #Linear annealing from 8% to 1%, epsilon 값을 0.01이 되기 전까지 점점 줄여가도록 함.\n",
        "        epsilon = epsilon_annealing(n_epi, MAX_EPISODE, EPS_END, EPS_START)\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "\n",
        "        if n_epi > 500:\n",
        "          boost = 5\n",
        "        \n",
        "        for i in range(15000):  # 200step으로 제한\n",
        "        # while not done:\n",
        "            a = q.sample_action(torch.LongTensor([s]), epsilon)  # action을 policy(eplsilon-grredy)에 따라 선택\n",
        "\n",
        "            # a = random.randint(0,5)\n",
        "            s_prime, r, done, info, _ = env.step(a)  # 단계 진행\n",
        "            print(s_prime, a, done)\n",
        "            # print(s, a, s_prime, r, done)\n",
        "            done_mask = 0.0 if done else 1.0\n",
        "            memory.put((s,a,r,s_prime, done_mask))  # r값이 너무 크면, train이 잘 안되서 줄엿음 (왜? 튜닝/테크닉의 영역이라는 교수님의 말씀)\n",
        "                                                          # replace_buffer 객체에 transition 데이터 저장\n",
        "            s = s_prime\n",
        "\n",
        "            score += r\n",
        "            if done:\n",
        "                return_list.append(score)\n",
        "                break\n",
        "\n",
        "        if memory.size()>20000:  # 메모리 사이즈가 2000까지 차기 전에는 train(x), 데이터가 적을때는 훈련이 무의미하므로\n",
        "            print('train', n_epi)\n",
        "            train(q, q_target, memory, optimizer)\n",
        "\n",
        "        print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(n_epi, score, memory.size(), epsilon*100))\n",
        "        score = 0.0\n",
        "\n",
        "        # if n_epi%print_interval==0 and n_epi!=0:  # 20번쨰 episode마다 메인넷(q)의 가중치값을 타겟넷(q_target)으로 카피함.\n",
        "        #     q_target.load_state_dict(q.state_dict())\n",
        "        #     print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n",
        "        #                                                     n_epi, score/print_interval, memory.size(), epsilon*100))\n",
        "        #     score = 0.0\n",
        "\n",
        "        \n",
        "        # pd.DataFrame(return_list).to_csv('/content/drive/MyDrive/강화학습/Double_DQN_model_record2.csv')\n",
        "        # torch.save(q.state_dict(), '/content/drive/MyDrive/강화학습/Double_DQN_model_last2.pth')\n",
        "\n",
        "    env.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "uEKa3Ib06332",
        "outputId": "a92472c7-86ef-4a28-f1a8-48039b8fb295"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8d2d4cacb9ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# pytorch library is used for deep learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tester\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/testing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from pandas._testing import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0massert_extension_array_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0massert_frame_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_testing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m \u001b[0mcython_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cython_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DDPG (Swimmer)"
      ],
      "metadata": {
        "id": "xXYWm78_UYPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "seed_num = 1000\n",
        "torch.manual_seed(seed_num)\n",
        "torch.cuda.manual_seed(seed_num)\n",
        "torch.cuda.manual_seed_all(seed_num)\n",
        "np.random.seed(seed_num)\n",
        "random.seed(seed_num)\n",
        "\n",
        "\n",
        "TrainingRecord = namedtuple('TrainingRecord', ['ep', 'reward'])\n",
        "Transition = namedtuple('Transition', ['s', 'a', 'r', 's_'])\n",
        "\n",
        "gamma = 0.98\n",
        "log_interval = 10\n",
        "\n",
        "class ActorNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ActorNet, self).__init__()\n",
        "        self.fc = nn.Linear(8, 100)\n",
        "        self.mu_head = nn.Linear(100, 2)\n",
        "\n",
        "    def forward(self, s):\n",
        "        x = F.relu(self.fc(s))\n",
        "        u = 2.0 * F.tanh(self.mu_head(x)) # pendulum task의 경우, action의 범위가 -2~2이므로, tanh를 활용하여 -1~1로 출력시키고 여기에 x2를 해준다.\n",
        "        return u\n",
        "\n",
        "\n",
        "class CriticNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CriticNet, self).__init__()\n",
        "        self.fc = nn.Linear(10, 100)\n",
        "        self.v_head = nn.Linear(100, 2)\n",
        "\n",
        "    def forward(self, s, a):\n",
        "        # print(s.shape, a.shape)\n",
        "        # print(s, a, torch.cat([s, a], dim=1), sep = '\\n\\n')\n",
        "        # s: tensor([[0.8553, -0.5181, 4.4607],[0.6519, -0.7583, 6.2618],[-0.8653, -0.5013, -6.8173], [0.1435, -0.9897, 7.0537]...  [0.1435, -1.9897, 5.053]]\n",
        "        # a: tensor([[ 2.7781], [ 1.7413],[ 0.0121],[ 0.5088],[ 0.1754],... [0.1211]\n",
        "        # tensor([[ 0.8553, -0.5181,  4.4607,  2.7781], [ 0.6519, -0.7583,  6.2618,  1.7413], [-0.8653, -0.5013, -6.8173,  0.0121], [ 0.1435, -0.9897,  7.0537,  0.5088],... [-0.8653, -0.5013, -6.8171,  0.1231]]\n",
        "\n",
        "        x = F.relu(self.fc(torch.cat([s, a], dim=1)))\n",
        "        state_value = self.v_head(x)  # 출력(state_value)은 선택된 1개의 action  (continuous value)\n",
        "        return state_value            # 출력(state_value)은 선택된 1개의 action\n",
        "\n",
        "\n",
        "class Memory():\n",
        "\n",
        "    data_pointer = 0\n",
        "    isfull = False\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = np.empty(capacity, dtype=object)\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def update(self, transition):\n",
        "        self.memory[self.data_pointer] = transition\n",
        "        self.data_pointer += 1\n",
        "        if self.data_pointer == self.capacity:\n",
        "            self.data_pointer = 0\n",
        "            self.isfull = True\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return np.random.choice(self.memory, batch_size)\n",
        "\n",
        "\n",
        "class Agent():\n",
        "\n",
        "    max_grad_norm = 0.5\n",
        "\n",
        "    def __init__(self):\n",
        "        self.training_step = 0\n",
        "        self.var = 1.\n",
        "        self.eval_cnet, self.target_cnet = CriticNet().float(), CriticNet().float()  # critic_main, critic_target\n",
        "        self.eval_anet, self.target_anet = ActorNet().float(), ActorNet().float()    # actor_main, actor_target\n",
        "        self.memory = Memory(2000)  # capacity = 2000\n",
        "        self.optimizer_c = optim.Adam(self.eval_cnet.parameters(), lr=1e-3)\n",
        "        self.optimizer_a = optim.Adam(self.eval_anet.parameters(), lr=3e-4)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        # print('state:', state, type(state))  # 출력 : state: [-0.07907514 -0.99686867  7.2913437 ], <class 'numpy.ndarray'>\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        mu = self.eval_anet(state) # actor_main의 출력 : action 값\n",
        "        dist = Normal(mu, torch.tensor(self.var, dtype=torch.float))  # (for explore)  action = action + Noise 을 구현하고자, 평균이 mu이고 분산이 1인 가우시안 분포를 만들고\n",
        "        action = dist.sample()                                        # 그 분포에서 1개의 값을 샘플링 함   => Noise를 더한것과 같은 효과\n",
        "        action.clamp(-1.0, 1.0)                                       # 값이 -2~2를 넘어가지 않도록 제한 (깍기)\n",
        "        # print(action)\n",
        "        return action.squeeze(0).numpy()\n",
        "\n",
        "    def save_param(self):\n",
        "        torch.save(self.eval_anet.state_dict(), '/content/drive/MyDrive/강화학습/DDPG_ACTOR_model3.pth')\n",
        "        torch.save(self.eval_cnet.state_dict(), '/content/drive/MyDrive/강화학습/DDPG_CRITIC_model3.pth')\n",
        "\n",
        "    def store_transition(self, transition):\n",
        "        self.memory.update(transition)\n",
        "\n",
        "    def update(self):           # gradient update\n",
        "        self.training_step += 1\n",
        "\n",
        "        transitions = self.memory.sample(32)\n",
        "        s = torch.tensor([t.s for t in transitions], dtype=torch.float)\n",
        "        a = torch.tensor([t.a for t in transitions], dtype=torch.float)\n",
        "        r = torch.tensor([t.r for t in transitions], dtype=torch.float).view(-1, 1)\n",
        "        s_ = torch.tensor([t.s_ for t in transitions], dtype=torch.float)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_target = r + gamma * self.target_cnet(s_, self.target_anet(s_))  # critic target 값 구하기 : actor_target으로 action을 얻어 (self.target_anet(s_)), critic_target(self.target_cnet)에 적용\n",
        "        q_eval = self.eval_cnet(s, a)  # critic output 값 구하기 : critic_main 출력 값 (Q값)\n",
        "\n",
        "        # update critic net\n",
        "        self.optimizer_c.zero_grad()\n",
        "        c_loss = F.smooth_l1_loss(q_eval, q_target)  # Critic의 Loss Function : Critic_Main과 Critic_target 간의 차이 (smooth_l1_loss)\n",
        "        c_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.eval_cnet.parameters(), self.max_grad_norm)  # gradient clipping (vanishing gradient를 방지하기 위한 즉, 학습을 잘되게 하기 위한 skill - 강화학습에만 국한되는게 아니라 여러 머신러닝에서 사용하는 스킬)\n",
        "        self.optimizer_c.step()\n",
        "\n",
        "        # update actor net\n",
        "        self.optimizer_a.zero_grad()\n",
        "        a_loss = -self.eval_cnet(s, self.eval_anet(s)).mean() # gradient ascent이므로 (-) 적용,   Actor의 Loss function : mean of Critic's Q_Value\n",
        "        a_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.eval_anet.parameters(), self.max_grad_norm)\n",
        "        self.optimizer_a.step()\n",
        "\n",
        "        if self.training_step % 200 == 0:\n",
        "            self.target_cnet.load_state_dict(self.eval_cnet.state_dict())\n",
        "        if self.training_step % 201 == 0:\n",
        "            self.target_anet.load_state_dict(self.eval_anet.state_dict())\n",
        "\n",
        "        self.var = max(self.var * 0.999, 0.01)\n",
        "\n",
        "        return q_eval.mean().item()\n",
        "\n",
        "\n",
        "def main():\n",
        "    env = gym.make('Swimmer')\n",
        "    agent = Agent()\n",
        "\n",
        "    training_records = []\n",
        "    return_list = []\n",
        "    running_reward, running_q = 0, 0\n",
        "    for i_ep in range(4000):\n",
        "        score = 0\n",
        "        state = env.reset()\n",
        "\n",
        "        for t in range(200):\n",
        "            action = agent.select_action(state)\n",
        "            state_, reward, done, _ = env.step(action)\n",
        "            score += reward\n",
        "            agent.store_transition(Transition(state, action, reward , state_))  # (reward + 8) / 8 는 일종의 학습이 잘되기 위한 trick\n",
        "            state = state_\n",
        "            if agent.memory.isfull:\n",
        "                q = agent.update()\n",
        "                running_q = 0.99 * running_q + 0.01 * q\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        running_reward = running_reward * 0.9 + score * 0.1\n",
        "\n",
        "        training_records.append(TrainingRecord(i_ep, running_reward))\n",
        "        return_list.append(score)\n",
        "        pd.DataFrame(return_list).to_csv('/content/drive/MyDrive/강화학습/DDPG_model_record3.csv')\n",
        "\n",
        "        if i_ep % log_interval == 0:\n",
        "            print('Step {}\\t score: {:.2f}\\tAverage Q: {:.2f}'.format(\n",
        "                i_ep, score, running_q))\n",
        "            agent.save_param()\n",
        "        #if running_reward > -200:\n",
        "        #    print(\"Solved! Running reward is now {}!\".format(running_reward))\n",
        "        #    env.close()\n",
        "        #    agent.save_param()\n",
        "        #    with open('ddpg_training_records.pkl', 'wb') as f:\n",
        "        #        pickle.dump(training_records, f)\n",
        "        #    break\n",
        "    agent.save_param()\n",
        "    env.close()\n",
        "\n",
        "    plt.plot([r.ep for r in training_records], [r.reward for r in training_records])\n",
        "    plt.title('DDPG')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Moving averaged episode reward')\n",
        "    plt.savefig(\"ddpg3.png\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9fb11657-38d8-4cae-ecc4-fec43a8a0a19",
        "id": "OM_0YfJ31vas"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0\t score: 4.45\tAverage Q: 0.00\n",
            "Step 10\t score: 24.94\tAverage Q: 0.18\n",
            "Step 20\t score: 25.94\tAverage Q: 2.15\n",
            "Step 30\t score: 24.46\tAverage Q: 4.33\n",
            "Step 40\t score: 14.11\tAverage Q: 8.46\n",
            "Step 50\t score: 31.87\tAverage Q: 6.41\n",
            "Step 60\t score: 23.70\tAverage Q: 6.24\n",
            "Step 70\t score: 23.96\tAverage Q: 5.24\n",
            "Step 80\t score: 31.11\tAverage Q: 4.89\n",
            "Step 90\t score: 34.91\tAverage Q: 4.82\n",
            "Step 100\t score: 37.54\tAverage Q: 5.08\n",
            "Step 110\t score: 32.10\tAverage Q: 4.94\n",
            "Step 120\t score: 39.51\tAverage Q: 4.56\n",
            "Step 130\t score: 31.78\tAverage Q: 7.04\n",
            "Step 140\t score: 31.68\tAverage Q: 6.91\n",
            "Step 150\t score: 36.00\tAverage Q: 7.26\n",
            "Step 160\t score: 37.86\tAverage Q: 6.93\n",
            "Step 170\t score: 33.64\tAverage Q: 7.44\n",
            "Step 180\t score: 33.96\tAverage Q: 6.98\n",
            "Step 190\t score: 35.49\tAverage Q: 6.85\n",
            "Step 200\t score: 37.56\tAverage Q: 7.08\n",
            "Step 210\t score: 30.82\tAverage Q: 6.66\n",
            "Step 220\t score: 38.84\tAverage Q: 6.54\n",
            "Step 230\t score: 30.87\tAverage Q: 6.85\n",
            "Step 240\t score: 36.49\tAverage Q: 6.36\n",
            "Step 250\t score: 34.93\tAverage Q: 6.26\n",
            "Step 260\t score: 34.23\tAverage Q: 6.13\n",
            "Step 270\t score: 36.95\tAverage Q: 5.89\n",
            "Step 280\t score: 31.28\tAverage Q: 5.37\n",
            "Step 290\t score: 35.51\tAverage Q: 5.65\n",
            "Step 300\t score: 35.31\tAverage Q: 5.07\n",
            "Step 310\t score: 35.30\tAverage Q: 4.98\n",
            "Step 320\t score: 33.46\tAverage Q: 5.19\n",
            "Step 330\t score: 37.37\tAverage Q: 5.25\n",
            "Step 340\t score: 37.40\tAverage Q: 4.91\n",
            "Step 350\t score: 36.74\tAverage Q: 5.10\n",
            "Step 360\t score: 39.75\tAverage Q: 5.49\n",
            "Step 370\t score: 34.86\tAverage Q: 5.34\n",
            "Step 380\t score: 38.81\tAverage Q: 4.96\n",
            "Step 390\t score: 36.04\tAverage Q: 5.31\n",
            "Step 400\t score: 28.81\tAverage Q: 9.02\n",
            "Step 410\t score: 18.76\tAverage Q: 10.18\n",
            "Step 420\t score: 37.65\tAverage Q: 7.48\n",
            "Step 430\t score: 6.26\tAverage Q: 17.69\n",
            "Step 440\t score: 12.89\tAverage Q: 19.26\n",
            "Step 450\t score: 14.18\tAverage Q: 15.40\n",
            "Step 460\t score: 13.18\tAverage Q: 12.38\n",
            "Step 470\t score: 26.31\tAverage Q: 7.34\n",
            "Step 480\t score: 30.92\tAverage Q: 7.35\n",
            "Step 490\t score: 12.82\tAverage Q: 17.13\n",
            "Step 500\t score: 39.62\tAverage Q: 28.89\n",
            "Step 510\t score: 31.94\tAverage Q: 26.10\n",
            "Step 520\t score: 30.89\tAverage Q: 22.33\n",
            "Step 530\t score: 34.15\tAverage Q: 18.93\n",
            "Step 540\t score: 34.23\tAverage Q: 16.26\n",
            "Step 550\t score: 33.38\tAverage Q: 20.37\n",
            "Step 560\t score: 37.27\tAverage Q: 19.96\n",
            "Step 570\t score: 32.70\tAverage Q: 16.61\n",
            "Step 580\t score: 29.15\tAverage Q: 14.14\n",
            "Step 590\t score: 35.62\tAverage Q: 12.52\n",
            "Step 600\t score: 38.30\tAverage Q: 11.50\n",
            "Step 610\t score: 33.22\tAverage Q: 10.51\n",
            "Step 620\t score: 34.25\tAverage Q: 10.12\n",
            "Step 630\t score: 35.14\tAverage Q: 8.84\n",
            "Step 640\t score: 38.51\tAverage Q: 7.90\n",
            "Step 650\t score: 34.42\tAverage Q: 7.44\n",
            "Step 660\t score: 38.49\tAverage Q: 7.19\n",
            "Step 670\t score: 21.23\tAverage Q: 10.54\n",
            "Step 680\t score: 29.75\tAverage Q: 13.92\n",
            "Step 690\t score: 26.84\tAverage Q: 11.55\n",
            "Step 700\t score: 27.38\tAverage Q: 10.41\n",
            "Step 710\t score: 26.87\tAverage Q: 9.76\n",
            "Step 720\t score: 30.43\tAverage Q: 8.29\n",
            "Step 730\t score: 32.06\tAverage Q: 6.93\n",
            "Step 740\t score: 34.66\tAverage Q: 6.74\n",
            "Step 750\t score: 34.09\tAverage Q: 3.97\n",
            "Step 760\t score: 31.93\tAverage Q: 4.46\n",
            "Step 770\t score: 35.78\tAverage Q: 8.01\n",
            "Step 780\t score: 32.54\tAverage Q: 10.70\n",
            "Step 790\t score: 29.63\tAverage Q: 11.63\n",
            "Step 800\t score: 19.16\tAverage Q: 10.29\n",
            "Step 810\t score: 18.97\tAverage Q: 9.98\n",
            "Step 820\t score: 36.74\tAverage Q: 9.18\n",
            "Step 830\t score: 34.22\tAverage Q: 8.67\n",
            "Step 840\t score: 35.85\tAverage Q: 8.00\n",
            "Step 850\t score: 31.58\tAverage Q: 7.50\n",
            "Step 860\t score: 33.87\tAverage Q: 7.21\n",
            "Step 870\t score: 35.04\tAverage Q: 6.41\n",
            "Step 880\t score: 36.21\tAverage Q: 6.16\n",
            "Step 890\t score: 30.42\tAverage Q: 5.69\n",
            "Step 900\t score: 33.79\tAverage Q: 5.33\n",
            "Step 910\t score: 40.06\tAverage Q: 5.66\n",
            "Step 920\t score: 35.22\tAverage Q: 5.54\n",
            "Step 930\t score: 39.11\tAverage Q: 5.29\n",
            "Step 940\t score: 32.51\tAverage Q: 5.46\n",
            "Step 950\t score: 36.81\tAverage Q: 5.55\n",
            "Step 960\t score: 34.60\tAverage Q: 7.71\n",
            "Step 970\t score: 38.41\tAverage Q: 6.91\n",
            "Step 980\t score: 41.28\tAverage Q: 21.59\n",
            "Step 990\t score: 27.85\tAverage Q: 7.04\n",
            "Step 1000\t score: 35.84\tAverage Q: 7.46\n",
            "Step 1010\t score: 27.88\tAverage Q: 10.84\n",
            "Step 1020\t score: 32.34\tAverage Q: 17.27\n",
            "Step 1030\t score: 34.43\tAverage Q: 14.91\n",
            "Step 1040\t score: 31.90\tAverage Q: 12.96\n",
            "Step 1050\t score: 35.55\tAverage Q: 11.63\n",
            "Step 1060\t score: 31.79\tAverage Q: 9.96\n",
            "Step 1070\t score: 32.24\tAverage Q: 9.56\n",
            "Step 1080\t score: 36.27\tAverage Q: 8.32\n",
            "Step 1090\t score: 37.80\tAverage Q: 7.30\n",
            "Step 1100\t score: 36.11\tAverage Q: 7.27\n",
            "Step 1110\t score: 38.16\tAverage Q: 9.07\n",
            "Step 1120\t score: 28.14\tAverage Q: 9.41\n",
            "Step 1130\t score: 36.94\tAverage Q: 8.78\n",
            "Step 1140\t score: 37.55\tAverage Q: 6.55\n",
            "Step 1150\t score: 38.79\tAverage Q: 6.80\n",
            "Step 1160\t score: 35.80\tAverage Q: 7.09\n",
            "Step 1170\t score: 39.10\tAverage Q: 6.27\n",
            "Step 1180\t score: 30.61\tAverage Q: 6.20\n",
            "Step 1190\t score: 34.59\tAverage Q: 6.30\n",
            "Step 1200\t score: 33.10\tAverage Q: 5.87\n",
            "Step 1210\t score: 30.81\tAverage Q: 5.39\n",
            "Step 1220\t score: 30.04\tAverage Q: 5.44\n",
            "Step 1230\t score: 37.07\tAverage Q: 5.70\n",
            "Step 1240\t score: 34.71\tAverage Q: 5.74\n",
            "Step 1250\t score: 35.00\tAverage Q: 5.61\n",
            "Step 1260\t score: 35.87\tAverage Q: 6.60\n",
            "Step 1270\t score: 35.81\tAverage Q: 6.29\n",
            "Step 1280\t score: 35.89\tAverage Q: 6.44\n",
            "Step 1290\t score: 36.31\tAverage Q: 6.33\n",
            "Step 1300\t score: 32.81\tAverage Q: 6.47\n",
            "Step 1310\t score: 37.95\tAverage Q: 6.32\n",
            "Step 1320\t score: 33.92\tAverage Q: 6.50\n",
            "Step 1330\t score: 31.55\tAverage Q: 6.83\n",
            "Step 1340\t score: 36.75\tAverage Q: 6.05\n",
            "Step 1350\t score: 34.98\tAverage Q: 6.05\n",
            "Step 1360\t score: 35.44\tAverage Q: 5.93\n",
            "Step 1370\t score: 30.66\tAverage Q: 5.72\n",
            "Step 1380\t score: 37.46\tAverage Q: 5.38\n",
            "Step 1390\t score: 34.39\tAverage Q: 5.34\n",
            "Step 1400\t score: 38.69\tAverage Q: 5.42\n",
            "Step 1410\t score: 39.79\tAverage Q: 5.52\n",
            "Step 1420\t score: 39.15\tAverage Q: 5.09\n",
            "Step 1430\t score: 35.96\tAverage Q: 5.21\n",
            "Step 1440\t score: 36.06\tAverage Q: 4.93\n",
            "Step 1450\t score: 38.08\tAverage Q: 4.99\n",
            "Step 1460\t score: 39.16\tAverage Q: 4.75\n",
            "Step 1470\t score: 35.52\tAverage Q: 4.89\n",
            "Step 1480\t score: 40.85\tAverage Q: 5.06\n",
            "Step 1490\t score: 39.08\tAverage Q: 4.69\n",
            "Step 1500\t score: 39.61\tAverage Q: 4.90\n",
            "Step 1510\t score: 34.17\tAverage Q: 4.83\n",
            "Step 1520\t score: 36.83\tAverage Q: 4.70\n",
            "Step 1530\t score: 41.22\tAverage Q: 5.37\n",
            "Step 1540\t score: 38.04\tAverage Q: 5.55\n",
            "Step 1550\t score: 41.67\tAverage Q: 5.63\n",
            "Step 1560\t score: 36.91\tAverage Q: 5.68\n",
            "Step 1570\t score: 39.26\tAverage Q: 5.56\n",
            "Step 1580\t score: 42.04\tAverage Q: 6.03\n",
            "Step 1590\t score: 34.16\tAverage Q: 5.72\n",
            "Step 1600\t score: 34.12\tAverage Q: 5.48\n",
            "Step 1610\t score: 38.29\tAverage Q: 5.73\n",
            "Step 1620\t score: 35.81\tAverage Q: 5.67\n",
            "Step 1630\t score: 33.00\tAverage Q: 5.55\n",
            "Step 1640\t score: 37.56\tAverage Q: 5.73\n",
            "Step 1650\t score: 38.59\tAverage Q: 5.54\n",
            "Step 1660\t score: 37.99\tAverage Q: 5.44\n",
            "Step 1670\t score: 31.76\tAverage Q: 5.34\n",
            "Step 1680\t score: 36.24\tAverage Q: 5.22\n",
            "Step 1690\t score: 36.93\tAverage Q: 4.97\n",
            "Step 1700\t score: 35.00\tAverage Q: 4.79\n",
            "Step 1710\t score: 38.48\tAverage Q: 5.04\n",
            "Step 1720\t score: 35.31\tAverage Q: 5.03\n",
            "Step 1730\t score: 35.36\tAverage Q: 4.79\n",
            "Step 1740\t score: 38.76\tAverage Q: 4.79\n",
            "Step 1750\t score: 35.28\tAverage Q: 4.91\n",
            "Step 1760\t score: 30.54\tAverage Q: 4.84\n",
            "Step 1770\t score: 37.21\tAverage Q: 4.72\n",
            "Step 1780\t score: 39.94\tAverage Q: 4.82\n",
            "Step 1790\t score: 34.84\tAverage Q: 4.66\n",
            "Step 1800\t score: 38.50\tAverage Q: 4.35\n",
            "Step 1810\t score: 31.22\tAverage Q: 3.96\n",
            "Step 1820\t score: 35.99\tAverage Q: 4.98\n",
            "Step 1830\t score: 35.75\tAverage Q: 4.91\n",
            "Step 1840\t score: 33.44\tAverage Q: 4.84\n",
            "Step 1850\t score: 35.23\tAverage Q: 4.82\n",
            "Step 1860\t score: 40.08\tAverage Q: 5.15\n",
            "Step 1870\t score: 36.97\tAverage Q: 5.06\n",
            "Step 1880\t score: 33.77\tAverage Q: 5.24\n",
            "Step 1890\t score: 39.56\tAverage Q: 5.32\n",
            "Step 1900\t score: 40.91\tAverage Q: 5.32\n",
            "Step 1910\t score: 38.07\tAverage Q: 5.23\n",
            "Step 1920\t score: 33.05\tAverage Q: 5.03\n",
            "Step 1930\t score: 40.69\tAverage Q: 5.77\n",
            "Step 1940\t score: 35.94\tAverage Q: 6.24\n",
            "Step 1950\t score: 36.44\tAverage Q: 5.88\n",
            "Step 1960\t score: 36.31\tAverage Q: 6.52\n",
            "Step 1970\t score: 36.29\tAverage Q: 6.56\n",
            "Step 1980\t score: 38.02\tAverage Q: 5.89\n",
            "Step 1990\t score: 34.93\tAverage Q: 6.35\n",
            "Step 2000\t score: 37.50\tAverage Q: 6.33\n",
            "Step 2010\t score: 39.08\tAverage Q: 6.46\n",
            "Step 2020\t score: 39.06\tAverage Q: 6.46\n",
            "Step 2030\t score: 34.85\tAverage Q: 6.38\n",
            "Step 2040\t score: 39.19\tAverage Q: 6.45\n",
            "Step 2050\t score: 40.69\tAverage Q: 5.97\n",
            "Step 2060\t score: 38.53\tAverage Q: 5.88\n",
            "Step 2070\t score: 32.81\tAverage Q: 5.98\n",
            "Step 2080\t score: 39.61\tAverage Q: 5.53\n",
            "Step 2090\t score: 39.54\tAverage Q: 5.59\n",
            "Step 2100\t score: 37.58\tAverage Q: 5.39\n",
            "Step 2110\t score: 40.99\tAverage Q: 5.18\n",
            "Step 2120\t score: 38.84\tAverage Q: 5.28\n",
            "Step 2130\t score: 36.44\tAverage Q: 5.71\n",
            "Step 2140\t score: 38.71\tAverage Q: 5.51\n",
            "Step 2150\t score: 39.20\tAverage Q: 5.52\n",
            "Step 2160\t score: 29.43\tAverage Q: 5.41\n",
            "Step 2170\t score: 37.95\tAverage Q: 5.00\n",
            "Step 2180\t score: 36.97\tAverage Q: 5.27\n",
            "Step 2190\t score: 35.00\tAverage Q: 5.54\n",
            "Step 2200\t score: 38.58\tAverage Q: 5.39\n",
            "Step 2210\t score: 36.73\tAverage Q: 5.27\n",
            "Step 2220\t score: 37.16\tAverage Q: 5.37\n",
            "Step 2230\t score: 40.34\tAverage Q: 5.61\n",
            "Step 2240\t score: 38.13\tAverage Q: 5.57\n",
            "Step 2250\t score: 32.67\tAverage Q: 6.10\n",
            "Step 2260\t score: 39.01\tAverage Q: 5.52\n",
            "Step 2270\t score: 35.90\tAverage Q: 5.52\n",
            "Step 2280\t score: 40.62\tAverage Q: 5.53\n",
            "Step 2290\t score: 36.69\tAverage Q: 5.57\n",
            "Step 2300\t score: 38.99\tAverage Q: 5.27\n",
            "Step 2310\t score: 39.29\tAverage Q: 5.36\n",
            "Step 2320\t score: 40.20\tAverage Q: 5.47\n",
            "Step 2330\t score: 41.78\tAverage Q: 5.29\n",
            "Step 2340\t score: 41.56\tAverage Q: 5.20\n",
            "Step 2350\t score: 40.27\tAverage Q: 5.25\n",
            "Step 2360\t score: 39.28\tAverage Q: 5.16\n",
            "Step 2370\t score: 34.58\tAverage Q: 5.37\n",
            "Step 2380\t score: 33.55\tAverage Q: 5.26\n",
            "Step 2390\t score: 38.78\tAverage Q: 5.45\n",
            "Step 2400\t score: 38.10\tAverage Q: 5.55\n",
            "Step 2410\t score: 36.36\tAverage Q: 5.25\n",
            "Step 2420\t score: 36.94\tAverage Q: 5.29\n",
            "Step 2430\t score: 37.94\tAverage Q: 5.56\n",
            "Step 2440\t score: 37.39\tAverage Q: 5.50\n",
            "Step 2450\t score: 41.46\tAverage Q: 5.83\n",
            "Step 2460\t score: 33.93\tAverage Q: 5.83\n",
            "Step 2470\t score: 35.90\tAverage Q: 5.99\n",
            "Step 2480\t score: 33.75\tAverage Q: 7.21\n",
            "Step 2490\t score: 36.52\tAverage Q: 5.42\n",
            "Step 2500\t score: 31.05\tAverage Q: 5.48\n",
            "Step 2510\t score: 30.97\tAverage Q: 5.46\n",
            "Step 2520\t score: 28.89\tAverage Q: 5.22\n",
            "Step 2530\t score: 33.26\tAverage Q: 4.83\n",
            "Step 2540\t score: 37.82\tAverage Q: 6.39\n",
            "Step 2550\t score: 28.49\tAverage Q: 6.29\n",
            "Step 2560\t score: 32.94\tAverage Q: 5.95\n",
            "Step 2570\t score: 27.51\tAverage Q: 5.69\n",
            "Step 2580\t score: 34.88\tAverage Q: 5.21\n",
            "Step 2590\t score: 31.33\tAverage Q: 4.99\n",
            "Step 2600\t score: 32.86\tAverage Q: 5.20\n",
            "Step 2610\t score: 35.28\tAverage Q: 5.58\n",
            "Step 2620\t score: 39.95\tAverage Q: 5.76\n",
            "Step 2630\t score: 37.20\tAverage Q: 5.76\n",
            "Step 2640\t score: 39.69\tAverage Q: 5.67\n",
            "Step 2650\t score: 32.68\tAverage Q: 7.59\n",
            "Step 2660\t score: 30.16\tAverage Q: 6.83\n",
            "Step 2670\t score: 33.11\tAverage Q: 7.49\n",
            "Step 2680\t score: 34.05\tAverage Q: 6.02\n",
            "Step 2690\t score: 33.48\tAverage Q: 7.01\n",
            "Step 2700\t score: 32.46\tAverage Q: 6.60\n",
            "Step 2710\t score: 34.34\tAverage Q: 6.32\n",
            "Step 2720\t score: 36.92\tAverage Q: 6.22\n",
            "Step 2730\t score: 34.40\tAverage Q: 6.76\n",
            "Step 2740\t score: 38.22\tAverage Q: 6.40\n",
            "Step 2750\t score: 35.17\tAverage Q: 6.08\n",
            "Step 2760\t score: 37.80\tAverage Q: 6.08\n",
            "Step 2770\t score: 37.84\tAverage Q: 5.91\n",
            "Step 2780\t score: 41.75\tAverage Q: 5.85\n",
            "Step 2790\t score: 40.14\tAverage Q: 5.68\n",
            "Step 2800\t score: 36.22\tAverage Q: 5.48\n",
            "Step 2810\t score: 35.73\tAverage Q: 5.32\n",
            "Step 2820\t score: 28.53\tAverage Q: 5.28\n",
            "Step 2830\t score: 31.49\tAverage Q: 4.82\n",
            "Step 2840\t score: 32.20\tAverage Q: 4.56\n",
            "Step 2850\t score: 29.85\tAverage Q: 4.41\n",
            "Step 2860\t score: 31.85\tAverage Q: 4.33\n",
            "Step 2870\t score: 33.29\tAverage Q: 4.38\n",
            "Step 2880\t score: 35.58\tAverage Q: 4.25\n",
            "Step 2890\t score: 33.39\tAverage Q: 4.05\n",
            "Step 2900\t score: 33.85\tAverage Q: 4.20\n",
            "Step 2910\t score: 34.17\tAverage Q: 4.29\n",
            "Step 2920\t score: 33.45\tAverage Q: 4.57\n",
            "Step 2930\t score: 37.68\tAverage Q: 4.74\n",
            "Step 2940\t score: 39.06\tAverage Q: 4.72\n",
            "Step 2950\t score: 37.59\tAverage Q: 4.33\n",
            "Step 2960\t score: 40.78\tAverage Q: 4.54\n",
            "Step 2970\t score: 29.70\tAverage Q: 4.53\n",
            "Step 2980\t score: 33.25\tAverage Q: 4.07\n",
            "Step 2990\t score: 38.59\tAverage Q: 4.70\n",
            "Step 3000\t score: 37.70\tAverage Q: 4.62\n",
            "Step 3010\t score: 37.62\tAverage Q: 4.32\n",
            "Step 3020\t score: 38.27\tAverage Q: 4.86\n",
            "Step 3030\t score: 37.90\tAverage Q: 5.02\n",
            "Step 3040\t score: 39.77\tAverage Q: 5.06\n",
            "Step 3050\t score: 37.87\tAverage Q: 5.07\n",
            "Step 3060\t score: 40.47\tAverage Q: 5.28\n",
            "Step 3070\t score: 40.87\tAverage Q: 5.28\n",
            "Step 3080\t score: 38.93\tAverage Q: 5.03\n",
            "Step 3090\t score: 36.44\tAverage Q: 5.27\n",
            "Step 3100\t score: 36.75\tAverage Q: 4.92\n",
            "Step 3110\t score: 35.34\tAverage Q: 4.68\n",
            "Step 3120\t score: 35.70\tAverage Q: 4.63\n",
            "Step 3130\t score: 37.46\tAverage Q: 4.41\n",
            "Step 3140\t score: 37.18\tAverage Q: 4.21\n",
            "Step 3150\t score: 37.52\tAverage Q: 4.29\n",
            "Step 3160\t score: 36.64\tAverage Q: 4.56\n",
            "Step 3170\t score: 40.48\tAverage Q: 4.99\n",
            "Step 3180\t score: 40.16\tAverage Q: 4.80\n",
            "Step 3190\t score: 36.30\tAverage Q: 4.45\n",
            "Step 3200\t score: 40.58\tAverage Q: 4.61\n",
            "Step 3210\t score: 40.72\tAverage Q: 4.78\n",
            "Step 3220\t score: 39.22\tAverage Q: 4.48\n",
            "Step 3230\t score: 41.02\tAverage Q: 4.40\n",
            "Step 3240\t score: 30.09\tAverage Q: 4.63\n",
            "Step 3250\t score: 34.84\tAverage Q: 4.52\n",
            "Step 3260\t score: 34.81\tAverage Q: 4.24\n",
            "Step 3270\t score: 37.77\tAverage Q: 4.40\n",
            "Step 3280\t score: 39.30\tAverage Q: 5.54\n",
            "Step 3290\t score: 38.49\tAverage Q: 5.32\n",
            "Step 3300\t score: 32.30\tAverage Q: 4.76\n",
            "Step 3310\t score: 40.53\tAverage Q: 4.44\n",
            "Step 3320\t score: 31.59\tAverage Q: 5.44\n",
            "Step 3330\t score: 36.95\tAverage Q: 4.86\n",
            "Step 3340\t score: 39.88\tAverage Q: 5.09\n",
            "Step 3350\t score: 40.19\tAverage Q: 5.67\n",
            "Step 3360\t score: 31.48\tAverage Q: 5.29\n",
            "Step 3370\t score: 39.91\tAverage Q: 5.25\n",
            "Step 3380\t score: 29.22\tAverage Q: 5.32\n",
            "Step 3390\t score: 35.04\tAverage Q: 5.26\n",
            "Step 3400\t score: 35.06\tAverage Q: 5.14\n",
            "Step 3410\t score: 36.63\tAverage Q: 4.97\n",
            "Step 3420\t score: 39.37\tAverage Q: 4.72\n",
            "Step 3430\t score: 37.96\tAverage Q: 4.49\n",
            "Step 3440\t score: 40.11\tAverage Q: 4.77\n",
            "Step 3450\t score: 36.49\tAverage Q: 4.72\n",
            "Step 3460\t score: 39.52\tAverage Q: 4.95\n",
            "Step 3470\t score: 36.24\tAverage Q: 4.95\n",
            "Step 3480\t score: 39.66\tAverage Q: 4.77\n",
            "Step 3490\t score: 35.49\tAverage Q: 4.61\n",
            "Step 3500\t score: 31.80\tAverage Q: 4.70\n",
            "Step 3510\t score: 36.77\tAverage Q: 4.52\n",
            "Step 3520\t score: 38.46\tAverage Q: 4.54\n",
            "Step 3530\t score: 36.07\tAverage Q: 4.67\n",
            "Step 3540\t score: 38.73\tAverage Q: 4.62\n",
            "Step 3550\t score: 37.69\tAverage Q: 4.85\n",
            "Step 3560\t score: 40.78\tAverage Q: 4.77\n",
            "Step 3570\t score: 38.31\tAverage Q: 4.61\n",
            "Step 3580\t score: 37.53\tAverage Q: 5.17\n",
            "Step 3590\t score: 38.40\tAverage Q: 5.14\n",
            "Step 3600\t score: 39.15\tAverage Q: 5.17\n",
            "Step 3610\t score: 35.90\tAverage Q: 5.38\n",
            "Step 3620\t score: 39.38\tAverage Q: 5.39\n",
            "Step 3630\t score: 38.19\tAverage Q: 5.27\n",
            "Step 3640\t score: 37.54\tAverage Q: 5.34\n",
            "Step 3650\t score: 38.53\tAverage Q: 5.35\n",
            "Step 3660\t score: 38.07\tAverage Q: 5.70\n",
            "Step 3670\t score: 39.83\tAverage Q: 5.74\n",
            "Step 3680\t score: 36.73\tAverage Q: 5.40\n",
            "Step 3690\t score: 39.08\tAverage Q: 5.41\n",
            "Step 3700\t score: 36.19\tAverage Q: 5.33\n",
            "Step 3710\t score: 37.94\tAverage Q: 5.15\n",
            "Step 3720\t score: 36.53\tAverage Q: 4.94\n",
            "Step 3730\t score: 35.46\tAverage Q: 5.31\n",
            "Step 3740\t score: 41.53\tAverage Q: 5.11\n",
            "Step 3750\t score: 36.77\tAverage Q: 5.13\n",
            "Step 3760\t score: 38.52\tAverage Q: 5.10\n",
            "Step 3770\t score: 36.18\tAverage Q: 4.99\n",
            "Step 3780\t score: 37.75\tAverage Q: 4.87\n",
            "Step 3790\t score: 36.15\tAverage Q: 4.83\n",
            "Step 3800\t score: 36.85\tAverage Q: 4.70\n",
            "Step 3810\t score: 39.25\tAverage Q: 4.73\n",
            "Step 3820\t score: 35.64\tAverage Q: 4.65\n",
            "Step 3830\t score: 39.15\tAverage Q: 4.65\n",
            "Step 3840\t score: 32.01\tAverage Q: 5.08\n",
            "Step 3850\t score: 39.51\tAverage Q: 6.10\n",
            "Step 3860\t score: 35.29\tAverage Q: 5.71\n",
            "Step 3870\t score: 35.81\tAverage Q: 5.65\n",
            "Step 3880\t score: 39.98\tAverage Q: 5.84\n",
            "Step 3890\t score: 40.00\tAverage Q: 5.52\n",
            "Step 3900\t score: 37.77\tAverage Q: 5.34\n",
            "Step 3910\t score: 42.16\tAverage Q: 5.26\n",
            "Step 3920\t score: 39.49\tAverage Q: 5.23\n",
            "Step 3930\t score: 38.82\tAverage Q: 4.88\n",
            "Step 3940\t score: 40.00\tAverage Q: 4.83\n",
            "Step 3950\t score: 37.09\tAverage Q: 4.61\n",
            "Step 3960\t score: 38.45\tAverage Q: 4.64\n",
            "Step 3970\t score: 36.70\tAverage Q: 4.88\n",
            "Step 3980\t score: 37.57\tAverage Q: 4.81\n",
            "Step 3990\t score: 38.43\tAverage Q: 4.88\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "seed_num = 2022\n",
        "torch.manual_seed(seed_num)\n",
        "torch.cuda.manual_seed(seed_num)\n",
        "torch.cuda.manual_seed_all(seed_num)\n",
        "np.random.seed(seed_num)\n",
        "random.seed(seed_num)\n",
        "\n",
        "\n",
        "TrainingRecord = namedtuple('TrainingRecord', ['ep', 'reward'])\n",
        "Transition = namedtuple('Transition', ['s', 'a', 'r', 's_'])\n",
        "\n",
        "gamma = 0.98\n",
        "log_interval = 10\n",
        "\n",
        "class ActorNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ActorNet, self).__init__()\n",
        "        self.fc = nn.Linear(8, 100)\n",
        "        self.mu_head = nn.Linear(100, 2)\n",
        "\n",
        "    def forward(self, s):\n",
        "        x = F.relu(self.fc(s))\n",
        "        u = 2.0 * F.tanh(self.mu_head(x)) # pendulum task의 경우, action의 범위가 -2~2이므로, tanh를 활용하여 -1~1로 출력시키고 여기에 x2를 해준다.\n",
        "        return u\n",
        "\n",
        "\n",
        "class CriticNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CriticNet, self).__init__()\n",
        "        self.fc = nn.Linear(10, 100)\n",
        "        self.v_head = nn.Linear(100, 2)\n",
        "\n",
        "    def forward(self, s, a):\n",
        "        # print(s.shape, a.shape)\n",
        "        # print(s, a, torch.cat([s, a], dim=1), sep = '\\n\\n')\n",
        "        # s: tensor([[0.8553, -0.5181, 4.4607],[0.6519, -0.7583, 6.2618],[-0.8653, -0.5013, -6.8173], [0.1435, -0.9897, 7.0537]...  [0.1435, -1.9897, 5.053]]\n",
        "        # a: tensor([[ 2.7781], [ 1.7413],[ 0.0121],[ 0.5088],[ 0.1754],... [0.1211]\n",
        "        # tensor([[ 0.8553, -0.5181,  4.4607,  2.7781], [ 0.6519, -0.7583,  6.2618,  1.7413], [-0.8653, -0.5013, -6.8173,  0.0121], [ 0.1435, -0.9897,  7.0537,  0.5088],... [-0.8653, -0.5013, -6.8171,  0.1231]]\n",
        "\n",
        "        x = F.relu(self.fc(torch.cat([s, a], dim=1)))\n",
        "        state_value = self.v_head(x)  # 출력(state_value)은 선택된 1개의 action  (continuous value)\n",
        "        return state_value            # 출력(state_value)은 선택된 1개의 action\n",
        "\n",
        "\n",
        "class Memory():\n",
        "\n",
        "    data_pointer = 0\n",
        "    isfull = False\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = np.empty(capacity, dtype=object)\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def update(self, transition):\n",
        "        self.memory[self.data_pointer] = transition\n",
        "        self.data_pointer += 1\n",
        "        if self.data_pointer == self.capacity:\n",
        "            self.data_pointer = 0\n",
        "            self.isfull = True\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return np.random.choice(self.memory, batch_size)\n",
        "\n",
        "\n",
        "class Agent():\n",
        "\n",
        "    max_grad_norm = 0.5\n",
        "\n",
        "    def __init__(self):\n",
        "        self.training_step = 0\n",
        "        self.var = 1.\n",
        "        self.eval_cnet, self.target_cnet = CriticNet().float(), CriticNet().float()  # critic_main, critic_target\n",
        "        self.eval_anet, self.target_anet = ActorNet().float(), ActorNet().float()    # actor_main, actor_target\n",
        "        self.memory = Memory(2000)  # capacity = 2000\n",
        "        self.optimizer_c = optim.Adam(self.eval_cnet.parameters(), lr=1e-3)\n",
        "        self.optimizer_a = optim.Adam(self.eval_anet.parameters(), lr=3e-4)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        # print('state:', state, type(state))  # 출력 : state: [-0.07907514 -0.99686867  7.2913437 ], <class 'numpy.ndarray'>\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        mu = self.eval_anet(state) # actor_main의 출력 : action 값\n",
        "        dist = Normal(mu, torch.tensor(self.var, dtype=torch.float))  # (for explore)  action = action + Noise 을 구현하고자, 평균이 mu이고 분산이 1인 가우시안 분포를 만들고\n",
        "        action = dist.sample()                                        # 그 분포에서 1개의 값을 샘플링 함   => Noise를 더한것과 같은 효과\n",
        "        action.clamp(-1.0, 1.0)                                       # 값이 -2~2를 넘어가지 않도록 제한 (깍기)\n",
        "        # print(action)\n",
        "        return action.squeeze(0).numpy()\n",
        "\n",
        "    def save_param(self):\n",
        "        torch.save(self.eval_anet.state_dict(), '/content/drive/MyDrive/강화학습/DDPG_ACTOR_model4.pth')\n",
        "        torch.save(self.eval_cnet.state_dict(), '/content/drive/MyDrive/강화학습/DDPG_CRITIC_model4.pth')\n",
        "\n",
        "    def store_transition(self, transition):\n",
        "        self.memory.update(transition)\n",
        "\n",
        "    def update(self):           # gradient update\n",
        "        self.training_step += 1\n",
        "\n",
        "        transitions = self.memory.sample(32)\n",
        "        s = torch.tensor([t.s for t in transitions], dtype=torch.float)\n",
        "        a = torch.tensor([t.a for t in transitions], dtype=torch.float)\n",
        "        r = torch.tensor([t.r for t in transitions], dtype=torch.float).view(-1, 1)\n",
        "        s_ = torch.tensor([t.s_ for t in transitions], dtype=torch.float)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_target = r + gamma * self.target_cnet(s_, self.target_anet(s_))  # critic target 값 구하기 : actor_target으로 action을 얻어 (self.target_anet(s_)), critic_target(self.target_cnet)에 적용\n",
        "        q_eval = self.eval_cnet(s, a)  # critic output 값 구하기 : critic_main 출력 값 (Q값)\n",
        "\n",
        "        # update critic net\n",
        "        self.optimizer_c.zero_grad()\n",
        "        c_loss = F.smooth_l1_loss(q_eval, q_target)  # Critic의 Loss Function : Critic_Main과 Critic_target 간의 차이 (smooth_l1_loss)\n",
        "        c_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.eval_cnet.parameters(), self.max_grad_norm)  # gradient clipping (vanishing gradient를 방지하기 위한 즉, 학습을 잘되게 하기 위한 skill - 강화학습에만 국한되는게 아니라 여러 머신러닝에서 사용하는 스킬)\n",
        "        self.optimizer_c.step()\n",
        "\n",
        "        # update actor net\n",
        "        self.optimizer_a.zero_grad()\n",
        "        a_loss = -self.eval_cnet(s, self.eval_anet(s)).mean() # gradient ascent이므로 (-) 적용,   Actor의 Loss function : mean of Critic's Q_Value\n",
        "        a_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.eval_anet.parameters(), self.max_grad_norm)\n",
        "        self.optimizer_a.step()\n",
        "\n",
        "        if self.training_step % 200 == 0:\n",
        "            self.target_cnet.load_state_dict(self.eval_cnet.state_dict())\n",
        "        if self.training_step % 201 == 0:\n",
        "            self.target_anet.load_state_dict(self.eval_anet.state_dict())\n",
        "\n",
        "        self.var = max(self.var * 0.999, 0.01)\n",
        "\n",
        "        return q_eval.mean().item()\n",
        "\n",
        "\n",
        "def main():\n",
        "    env = gym.make('Swimmer')\n",
        "    agent = Agent()\n",
        "\n",
        "    training_records = []\n",
        "    return_list = []\n",
        "    running_reward, running_q = 0, 0\n",
        "    for i_ep in range(4000):\n",
        "        score = 0\n",
        "        state = env.reset()\n",
        "\n",
        "        for t in range(200):\n",
        "            action = agent.select_action(state)\n",
        "            state_, reward, done, _ = env.step(action)\n",
        "            score += reward\n",
        "            agent.store_transition(Transition(state, action, reward , state_))  # (reward + 8) / 8 는 일종의 학습이 잘되기 위한 trick\n",
        "            state = state_\n",
        "            if agent.memory.isfull:\n",
        "                q = agent.update()\n",
        "                running_q = 0.99 * running_q + 0.01 * q\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        running_reward = running_reward * 0.9 + score * 0.1\n",
        "\n",
        "        training_records.append(TrainingRecord(i_ep, running_reward))\n",
        "        return_list.append(score)\n",
        "        pd.DataFrame(return_list).to_csv('/content/drive/MyDrive/강화학습/DDPG_model_record4.csv')\n",
        "\n",
        "        if i_ep % log_interval == 0:\n",
        "            print('Step {}\\t score: {:.2f}\\tAverage Q: {:.2f}'.format(\n",
        "                i_ep, score, running_q))\n",
        "            agent.save_param()\n",
        "        #if running_reward > -200:\n",
        "        #    print(\"Solved! Running reward is now {}!\".format(running_reward))\n",
        "        #    env.close()\n",
        "        #    agent.save_param()\n",
        "        #    with open('ddpg_training_records.pkl', 'wb') as f:\n",
        "        #        pickle.dump(training_records, f)\n",
        "        #    break\n",
        "    agent.save_param()\n",
        "    env.close()\n",
        "\n",
        "    plt.plot([r.ep for r in training_records], [r.reward for r in training_records])\n",
        "    plt.title('DDPG')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Moving averaged episode reward')\n",
        "    plt.savefig(\"ddpg4.png\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b14278af-a538-4378-fc7c-9a7b23583cfb",
        "id": "fmSS7La2M4_n"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0\t score: 25.26\tAverage Q: 0.00\n",
            "Step 10\t score: 27.95\tAverage Q: -0.09\n",
            "Step 20\t score: 8.34\tAverage Q: 1.97\n",
            "Step 30\t score: 16.61\tAverage Q: 1.40\n",
            "Step 40\t score: 16.60\tAverage Q: 1.45\n",
            "Step 50\t score: 18.81\tAverage Q: 1.16\n",
            "Step 60\t score: 22.64\tAverage Q: 1.02\n",
            "Step 70\t score: 9.98\tAverage Q: 0.90\n",
            "Step 80\t score: 9.38\tAverage Q: 1.05\n",
            "Step 90\t score: 14.80\tAverage Q: 1.07\n",
            "Step 100\t score: 18.00\tAverage Q: 1.19\n",
            "Step 110\t score: 15.15\tAverage Q: 1.49\n",
            "Step 120\t score: 14.25\tAverage Q: 1.02\n",
            "Step 130\t score: 15.42\tAverage Q: 1.28\n",
            "Step 140\t score: 10.17\tAverage Q: 0.94\n",
            "Step 150\t score: 14.91\tAverage Q: 1.02\n",
            "Step 160\t score: 15.87\tAverage Q: 1.45\n",
            "Step 170\t score: 11.90\tAverage Q: 1.42\n",
            "Step 180\t score: 7.63\tAverage Q: 1.02\n",
            "Step 190\t score: 19.38\tAverage Q: 0.84\n",
            "Step 200\t score: 19.90\tAverage Q: 1.07\n",
            "Step 210\t score: 20.83\tAverage Q: 1.38\n",
            "Step 220\t score: 18.21\tAverage Q: 1.02\n",
            "Step 230\t score: 13.63\tAverage Q: 1.04\n",
            "Step 240\t score: 8.84\tAverage Q: 0.95\n",
            "Step 250\t score: 13.70\tAverage Q: 0.82\n",
            "Step 260\t score: 11.55\tAverage Q: 1.03\n",
            "Step 270\t score: 14.10\tAverage Q: 1.13\n",
            "Step 280\t score: 17.95\tAverage Q: 0.68\n",
            "Step 290\t score: 12.76\tAverage Q: 0.71\n",
            "Step 300\t score: 18.31\tAverage Q: 0.66\n",
            "Step 310\t score: 14.39\tAverage Q: 0.98\n",
            "Step 320\t score: 6.89\tAverage Q: 0.75\n",
            "Step 330\t score: 16.62\tAverage Q: 0.46\n",
            "Step 340\t score: 13.42\tAverage Q: 0.59\n",
            "Step 350\t score: 20.53\tAverage Q: 0.63\n",
            "Step 360\t score: 21.05\tAverage Q: 0.71\n",
            "Step 370\t score: 13.92\tAverage Q: 0.70\n",
            "Step 380\t score: 12.15\tAverage Q: 0.85\n",
            "Step 390\t score: 15.38\tAverage Q: 0.84\n",
            "Step 400\t score: 5.55\tAverage Q: 0.77\n",
            "Step 410\t score: 16.21\tAverage Q: 1.01\n",
            "Step 420\t score: 13.07\tAverage Q: 0.67\n",
            "Step 430\t score: 22.55\tAverage Q: 1.10\n",
            "Step 440\t score: 11.39\tAverage Q: 1.00\n",
            "Step 450\t score: 16.23\tAverage Q: 1.22\n",
            "Step 460\t score: 11.17\tAverage Q: 0.86\n",
            "Step 470\t score: 15.06\tAverage Q: 0.51\n",
            "Step 480\t score: 14.60\tAverage Q: 1.00\n",
            "Step 490\t score: 12.63\tAverage Q: 0.68\n",
            "Step 500\t score: 14.90\tAverage Q: 0.93\n",
            "Step 510\t score: 11.57\tAverage Q: 1.21\n",
            "Step 520\t score: 10.01\tAverage Q: 1.34\n",
            "Step 530\t score: 12.54\tAverage Q: 0.52\n",
            "Step 540\t score: 17.97\tAverage Q: 0.66\n",
            "Step 550\t score: 14.70\tAverage Q: 1.26\n",
            "Step 560\t score: 13.29\tAverage Q: 0.51\n",
            "Step 570\t score: 11.06\tAverage Q: 0.68\n",
            "Step 580\t score: 24.00\tAverage Q: 0.86\n",
            "Step 590\t score: 14.30\tAverage Q: 0.82\n",
            "Step 600\t score: 8.70\tAverage Q: 0.60\n",
            "Step 610\t score: 12.57\tAverage Q: 0.85\n",
            "Step 620\t score: 19.42\tAverage Q: 0.61\n",
            "Step 630\t score: 6.72\tAverage Q: 0.13\n",
            "Step 640\t score: 16.99\tAverage Q: 0.77\n",
            "Step 650\t score: 15.00\tAverage Q: 0.52\n",
            "Step 660\t score: 10.53\tAverage Q: 0.57\n",
            "Step 670\t score: 14.39\tAverage Q: 0.66\n",
            "Step 680\t score: 23.44\tAverage Q: 0.75\n",
            "Step 690\t score: 18.45\tAverage Q: 0.87\n",
            "Step 700\t score: 21.32\tAverage Q: 0.72\n",
            "Step 710\t score: 14.16\tAverage Q: 0.85\n",
            "Step 720\t score: 13.65\tAverage Q: 0.90\n",
            "Step 730\t score: 14.32\tAverage Q: 0.87\n",
            "Step 740\t score: 13.11\tAverage Q: 0.45\n",
            "Step 750\t score: 14.58\tAverage Q: 0.40\n",
            "Step 760\t score: 23.59\tAverage Q: 1.08\n",
            "Step 770\t score: 16.38\tAverage Q: 0.99\n",
            "Step 780\t score: 20.26\tAverage Q: 0.55\n",
            "Step 790\t score: 12.81\tAverage Q: 0.98\n",
            "Step 800\t score: 15.93\tAverage Q: 0.94\n",
            "Step 810\t score: 8.06\tAverage Q: 0.65\n",
            "Step 820\t score: 12.96\tAverage Q: 0.91\n",
            "Step 830\t score: 19.65\tAverage Q: 0.73\n",
            "Step 840\t score: 20.61\tAverage Q: 0.48\n",
            "Step 850\t score: 11.04\tAverage Q: 1.34\n",
            "Step 860\t score: 17.48\tAverage Q: 1.15\n",
            "Step 870\t score: 18.52\tAverage Q: 0.84\n",
            "Step 880\t score: 18.19\tAverage Q: 0.99\n",
            "Step 890\t score: 22.70\tAverage Q: 1.00\n",
            "Step 900\t score: 19.93\tAverage Q: 1.48\n",
            "Step 910\t score: 16.67\tAverage Q: 1.22\n",
            "Step 920\t score: 14.45\tAverage Q: 1.45\n",
            "Step 930\t score: 14.98\tAverage Q: 0.80\n",
            "Step 940\t score: 19.00\tAverage Q: 1.05\n",
            "Step 950\t score: 12.14\tAverage Q: 0.67\n",
            "Step 960\t score: 9.76\tAverage Q: 0.90\n",
            "Step 970\t score: 15.14\tAverage Q: 0.96\n",
            "Step 980\t score: 20.77\tAverage Q: 0.86\n",
            "Step 990\t score: 18.02\tAverage Q: 0.78\n",
            "Step 1000\t score: 20.16\tAverage Q: 1.11\n",
            "Step 1010\t score: 17.71\tAverage Q: 1.10\n",
            "Step 1020\t score: 18.40\tAverage Q: 0.65\n",
            "Step 1030\t score: 21.22\tAverage Q: 0.76\n",
            "Step 1040\t score: 7.83\tAverage Q: 1.23\n",
            "Step 1050\t score: 4.92\tAverage Q: 0.99\n",
            "Step 1060\t score: 12.52\tAverage Q: 1.40\n",
            "Step 1070\t score: 13.35\tAverage Q: 0.83\n",
            "Step 1080\t score: 4.10\tAverage Q: 1.22\n",
            "Step 1090\t score: 11.74\tAverage Q: 1.09\n",
            "Step 1100\t score: 10.34\tAverage Q: 1.02\n",
            "Step 1110\t score: 18.02\tAverage Q: 1.00\n",
            "Step 1120\t score: 16.02\tAverage Q: 1.02\n",
            "Step 1130\t score: 17.42\tAverage Q: 1.00\n",
            "Step 1140\t score: 11.45\tAverage Q: 1.13\n",
            "Step 1150\t score: 12.75\tAverage Q: 1.41\n",
            "Step 1160\t score: 18.53\tAverage Q: 0.93\n",
            "Step 1170\t score: 20.12\tAverage Q: 0.94\n",
            "Step 1180\t score: 17.65\tAverage Q: 1.15\n",
            "Step 1190\t score: 10.52\tAverage Q: 1.25\n",
            "Step 1200\t score: 15.54\tAverage Q: 1.12\n",
            "Step 1210\t score: 22.92\tAverage Q: 0.74\n",
            "Step 1220\t score: 11.26\tAverage Q: 1.09\n",
            "Step 1230\t score: 17.08\tAverage Q: 0.46\n",
            "Step 1240\t score: 16.65\tAverage Q: 0.69\n",
            "Step 1250\t score: 18.33\tAverage Q: 0.79\n",
            "Step 1260\t score: 17.74\tAverage Q: 0.52\n",
            "Step 1270\t score: 12.50\tAverage Q: 0.82\n",
            "Step 1280\t score: 12.53\tAverage Q: 0.33\n",
            "Step 1290\t score: 17.38\tAverage Q: 0.63\n",
            "Step 1300\t score: 17.08\tAverage Q: 1.10\n",
            "Step 1310\t score: 12.93\tAverage Q: 1.19\n",
            "Step 1320\t score: 18.07\tAverage Q: 0.86\n",
            "Step 1330\t score: 20.81\tAverage Q: 1.11\n",
            "Step 1340\t score: 10.44\tAverage Q: 1.18\n",
            "Step 1350\t score: 11.84\tAverage Q: 0.75\n",
            "Step 1360\t score: 21.49\tAverage Q: 0.98\n",
            "Step 1370\t score: 12.95\tAverage Q: 0.82\n",
            "Step 1380\t score: 14.68\tAverage Q: 0.76\n",
            "Step 1390\t score: 16.78\tAverage Q: 0.83\n",
            "Step 1400\t score: 21.84\tAverage Q: 0.74\n",
            "Step 1410\t score: 11.86\tAverage Q: 0.76\n",
            "Step 1420\t score: 14.98\tAverage Q: 0.88\n",
            "Step 1430\t score: 14.22\tAverage Q: 0.39\n",
            "Step 1440\t score: 8.16\tAverage Q: 0.19\n",
            "Step 1450\t score: 9.16\tAverage Q: 0.66\n",
            "Step 1460\t score: 7.06\tAverage Q: 0.77\n",
            "Step 1470\t score: 14.26\tAverage Q: 0.80\n",
            "Step 1480\t score: 12.75\tAverage Q: 0.74\n",
            "Step 1490\t score: 13.05\tAverage Q: 0.63\n",
            "Step 1500\t score: 13.21\tAverage Q: 0.83\n",
            "Step 1510\t score: 12.94\tAverage Q: 0.69\n",
            "Step 1520\t score: 14.61\tAverage Q: 0.97\n",
            "Step 1530\t score: 7.54\tAverage Q: 0.39\n",
            "Step 1540\t score: 14.07\tAverage Q: 0.12\n",
            "Step 1550\t score: 19.90\tAverage Q: 0.92\n",
            "Step 1560\t score: 14.87\tAverage Q: 0.44\n",
            "Step 1570\t score: 14.02\tAverage Q: 0.53\n",
            "Step 1580\t score: 9.34\tAverage Q: 0.46\n",
            "Step 1590\t score: 14.88\tAverage Q: 0.36\n",
            "Step 1600\t score: 10.51\tAverage Q: 0.85\n",
            "Step 1610\t score: 12.96\tAverage Q: 0.70\n",
            "Step 1620\t score: 7.81\tAverage Q: 0.41\n",
            "Step 1630\t score: 12.74\tAverage Q: 0.34\n",
            "Step 1640\t score: 12.76\tAverage Q: 0.33\n",
            "Step 1650\t score: 11.60\tAverage Q: 0.86\n",
            "Step 1660\t score: 13.54\tAverage Q: 0.60\n",
            "Step 1670\t score: 10.81\tAverage Q: 0.83\n",
            "Step 1680\t score: 17.38\tAverage Q: 0.75\n",
            "Step 1690\t score: 12.63\tAverage Q: 0.86\n",
            "Step 1700\t score: 14.44\tAverage Q: 0.75\n",
            "Step 1710\t score: 17.77\tAverage Q: 1.04\n",
            "Step 1720\t score: 21.82\tAverage Q: 1.01\n",
            "Step 1730\t score: 14.65\tAverage Q: 0.96\n",
            "Step 1740\t score: 10.05\tAverage Q: 1.05\n",
            "Step 1750\t score: 17.97\tAverage Q: 0.82\n",
            "Step 1760\t score: 15.18\tAverage Q: 1.03\n",
            "Step 1770\t score: 12.08\tAverage Q: 0.72\n",
            "Step 1780\t score: 19.88\tAverage Q: 0.47\n",
            "Step 1790\t score: 11.17\tAverage Q: 0.72\n",
            "Step 1800\t score: 16.85\tAverage Q: 0.72\n",
            "Step 1810\t score: 20.75\tAverage Q: 0.71\n",
            "Step 1820\t score: 11.88\tAverage Q: 0.82\n",
            "Step 1830\t score: 10.72\tAverage Q: 0.73\n",
            "Step 1840\t score: 17.56\tAverage Q: 0.96\n",
            "Step 1850\t score: 16.45\tAverage Q: 0.90\n",
            "Step 1860\t score: 13.48\tAverage Q: 1.10\n",
            "Step 1870\t score: 10.34\tAverage Q: 0.75\n",
            "Step 1880\t score: 10.41\tAverage Q: 1.05\n",
            "Step 1890\t score: 15.87\tAverage Q: 1.11\n",
            "Step 1900\t score: 7.26\tAverage Q: 0.76\n",
            "Step 1910\t score: 12.70\tAverage Q: 0.81\n",
            "Step 1920\t score: 15.82\tAverage Q: 0.92\n",
            "Step 1930\t score: 15.96\tAverage Q: 1.21\n",
            "Step 1940\t score: 11.62\tAverage Q: 1.04\n",
            "Step 1950\t score: 12.91\tAverage Q: 0.84\n",
            "Step 1960\t score: 17.86\tAverage Q: 0.95\n",
            "Step 1970\t score: 19.69\tAverage Q: 0.73\n",
            "Step 1980\t score: 12.90\tAverage Q: 0.59\n",
            "Step 1990\t score: 9.65\tAverage Q: 0.61\n",
            "Step 2000\t score: 20.01\tAverage Q: 0.76\n",
            "Step 2010\t score: 14.89\tAverage Q: 1.14\n",
            "Step 2020\t score: 9.87\tAverage Q: 1.04\n",
            "Step 2030\t score: 13.17\tAverage Q: 0.82\n",
            "Step 2040\t score: 19.88\tAverage Q: 1.18\n",
            "Step 2050\t score: 7.73\tAverage Q: 1.25\n",
            "Step 2060\t score: 11.49\tAverage Q: 1.16\n",
            "Step 2070\t score: 11.72\tAverage Q: 0.95\n",
            "Step 2080\t score: 11.82\tAverage Q: 1.11\n",
            "Step 2090\t score: 7.68\tAverage Q: 0.77\n",
            "Step 2100\t score: 18.97\tAverage Q: 0.90\n",
            "Step 2110\t score: 16.54\tAverage Q: 0.94\n",
            "Step 2120\t score: 10.74\tAverage Q: 0.77\n",
            "Step 2130\t score: 12.46\tAverage Q: 1.04\n",
            "Step 2140\t score: 10.40\tAverage Q: 0.71\n",
            "Step 2150\t score: 15.45\tAverage Q: 0.77\n",
            "Step 2160\t score: 13.92\tAverage Q: 0.85\n",
            "Step 2170\t score: 19.34\tAverage Q: 0.95\n",
            "Step 2180\t score: 11.05\tAverage Q: 1.07\n",
            "Step 2190\t score: 17.29\tAverage Q: 0.81\n",
            "Step 2200\t score: 14.28\tAverage Q: 0.90\n",
            "Step 2210\t score: 16.01\tAverage Q: 0.79\n",
            "Step 2220\t score: 14.83\tAverage Q: 0.77\n",
            "Step 2230\t score: 14.53\tAverage Q: 0.84\n",
            "Step 2240\t score: 13.09\tAverage Q: 1.03\n",
            "Step 2250\t score: 11.56\tAverage Q: 0.80\n",
            "Step 2260\t score: 3.87\tAverage Q: 0.68\n",
            "Step 2270\t score: 9.89\tAverage Q: 1.05\n",
            "Step 2280\t score: 10.17\tAverage Q: 0.54\n",
            "Step 2290\t score: 9.18\tAverage Q: 0.41\n",
            "Step 2300\t score: 17.49\tAverage Q: 0.70\n",
            "Step 2310\t score: 13.67\tAverage Q: 0.97\n",
            "Step 2320\t score: 12.46\tAverage Q: 0.75\n",
            "Step 2330\t score: 17.96\tAverage Q: 0.81\n",
            "Step 2340\t score: 15.76\tAverage Q: 0.73\n",
            "Step 2350\t score: 13.41\tAverage Q: 0.83\n",
            "Step 2360\t score: 7.98\tAverage Q: 0.95\n",
            "Step 2370\t score: 11.52\tAverage Q: 0.79\n",
            "Step 2380\t score: 8.74\tAverage Q: 0.72\n",
            "Step 2390\t score: 5.19\tAverage Q: 0.36\n",
            "Step 2400\t score: 9.02\tAverage Q: 0.77\n",
            "Step 2410\t score: 21.77\tAverage Q: 0.67\n",
            "Step 2420\t score: 12.51\tAverage Q: 0.83\n",
            "Step 2430\t score: 14.02\tAverage Q: 1.15\n",
            "Step 2440\t score: 8.34\tAverage Q: 0.71\n",
            "Step 2450\t score: 13.77\tAverage Q: 0.42\n",
            "Step 2460\t score: 20.66\tAverage Q: 0.60\n",
            "Step 2470\t score: 14.22\tAverage Q: 0.70\n",
            "Step 2480\t score: 12.30\tAverage Q: 0.80\n",
            "Step 2490\t score: 13.54\tAverage Q: 1.10\n",
            "Step 2500\t score: 9.22\tAverage Q: 0.71\n",
            "Step 2510\t score: 15.24\tAverage Q: 0.85\n",
            "Step 2520\t score: 13.57\tAverage Q: 0.54\n",
            "Step 2530\t score: 11.12\tAverage Q: 0.58\n",
            "Step 2540\t score: 15.64\tAverage Q: 0.91\n",
            "Step 2550\t score: 18.29\tAverage Q: 0.78\n",
            "Step 2560\t score: 21.51\tAverage Q: 0.78\n",
            "Step 2570\t score: 8.45\tAverage Q: 0.70\n",
            "Step 2580\t score: 12.65\tAverage Q: 0.88\n",
            "Step 2590\t score: 15.70\tAverage Q: 0.95\n",
            "Step 2600\t score: 17.19\tAverage Q: 0.78\n",
            "Step 2610\t score: 13.65\tAverage Q: 0.70\n",
            "Step 2620\t score: 14.24\tAverage Q: 0.54\n",
            "Step 2630\t score: 7.36\tAverage Q: 0.57\n",
            "Step 2640\t score: 10.06\tAverage Q: 1.17\n",
            "Step 2650\t score: 8.83\tAverage Q: 0.69\n",
            "Step 2660\t score: 12.65\tAverage Q: 0.90\n",
            "Step 2670\t score: 12.90\tAverage Q: 1.05\n",
            "Step 2680\t score: 17.39\tAverage Q: 1.07\n",
            "Step 2690\t score: 15.72\tAverage Q: 0.72\n",
            "Step 2700\t score: 15.42\tAverage Q: 0.60\n",
            "Step 2710\t score: 16.52\tAverage Q: 0.95\n",
            "Step 2720\t score: 17.52\tAverage Q: 0.82\n",
            "Step 2730\t score: 7.95\tAverage Q: 1.08\n",
            "Step 2740\t score: 18.85\tAverage Q: 0.90\n",
            "Step 2750\t score: 10.44\tAverage Q: 0.69\n",
            "Step 2760\t score: 17.81\tAverage Q: 0.67\n",
            "Step 2770\t score: 10.44\tAverage Q: 0.68\n",
            "Step 2780\t score: 19.90\tAverage Q: 0.66\n",
            "Step 2790\t score: 12.35\tAverage Q: 0.98\n",
            "Step 2800\t score: 10.29\tAverage Q: 0.59\n",
            "Step 2810\t score: 15.08\tAverage Q: 0.85\n",
            "Step 2820\t score: 13.45\tAverage Q: 0.90\n",
            "Step 2830\t score: 19.45\tAverage Q: 0.81\n",
            "Step 2840\t score: 10.13\tAverage Q: 0.95\n",
            "Step 2850\t score: 21.73\tAverage Q: 0.96\n",
            "Step 2860\t score: 16.86\tAverage Q: 1.14\n",
            "Step 2870\t score: 13.47\tAverage Q: 1.56\n",
            "Step 2880\t score: 4.07\tAverage Q: 1.09\n",
            "Step 2890\t score: 17.15\tAverage Q: 1.19\n",
            "Step 2900\t score: 13.85\tAverage Q: 0.98\n",
            "Step 2910\t score: 10.53\tAverage Q: 0.98\n",
            "Step 2920\t score: 18.80\tAverage Q: 1.22\n",
            "Step 2930\t score: 18.17\tAverage Q: 0.74\n",
            "Step 2940\t score: 14.71\tAverage Q: 0.71\n",
            "Step 2950\t score: 12.69\tAverage Q: 1.21\n",
            "Step 2960\t score: 7.60\tAverage Q: 0.94\n",
            "Step 2970\t score: 11.35\tAverage Q: 0.99\n",
            "Step 2980\t score: 19.72\tAverage Q: 0.99\n",
            "Step 2990\t score: 17.16\tAverage Q: 0.41\n",
            "Step 3000\t score: 12.39\tAverage Q: 0.55\n",
            "Step 3010\t score: 17.74\tAverage Q: 0.97\n",
            "Step 3020\t score: 18.04\tAverage Q: 0.77\n",
            "Step 3030\t score: 13.90\tAverage Q: 0.93\n",
            "Step 3040\t score: 12.69\tAverage Q: 0.93\n",
            "Step 3050\t score: 21.98\tAverage Q: 1.23\n",
            "Step 3060\t score: 12.13\tAverage Q: 1.00\n",
            "Step 3070\t score: 17.04\tAverage Q: 0.65\n",
            "Step 3080\t score: 22.77\tAverage Q: 0.65\n",
            "Step 3090\t score: 18.79\tAverage Q: 0.83\n",
            "Step 3100\t score: 8.09\tAverage Q: 1.20\n",
            "Step 3110\t score: 21.46\tAverage Q: 0.88\n",
            "Step 3120\t score: 16.06\tAverage Q: 0.72\n",
            "Step 3130\t score: 15.47\tAverage Q: 0.90\n",
            "Step 3140\t score: 19.31\tAverage Q: 0.88\n",
            "Step 3150\t score: 16.90\tAverage Q: 1.27\n",
            "Step 3160\t score: 15.51\tAverage Q: 0.73\n",
            "Step 3170\t score: 13.93\tAverage Q: 0.71\n",
            "Step 3180\t score: 8.51\tAverage Q: 0.99\n",
            "Step 3190\t score: 16.83\tAverage Q: 0.69\n",
            "Step 3200\t score: 16.37\tAverage Q: 0.61\n",
            "Step 3210\t score: 16.72\tAverage Q: 0.76\n",
            "Step 3220\t score: 11.12\tAverage Q: 0.87\n",
            "Step 3230\t score: 17.71\tAverage Q: 1.01\n",
            "Step 3240\t score: 11.97\tAverage Q: 0.74\n",
            "Step 3250\t score: 14.63\tAverage Q: 1.16\n",
            "Step 3260\t score: 12.85\tAverage Q: 0.70\n",
            "Step 3270\t score: 8.73\tAverage Q: 0.59\n",
            "Step 3280\t score: 13.72\tAverage Q: 0.76\n",
            "Step 3290\t score: 10.57\tAverage Q: 0.72\n",
            "Step 3300\t score: 13.74\tAverage Q: 0.80\n",
            "Step 3310\t score: 17.11\tAverage Q: 0.78\n",
            "Step 3320\t score: 16.85\tAverage Q: 1.08\n",
            "Step 3330\t score: 21.81\tAverage Q: 1.30\n",
            "Step 3340\t score: 13.46\tAverage Q: 1.07\n",
            "Step 3350\t score: 10.30\tAverage Q: 0.99\n",
            "Step 3360\t score: 6.63\tAverage Q: 0.74\n",
            "Step 3370\t score: 12.38\tAverage Q: 0.93\n",
            "Step 3380\t score: 14.99\tAverage Q: 0.92\n",
            "Step 3390\t score: 17.59\tAverage Q: 0.95\n",
            "Step 3400\t score: 21.23\tAverage Q: 0.93\n",
            "Step 3410\t score: 19.81\tAverage Q: 1.27\n",
            "Step 3420\t score: 17.31\tAverage Q: 1.00\n",
            "Step 3430\t score: 12.69\tAverage Q: 0.76\n",
            "Step 3440\t score: 12.60\tAverage Q: 1.20\n",
            "Step 3450\t score: 17.36\tAverage Q: 0.89\n",
            "Step 3460\t score: 16.76\tAverage Q: 0.70\n",
            "Step 3470\t score: 16.72\tAverage Q: 0.98\n",
            "Step 3480\t score: 6.25\tAverage Q: 0.73\n",
            "Step 3490\t score: 15.52\tAverage Q: 0.56\n",
            "Step 3500\t score: 24.16\tAverage Q: 0.78\n",
            "Step 3510\t score: 15.78\tAverage Q: 0.52\n",
            "Step 3520\t score: 10.17\tAverage Q: 0.83\n",
            "Step 3530\t score: 18.56\tAverage Q: 1.24\n",
            "Step 3540\t score: 9.29\tAverage Q: 0.58\n",
            "Step 3550\t score: 13.86\tAverage Q: 0.99\n",
            "Step 3560\t score: 22.51\tAverage Q: 1.53\n",
            "Step 3570\t score: 20.49\tAverage Q: 1.09\n",
            "Step 3580\t score: 16.83\tAverage Q: 1.16\n",
            "Step 3590\t score: 20.12\tAverage Q: 1.29\n",
            "Step 3600\t score: 11.08\tAverage Q: 0.82\n",
            "Step 3610\t score: 17.49\tAverage Q: 0.75\n",
            "Step 3620\t score: 7.69\tAverage Q: 0.98\n",
            "Step 3630\t score: 18.51\tAverage Q: 0.91\n",
            "Step 3640\t score: 20.92\tAverage Q: 0.60\n",
            "Step 3650\t score: 8.44\tAverage Q: 0.72\n",
            "Step 3660\t score: 11.36\tAverage Q: 0.60\n",
            "Step 3670\t score: 8.76\tAverage Q: 0.58\n",
            "Step 3680\t score: 14.95\tAverage Q: 0.37\n",
            "Step 3690\t score: 14.25\tAverage Q: 0.62\n",
            "Step 3700\t score: 19.63\tAverage Q: 0.81\n",
            "Step 3710\t score: 11.80\tAverage Q: 0.76\n",
            "Step 3720\t score: 11.25\tAverage Q: 0.95\n",
            "Step 3730\t score: 18.79\tAverage Q: 0.86\n",
            "Step 3740\t score: 12.62\tAverage Q: 0.76\n",
            "Step 3750\t score: 12.47\tAverage Q: 1.00\n",
            "Step 3760\t score: 12.31\tAverage Q: 0.99\n",
            "Step 3770\t score: 24.63\tAverage Q: 1.11\n",
            "Step 3780\t score: 20.49\tAverage Q: 1.06\n",
            "Step 3790\t score: 9.26\tAverage Q: 1.10\n",
            "Step 3800\t score: 16.50\tAverage Q: 0.99\n",
            "Step 3810\t score: 11.51\tAverage Q: 1.04\n",
            "Step 3820\t score: 19.96\tAverage Q: 0.97\n",
            "Step 3830\t score: 15.47\tAverage Q: 0.84\n",
            "Step 3840\t score: 10.30\tAverage Q: 0.67\n",
            "Step 3850\t score: 6.03\tAverage Q: 1.23\n",
            "Step 3860\t score: 14.64\tAverage Q: 1.20\n",
            "Step 3870\t score: 17.88\tAverage Q: 0.63\n",
            "Step 3880\t score: 13.91\tAverage Q: 0.74\n",
            "Step 3890\t score: 11.96\tAverage Q: 0.85\n",
            "Step 3900\t score: 16.38\tAverage Q: 0.79\n",
            "Step 3910\t score: 20.10\tAverage Q: 1.10\n",
            "Step 3920\t score: 12.52\tAverage Q: 0.77\n",
            "Step 3930\t score: 11.54\tAverage Q: 0.83\n",
            "Step 3940\t score: 21.19\tAverage Q: 1.19\n",
            "Step 3950\t score: 19.07\tAverage Q: 0.72\n",
            "Step 3960\t score: 18.11\tAverage Q: 1.34\n",
            "Step 3970\t score: 14.61\tAverage Q: 1.02\n",
            "Step 3980\t score: 22.72\tAverage Q: 1.32\n",
            "Step 3990\t score: 6.38\tAverage Q: 1.05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "seed_num = 700\n",
        "torch.manual_seed(seed_num)\n",
        "torch.cuda.manual_seed(seed_num)\n",
        "torch.cuda.manual_seed_all(seed_num)\n",
        "np.random.seed(seed_num)\n",
        "random.seed(seed_num)\n",
        "\n",
        "\n",
        "TrainingRecord = namedtuple('TrainingRecord', ['ep', 'reward'])\n",
        "Transition = namedtuple('Transition', ['s', 'a', 'r', 's_'])\n",
        "\n",
        "gamma = 0.98\n",
        "log_interval = 10\n",
        "\n",
        "class ActorNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ActorNet, self).__init__()\n",
        "        self.fc = nn.Linear(8, 100)\n",
        "        self.mu_head = nn.Linear(100, 2)\n",
        "\n",
        "    def forward(self, s):\n",
        "        x = F.relu(self.fc(s))\n",
        "        u = 2.0 * F.tanh(self.mu_head(x)) # pendulum task의 경우, action의 범위가 -2~2이므로, tanh를 활용하여 -1~1로 출력시키고 여기에 x2를 해준다.\n",
        "        return u\n",
        "\n",
        "\n",
        "class CriticNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CriticNet, self).__init__()\n",
        "        self.fc = nn.Linear(10, 100)\n",
        "        self.v_head = nn.Linear(100, 2)\n",
        "\n",
        "    def forward(self, s, a):\n",
        "        # print(s.shape, a.shape)\n",
        "        # print(s, a, torch.cat([s, a], dim=1), sep = '\\n\\n')\n",
        "        # s: tensor([[0.8553, -0.5181, 4.4607],[0.6519, -0.7583, 6.2618],[-0.8653, -0.5013, -6.8173], [0.1435, -0.9897, 7.0537]...  [0.1435, -1.9897, 5.053]]\n",
        "        # a: tensor([[ 2.7781], [ 1.7413],[ 0.0121],[ 0.5088],[ 0.1754],... [0.1211]\n",
        "        # tensor([[ 0.8553, -0.5181,  4.4607,  2.7781], [ 0.6519, -0.7583,  6.2618,  1.7413], [-0.8653, -0.5013, -6.8173,  0.0121], [ 0.1435, -0.9897,  7.0537,  0.5088],... [-0.8653, -0.5013, -6.8171,  0.1231]]\n",
        "\n",
        "        x = F.relu(self.fc(torch.cat([s, a], dim=1)))\n",
        "        state_value = self.v_head(x)  # 출력(state_value)은 선택된 1개의 action  (continuous value)\n",
        "        return state_value            # 출력(state_value)은 선택된 1개의 action\n",
        "\n",
        "\n",
        "class Memory():\n",
        "\n",
        "    data_pointer = 0\n",
        "    isfull = False\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = np.empty(capacity, dtype=object)\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def update(self, transition):\n",
        "        self.memory[self.data_pointer] = transition\n",
        "        self.data_pointer += 1\n",
        "        if self.data_pointer == self.capacity:\n",
        "            self.data_pointer = 0\n",
        "            self.isfull = True\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return np.random.choice(self.memory, batch_size)\n",
        "\n",
        "\n",
        "class Agent():\n",
        "\n",
        "    max_grad_norm = 0.5\n",
        "\n",
        "    def __init__(self):\n",
        "        self.training_step = 0\n",
        "        self.var = 1.\n",
        "        self.eval_cnet, self.target_cnet = CriticNet().float(), CriticNet().float()  # critic_main, critic_target\n",
        "        self.eval_anet, self.target_anet = ActorNet().float(), ActorNet().float()    # actor_main, actor_target\n",
        "        self.memory = Memory(2000)  # capacity = 2000\n",
        "        self.optimizer_c = optim.Adam(self.eval_cnet.parameters(), lr=1e-3)\n",
        "        self.optimizer_a = optim.Adam(self.eval_anet.parameters(), lr=3e-4)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        # print('state:', state, type(state))  # 출력 : state: [-0.07907514 -0.99686867  7.2913437 ], <class 'numpy.ndarray'>\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        mu = self.eval_anet(state) # actor_main의 출력 : action 값\n",
        "        dist = Normal(mu, torch.tensor(self.var, dtype=torch.float))  # (for explore)  action = action + Noise 을 구현하고자, 평균이 mu이고 분산이 1인 가우시안 분포를 만들고\n",
        "        action = dist.sample()                                        # 그 분포에서 1개의 값을 샘플링 함   => Noise를 더한것과 같은 효과\n",
        "        action.clamp(-1.0, 1.0)                                       # 값이 -2~2를 넘어가지 않도록 제한 (깍기)\n",
        "        # print(action)\n",
        "        return action.squeeze(0).numpy()\n",
        "\n",
        "    def save_param(self):\n",
        "        torch.save(self.eval_anet.state_dict(), '/content/drive/MyDrive/강화학습/DDPG_ACTOR_model2.pth')\n",
        "        torch.save(self.eval_cnet.state_dict(), '/content/drive/MyDrive/강화학습/DDPG_CRITIC_model2.pth')\n",
        "\n",
        "    def store_transition(self, transition):\n",
        "        self.memory.update(transition)\n",
        "\n",
        "    def update(self):           # gradient update\n",
        "        self.training_step += 1\n",
        "\n",
        "        transitions = self.memory.sample(32)\n",
        "        s = torch.tensor([t.s for t in transitions], dtype=torch.float)\n",
        "        a = torch.tensor([t.a for t in transitions], dtype=torch.float)\n",
        "        r = torch.tensor([t.r for t in transitions], dtype=torch.float).view(-1, 1)\n",
        "        s_ = torch.tensor([t.s_ for t in transitions], dtype=torch.float)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_target = r + gamma * self.target_cnet(s_, self.target_anet(s_))  # critic target 값 구하기 : actor_target으로 action을 얻어 (self.target_anet(s_)), critic_target(self.target_cnet)에 적용\n",
        "        q_eval = self.eval_cnet(s, a)  # critic output 값 구하기 : critic_main 출력 값 (Q값)\n",
        "\n",
        "        # update critic net\n",
        "        self.optimizer_c.zero_grad()\n",
        "        c_loss = F.smooth_l1_loss(q_eval, q_target)  # Critic의 Loss Function : Critic_Main과 Critic_target 간의 차이 (smooth_l1_loss)\n",
        "        c_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.eval_cnet.parameters(), self.max_grad_norm)  # gradient clipping (vanishing gradient를 방지하기 위한 즉, 학습을 잘되게 하기 위한 skill - 강화학습에만 국한되는게 아니라 여러 머신러닝에서 사용하는 스킬)\n",
        "        self.optimizer_c.step()\n",
        "\n",
        "        # update actor net\n",
        "        self.optimizer_a.zero_grad()\n",
        "        a_loss = -self.eval_cnet(s, self.eval_anet(s)).mean() # gradient ascent이므로 (-) 적용,   Actor의 Loss function : mean of Critic's Q_Value\n",
        "        a_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.eval_anet.parameters(), self.max_grad_norm)\n",
        "        self.optimizer_a.step()\n",
        "\n",
        "        if self.training_step % 200 == 0:\n",
        "            self.target_cnet.load_state_dict(self.eval_cnet.state_dict())\n",
        "        if self.training_step % 201 == 0:\n",
        "            self.target_anet.load_state_dict(self.eval_anet.state_dict())\n",
        "\n",
        "        self.var = max(self.var * 0.999, 0.01)\n",
        "\n",
        "        return q_eval.mean().item()\n",
        "\n",
        "\n",
        "def main():\n",
        "    env = gym.make('Swimmer')\n",
        "    agent = Agent()\n",
        "\n",
        "    training_records = []\n",
        "    return_list = []\n",
        "    running_reward, running_q = 0, 0\n",
        "    for i_ep in range(1000):\n",
        "        score = 0\n",
        "        state = env.reset()\n",
        "\n",
        "        for t in range(200):\n",
        "            action = agent.select_action(state)\n",
        "            # print(action.shape, state.shape)\n",
        "            state_, reward, done, _ = env.step(action)\n",
        "            # if i_ep >= 100: env.render()\n",
        "            score += reward\n",
        "            # if args.render:\n",
        "            #     if i_ep >= 50: env.render()\n",
        "            agent.store_transition(Transition(state, action, reward , state_))  # (reward + 8) / 8 는 일종의 학습이 잘되기 위한 trick\n",
        "            state = state_\n",
        "            if agent.memory.isfull:\n",
        "                q = agent.update()\n",
        "                running_q = 0.99 * running_q + 0.01 * q\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        running_reward = running_reward * 0.9 + score * 0.1\n",
        "\n",
        "        training_records.append(TrainingRecord(i_ep, running_reward))\n",
        "        return_list.append(score)\n",
        "        pd.DataFrame(return_list).to_csv('/content/drive/MyDrive/강화학습/DDPG_model_record2.csv')\n",
        "\n",
        "        if i_ep % log_interval == 0:\n",
        "            print('Step {}\\t score: {:.2f}\\tAverage Q: {:.2f}'.format(\n",
        "                i_ep, score, running_q))\n",
        "            agent.save_param()\n",
        "        #if running_reward > -200:\n",
        "        #    print(\"Solved! Running reward is now {}!\".format(running_reward))\n",
        "        #    env.close()\n",
        "        #    agent.save_param()\n",
        "        #    with open('ddpg_training_records.pkl', 'wb') as f:\n",
        "        #        pickle.dump(training_records, f)\n",
        "        #    break\n",
        "    agent.save_param()\n",
        "    env.close()\n",
        "\n",
        "    plt.plot([r.ep for r in training_records], [r.reward for r in training_records])\n",
        "    plt.title('DDPG')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Moving averaged episode reward')\n",
        "    plt.savefig(\"ddpg2.png\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jo65QempIcfR",
        "outputId": "d830d5da-0e72-44c4-8de5-59636cee71aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py:601: UserWarning: \u001b[33mWARN: Using the latest versioned environment `Swimmer-v4` instead of the unversioned environment `Swimmer`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1956: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0\t score: 3.53\tAverage Q: 0.00\n",
            "Step 10\t score: -2.11\tAverage Q: -0.02\n",
            "Step 20\t score: 23.79\tAverage Q: 4.75\n",
            "Step 30\t score: 24.86\tAverage Q: 4.82\n",
            "Step 40\t score: 27.85\tAverage Q: 4.59\n",
            "Step 50\t score: 24.79\tAverage Q: 4.28\n",
            "Step 60\t score: 27.36\tAverage Q: 3.84\n",
            "Step 70\t score: 28.44\tAverage Q: 3.49\n",
            "Step 80\t score: 31.99\tAverage Q: 3.84\n",
            "Step 90\t score: 34.15\tAverage Q: 4.25\n",
            "Step 100\t score: 13.45\tAverage Q: 4.99\n",
            "Step 110\t score: 34.25\tAverage Q: 5.06\n",
            "Step 120\t score: 32.36\tAverage Q: 5.37\n",
            "Step 130\t score: 30.21\tAverage Q: 5.10\n",
            "Step 140\t score: 32.61\tAverage Q: 5.30\n",
            "Step 150\t score: 36.68\tAverage Q: 5.57\n",
            "Step 160\t score: 32.87\tAverage Q: 5.21\n",
            "Step 170\t score: 30.68\tAverage Q: 5.79\n",
            "Step 180\t score: 34.70\tAverage Q: 5.50\n",
            "Step 190\t score: 34.04\tAverage Q: 5.49\n",
            "Step 200\t score: 32.52\tAverage Q: 5.25\n",
            "Step 210\t score: 34.99\tAverage Q: 5.37\n",
            "Step 220\t score: 34.56\tAverage Q: 5.18\n",
            "Step 230\t score: 35.34\tAverage Q: 5.17\n",
            "Step 240\t score: 38.00\tAverage Q: 4.97\n",
            "Step 250\t score: 34.95\tAverage Q: 5.18\n",
            "Step 260\t score: 32.54\tAverage Q: 5.66\n",
            "Step 270\t score: 33.07\tAverage Q: 5.38\n",
            "Step 280\t score: 31.92\tAverage Q: 6.36\n",
            "Step 290\t score: 32.92\tAverage Q: 6.21\n",
            "Step 300\t score: 33.52\tAverage Q: 6.15\n",
            "Step 310\t score: 35.02\tAverage Q: 6.11\n",
            "Step 320\t score: 39.26\tAverage Q: 5.81\n",
            "Step 330\t score: 33.19\tAverage Q: 6.03\n",
            "Step 340\t score: 34.23\tAverage Q: 5.79\n",
            "Step 350\t score: 35.42\tAverage Q: 5.48\n",
            "Step 360\t score: 36.43\tAverage Q: 5.61\n",
            "Step 370\t score: 31.71\tAverage Q: 5.78\n",
            "Step 380\t score: 37.17\tAverage Q: 5.80\n",
            "Step 390\t score: 35.09\tAverage Q: 6.49\n",
            "Step 400\t score: 34.18\tAverage Q: 6.22\n",
            "Step 410\t score: 35.02\tAverage Q: 5.81\n",
            "Step 420\t score: 38.80\tAverage Q: 5.99\n",
            "Step 430\t score: 32.62\tAverage Q: 6.00\n",
            "Step 440\t score: 36.54\tAverage Q: 5.97\n",
            "Step 450\t score: 34.43\tAverage Q: 5.94\n",
            "Step 460\t score: 36.99\tAverage Q: 6.32\n",
            "Step 470\t score: 39.28\tAverage Q: 5.77\n",
            "Step 480\t score: 36.24\tAverage Q: 5.68\n",
            "Step 490\t score: 39.74\tAverage Q: 5.59\n",
            "Step 500\t score: 35.65\tAverage Q: 5.68\n",
            "Step 510\t score: 33.26\tAverage Q: 5.46\n",
            "Step 520\t score: 35.22\tAverage Q: 5.23\n",
            "Step 530\t score: 36.33\tAverage Q: 4.93\n",
            "Step 540\t score: 38.95\tAverage Q: 4.94\n",
            "Step 550\t score: 38.58\tAverage Q: 4.90\n",
            "Step 560\t score: 37.47\tAverage Q: 5.06\n",
            "Step 570\t score: 34.68\tAverage Q: 5.00\n",
            "Step 580\t score: 35.38\tAverage Q: 5.16\n",
            "Step 590\t score: 32.97\tAverage Q: 5.07\n",
            "Step 600\t score: 35.70\tAverage Q: 4.89\n",
            "Step 610\t score: 34.89\tAverage Q: 4.74\n",
            "Step 620\t score: 35.63\tAverage Q: 4.86\n",
            "Step 630\t score: 34.32\tAverage Q: 4.63\n",
            "Step 640\t score: 36.12\tAverage Q: 4.47\n",
            "Step 650\t score: 32.21\tAverage Q: 4.47\n",
            "Step 660\t score: 32.16\tAverage Q: 4.47\n",
            "Step 670\t score: 34.52\tAverage Q: 4.38\n",
            "Step 680\t score: 31.33\tAverage Q: 4.38\n",
            "Step 690\t score: 35.12\tAverage Q: 4.29\n",
            "Step 700\t score: 37.46\tAverage Q: 4.59\n",
            "Step 710\t score: 35.19\tAverage Q: 4.39\n",
            "Step 720\t score: 37.10\tAverage Q: 4.47\n",
            "Step 730\t score: 38.55\tAverage Q: 4.37\n",
            "Step 740\t score: 35.14\tAverage Q: 4.48\n",
            "Step 750\t score: 34.14\tAverage Q: 4.48\n",
            "Step 760\t score: 33.85\tAverage Q: 4.47\n",
            "Step 770\t score: 33.84\tAverage Q: 4.50\n",
            "Step 780\t score: 36.45\tAverage Q: 4.69\n",
            "Step 790\t score: 34.99\tAverage Q: 4.68\n",
            "Step 800\t score: 35.31\tAverage Q: 4.63\n",
            "Step 810\t score: 36.76\tAverage Q: 4.72\n",
            "Step 820\t score: 35.65\tAverage Q: 4.66\n",
            "Step 830\t score: 35.95\tAverage Q: 4.46\n",
            "Step 840\t score: 35.34\tAverage Q: 4.58\n",
            "Step 850\t score: 36.15\tAverage Q: 4.46\n",
            "Step 860\t score: 39.34\tAverage Q: 4.61\n",
            "Step 870\t score: 35.47\tAverage Q: 4.50\n",
            "Step 880\t score: 34.88\tAverage Q: 4.49\n",
            "Step 890\t score: 34.90\tAverage Q: 4.69\n",
            "Step 900\t score: 36.17\tAverage Q: 4.73\n",
            "Step 910\t score: 39.60\tAverage Q: 4.63\n",
            "Step 920\t score: 28.99\tAverage Q: 8.52\n",
            "Step 930\t score: 26.28\tAverage Q: 7.49\n",
            "Step 940\t score: 31.96\tAverage Q: 6.60\n",
            "Step 950\t score: 32.50\tAverage Q: 7.05\n",
            "Step 960\t score: 33.85\tAverage Q: 7.00\n",
            "Step 970\t score: 36.60\tAverage Q: 6.54\n",
            "Step 980\t score: 37.77\tAverage Q: 6.29\n",
            "Step 990\t score: 39.12\tAverage Q: 6.31\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python DDPG_swimmer.py\n",
        "\n",
        "import pickle\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "seed_num = 0\n",
        "torch.manual_seed(seed_num)\n",
        "torch.cuda.manual_seed(seed_num)\n",
        "torch.cuda.manual_seed_all(seed_num)\n",
        "np.random.seed(seed_num)\n",
        "random.seed(seed_num)\n",
        "\n",
        "\n",
        "TrainingRecord = namedtuple('TrainingRecord', ['ep', 'reward'])\n",
        "Transition = namedtuple('Transition', ['s', 'a', 'r', 's_'])\n",
        "\n",
        "gamma = 0.98\n",
        "log_interval = 10\n",
        "\n",
        "class ActorNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ActorNet, self).__init__()\n",
        "        self.fc = nn.Linear(8, 100)\n",
        "        self.mu_head = nn.Linear(100, 2)\n",
        "\n",
        "    def forward(self, s):\n",
        "        x = F.relu(self.fc(s))\n",
        "        u = 2.0 * F.tanh(self.mu_head(x)) # pendulum task의 경우, action의 범위가 -2~2이므로, tanh를 활용하여 -1~1로 출력시키고 여기에 x2를 해준다.\n",
        "        return u\n",
        "\n",
        "\n",
        "class CriticNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CriticNet, self).__init__()\n",
        "        self.fc = nn.Linear(10, 100)\n",
        "        self.v_head = nn.Linear(100, 2)\n",
        "\n",
        "    def forward(self, s, a):\n",
        "        # print(s.shape, a.shape)\n",
        "        # print(s, a, torch.cat([s, a], dim=1), sep = '\\n\\n')\n",
        "        # s: tensor([[0.8553, -0.5181, 4.4607],[0.6519, -0.7583, 6.2618],[-0.8653, -0.5013, -6.8173], [0.1435, -0.9897, 7.0537]...  [0.1435, -1.9897, 5.053]]\n",
        "        # a: tensor([[ 2.7781], [ 1.7413],[ 0.0121],[ 0.5088],[ 0.1754],... [0.1211]\n",
        "        # tensor([[ 0.8553, -0.5181,  4.4607,  2.7781], [ 0.6519, -0.7583,  6.2618,  1.7413], [-0.8653, -0.5013, -6.8173,  0.0121], [ 0.1435, -0.9897,  7.0537,  0.5088],... [-0.8653, -0.5013, -6.8171,  0.1231]]\n",
        "\n",
        "        x = F.relu(self.fc(torch.cat([s, a], dim=1)))\n",
        "        state_value = self.v_head(x)  # 출력(state_value)은 선택된 1개의 action  (continuous value)\n",
        "        return state_value            # 출력(state_value)은 선택된 1개의 action\n",
        "\n",
        "\n",
        "class Memory():\n",
        "\n",
        "    data_pointer = 0\n",
        "    isfull = False\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = np.empty(capacity, dtype=object)\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def update(self, transition):\n",
        "        self.memory[self.data_pointer] = transition\n",
        "        self.data_pointer += 1\n",
        "        if self.data_pointer == self.capacity:\n",
        "            self.data_pointer = 0\n",
        "            self.isfull = True\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return np.random.choice(self.memory, batch_size)\n",
        "\n",
        "\n",
        "class Agent():\n",
        "\n",
        "    max_grad_norm = 0.5\n",
        "\n",
        "    def __init__(self):\n",
        "        self.training_step = 0\n",
        "        self.var = 1.\n",
        "        self.eval_cnet, self.target_cnet = CriticNet().float(), CriticNet().float()  # critic_main, critic_target\n",
        "        self.eval_anet, self.target_anet = ActorNet().float(), ActorNet().float()    # actor_main, actor_target\n",
        "        self.memory = Memory(2000)  # capacity = 2000\n",
        "        self.optimizer_c = optim.Adam(self.eval_cnet.parameters(), lr=1e-3)\n",
        "        self.optimizer_a = optim.Adam(self.eval_anet.parameters(), lr=3e-4)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        # print('state:', state, type(state))  # 출력 : state: [-0.07907514 -0.99686867  7.2913437 ], <class 'numpy.ndarray'>\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        mu = self.eval_anet(state) # actor_main의 출력 : action 값\n",
        "        dist = Normal(mu, torch.tensor(self.var, dtype=torch.float))  # (for explore)  action = action + Noise 을 구현하고자, 평균이 mu이고 분산이 1인 가우시안 분포를 만들고\n",
        "        action = dist.sample()                                        # 그 분포에서 1개의 값을 샘플링 함   => Noise를 더한것과 같은 효과\n",
        "        action.clamp(-1.0, 1.0)                                       # 값이 -2~2를 넘어가지 않도록 제한 (깍기)\n",
        "        # print(action)\n",
        "        return action.squeeze(0).numpy()\n",
        "\n",
        "    def save_param(self):\n",
        "        torch.save(self.eval_anet.state_dict(), '/content/drive/MyDrive/강화학습/DDPG_ACTOR_model2.pth')\n",
        "        torch.save(self.eval_cnet.state_dict(), '/content/drive/MyDrive/강화학습/DDPG_CRITIC_model2.pth')\n",
        "\n",
        "    def store_transition(self, transition):\n",
        "        self.memory.update(transition)\n",
        "\n",
        "    def update(self):           # gradient update\n",
        "        self.training_step += 1\n",
        "\n",
        "        transitions = self.memory.sample(32)\n",
        "        s = torch.tensor([t.s for t in transitions], dtype=torch.float)\n",
        "        a = torch.tensor([t.a for t in transitions], dtype=torch.float)\n",
        "        r = torch.tensor([t.r for t in transitions], dtype=torch.float).view(-1, 1)\n",
        "        s_ = torch.tensor([t.s_ for t in transitions], dtype=torch.float)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_target = r + gamma * self.target_cnet(s_, self.target_anet(s_))  # critic target 값 구하기 : actor_target으로 action을 얻어 (self.target_anet(s_)), critic_target(self.target_cnet)에 적용\n",
        "        q_eval = self.eval_cnet(s, a)  # critic output 값 구하기 : critic_main 출력 값 (Q값)\n",
        "\n",
        "        # update critic net\n",
        "        self.optimizer_c.zero_grad()\n",
        "        c_loss = F.smooth_l1_loss(q_eval, q_target)  # Critic의 Loss Function : Critic_Main과 Critic_target 간의 차이 (smooth_l1_loss)\n",
        "        c_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.eval_cnet.parameters(), self.max_grad_norm)  # gradient clipping (vanishing gradient를 방지하기 위한 즉, 학습을 잘되게 하기 위한 skill - 강화학습에만 국한되는게 아니라 여러 머신러닝에서 사용하는 스킬)\n",
        "        self.optimizer_c.step()\n",
        "\n",
        "        # update actor net\n",
        "        self.optimizer_a.zero_grad()\n",
        "        a_loss = -self.eval_cnet(s, self.eval_anet(s)).mean() # gradient ascent이므로 (-) 적용,   Actor의 Loss function : mean of Critic's Q_Value\n",
        "        a_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.eval_anet.parameters(), self.max_grad_norm)\n",
        "        self.optimizer_a.step()\n",
        "\n",
        "        if self.training_step % 200 == 0:\n",
        "            self.target_cnet.load_state_dict(self.eval_cnet.state_dict())\n",
        "        if self.training_step % 201 == 0:\n",
        "            self.target_anet.load_state_dict(self.eval_anet.state_dict())\n",
        "\n",
        "        self.var = max(self.var * 0.999, 0.01)\n",
        "\n",
        "        return q_eval.mean().item()\n",
        "\n",
        "\n",
        "def main():\n",
        "    env = gym.make('Swimmer')\n",
        "    agent = Agent()\n",
        "\n",
        "    training_records = []\n",
        "    return_list = []\n",
        "    running_reward, running_q = 0, 0\n",
        "    for i_ep in range(1000):\n",
        "        score = 0\n",
        "        state = env.reset()\n",
        "\n",
        "        for t in range(200):\n",
        "            action = agent.select_action(state)\n",
        "            # print(action.shape, state.shape)\n",
        "            state_, reward, done, _ = env.step(action)\n",
        "            # if i_ep >= 100: env.render()\n",
        "            score += reward\n",
        "            # if args.render:\n",
        "            #     if i_ep >= 50: env.render()\n",
        "            agent.store_transition(Transition(state, action, reward , state_))  # (reward + 8) / 8 는 일종의 학습이 잘되기 위한 trick\n",
        "            state = state_\n",
        "            if agent.memory.isfull:\n",
        "                q = agent.update()\n",
        "                running_q = 0.99 * running_q + 0.01 * q\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        running_reward = running_reward * 0.9 + score * 0.1\n",
        "\n",
        "        training_records.append(TrainingRecord(i_ep, running_reward))\n",
        "        return_list.append(score)\n",
        "        pd.DataFrame(return_list).to_csv('/content/drive/MyDrive/강화학습/DDPG_model_record2.csv')\n",
        "\n",
        "        if i_ep % log_interval == 0:\n",
        "            print('Step {}\\t score: {:.2f}\\tAverage Q: {:.2f}'.format(\n",
        "                i_ep, score, running_q))\n",
        "            agent.save_param()\n",
        "        #if running_reward > -200:\n",
        "        #    print(\"Solved! Running reward is now {}!\".format(running_reward))\n",
        "        #    env.close()\n",
        "        #    agent.save_param()\n",
        "        #    with open('ddpg_training_records.pkl', 'wb') as f:\n",
        "        #        pickle.dump(training_records, f)\n",
        "        #    break\n",
        "    agent.save_param()\n",
        "    env.close()\n",
        "\n",
        "    plt.plot([r.ep for r in training_records], [r.reward for r in training_records])\n",
        "    plt.title('DDPG')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Moving averaged episode reward')\n",
        "    plt.savefig(\"ddpg2.png\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7N1oO2N22bW",
        "outputId": "a7a8f0cf-5b89-4754-a44f-7ac476beef25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py:601: UserWarning: \u001b[33mWARN: Using the latest versioned environment `Swimmer-v4` instead of the unversioned environment `Swimmer`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1956: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "Step 0\tAverage score: -0.74\tAverage Q: 0.00\n",
            "DDPG_swimmer.py:120: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  s = torch.tensor([t.s for t in transitions], dtype=torch.float)\n",
            "Step 10\tAverage score: 2.08\tAverage Q: -0.12\n",
            "Step 20\tAverage score: 18.20\tAverage Q: 0.94\n",
            "Step 30\tAverage score: 23.70\tAverage Q: 1.03\n",
            "Step 40\tAverage score: 25.15\tAverage Q: 0.99\n",
            "Step 50\tAverage score: 26.20\tAverage Q: 1.02\n",
            "Step 60\tAverage score: 26.11\tAverage Q: 1.09\n",
            "Step 70\tAverage score: 26.06\tAverage Q: 1.06\n",
            "Step 80\tAverage score: 26.42\tAverage Q: 0.96\n",
            "Step 90\tAverage score: 26.52\tAverage Q: 1.07\n",
            "Step 100\tAverage score: 26.53\tAverage Q: 1.01\n",
            "Step 110\tAverage score: 26.50\tAverage Q: 0.99\n",
            "Step 120\tAverage score: 26.60\tAverage Q: 1.00\n",
            "Step 130\tAverage score: 26.52\tAverage Q: 1.06\n",
            "Step 140\tAverage score: 26.50\tAverage Q: 1.07\n",
            "Step 150\tAverage score: 26.48\tAverage Q: 1.01\n",
            "Step 160\tAverage score: 26.27\tAverage Q: 1.02\n",
            "Step 170\tAverage score: 26.02\tAverage Q: 1.07\n",
            "Step 180\tAverage score: 25.91\tAverage Q: 1.03\n",
            "Step 190\tAverage score: 25.84\tAverage Q: 1.06\n",
            "Step 200\tAverage score: 25.98\tAverage Q: 0.99\n",
            "Step 210\tAverage score: 26.28\tAverage Q: 1.01\n",
            "Step 220\tAverage score: 26.61\tAverage Q: 1.05\n",
            "Step 230\tAverage score: 26.65\tAverage Q: 1.04\n",
            "Step 240\tAverage score: 26.65\tAverage Q: 0.98\n",
            "Step 250\tAverage score: 26.30\tAverage Q: 1.02\n",
            "Step 260\tAverage score: 26.49\tAverage Q: 1.08\n",
            "Step 270\tAverage score: 26.64\tAverage Q: 1.00\n",
            "Step 280\tAverage score: 26.59\tAverage Q: 1.08\n",
            "Step 290\tAverage score: 26.94\tAverage Q: 1.04\n",
            "Step 300\tAverage score: 27.08\tAverage Q: 1.11\n",
            "Step 310\tAverage score: 27.13\tAverage Q: 1.08\n",
            "Step 320\tAverage score: 26.85\tAverage Q: 1.02\n",
            "Step 330\tAverage score: 26.55\tAverage Q: 1.01\n",
            "Step 340\tAverage score: 26.91\tAverage Q: 1.08\n",
            "Step 350\tAverage score: 27.72\tAverage Q: 1.11\n",
            "Step 360\tAverage score: 27.52\tAverage Q: 1.16\n",
            "Step 370\tAverage score: 27.51\tAverage Q: 1.18\n",
            "Step 380\tAverage score: 27.08\tAverage Q: 1.10\n",
            "Step 390\tAverage score: 26.73\tAverage Q: 1.05\n",
            "Step 400\tAverage score: 26.55\tAverage Q: 1.05\n",
            "Step 410\tAverage score: 26.45\tAverage Q: 1.07\n",
            "Step 420\tAverage score: 26.82\tAverage Q: 1.06\n",
            "Step 430\tAverage score: 26.93\tAverage Q: 1.09\n",
            "Step 440\tAverage score: 27.24\tAverage Q: 1.04\n",
            "Step 450\tAverage score: 27.30\tAverage Q: 1.09\n",
            "Step 460\tAverage score: 27.39\tAverage Q: 1.10\n",
            "Step 470\tAverage score: 27.00\tAverage Q: 1.08\n",
            "Step 480\tAverage score: 26.91\tAverage Q: 1.12\n",
            "Step 490\tAverage score: 26.64\tAverage Q: 1.04\n",
            "Step 500\tAverage score: 26.86\tAverage Q: 1.11\n",
            "Step 510\tAverage score: 26.85\tAverage Q: 1.01\n",
            "Step 520\tAverage score: 26.85\tAverage Q: 1.04\n",
            "Step 530\tAverage score: 26.65\tAverage Q: 1.03\n",
            "Step 540\tAverage score: 27.02\tAverage Q: 1.08\n",
            "Step 550\tAverage score: 27.32\tAverage Q: 1.11\n",
            "Step 560\tAverage score: 27.57\tAverage Q: 1.10\n",
            "Step 570\tAverage score: 26.87\tAverage Q: 1.14\n",
            "Step 580\tAverage score: 27.07\tAverage Q: 1.05\n",
            "Step 590\tAverage score: 27.21\tAverage Q: 1.13\n",
            "Step 600\tAverage score: 26.96\tAverage Q: 1.07\n",
            "Step 610\tAverage score: 27.06\tAverage Q: 1.10\n",
            "Step 620\tAverage score: 26.97\tAverage Q: 1.11\n",
            "Step 630\tAverage score: 26.88\tAverage Q: 1.01\n",
            "Step 640\tAverage score: 26.66\tAverage Q: 1.08\n",
            "Step 650\tAverage score: 26.68\tAverage Q: 1.10\n",
            "Step 660\tAverage score: 26.40\tAverage Q: 1.07\n",
            "Step 670\tAverage score: 26.84\tAverage Q: 0.99\n",
            "Step 680\tAverage score: 26.88\tAverage Q: 1.06\n",
            "Step 690\tAverage score: 26.85\tAverage Q: 1.03\n",
            "Step 700\tAverage score: 26.80\tAverage Q: 1.03\n",
            "Step 710\tAverage score: 26.83\tAverage Q: 1.00\n",
            "Step 720\tAverage score: 26.91\tAverage Q: 0.99\n",
            "Step 730\tAverage score: 27.06\tAverage Q: 1.07\n",
            "Step 740\tAverage score: 26.57\tAverage Q: 1.07\n",
            "Step 750\tAverage score: 26.82\tAverage Q: 1.04\n",
            "Step 760\tAverage score: 27.23\tAverage Q: 1.15\n",
            "Step 770\tAverage score: 26.82\tAverage Q: 1.11\n",
            "Step 780\tAverage score: 26.83\tAverage Q: 1.09\n",
            "Step 790\tAverage score: 26.90\tAverage Q: 1.14\n",
            "Step 800\tAverage score: 26.71\tAverage Q: 1.08\n",
            "Step 810\tAverage score: 26.63\tAverage Q: 1.10\n",
            "Step 820\tAverage score: 26.99\tAverage Q: 1.06\n",
            "Step 830\tAverage score: 26.87\tAverage Q: 1.13\n",
            "Step 840\tAverage score: 27.08\tAverage Q: 1.01\n",
            "Step 850\tAverage score: 26.99\tAverage Q: 1.07\n",
            "Step 860\tAverage score: 27.19\tAverage Q: 1.07\n",
            "Step 870\tAverage score: 27.15\tAverage Q: 1.15\n",
            "Step 880\tAverage score: 26.99\tAverage Q: 1.10\n",
            "Step 890\tAverage score: 26.94\tAverage Q: 1.01\n",
            "Step 900\tAverage score: 26.89\tAverage Q: 1.13\n",
            "Step 910\tAverage score: 26.81\tAverage Q: 1.07\n",
            "Step 920\tAverage score: 26.69\tAverage Q: 1.09\n",
            "Step 930\tAverage score: 26.77\tAverage Q: 1.13\n",
            "Step 940\tAverage score: 26.30\tAverage Q: 1.00\n",
            "Step 950\tAverage score: 26.50\tAverage Q: 1.06\n",
            "Step 960\tAverage score: 26.59\tAverage Q: 1.04\n",
            "Step 970\tAverage score: 26.47\tAverage Q: 1.08\n",
            "Step 980\tAverage score: 26.35\tAverage Q: 1.07\n",
            "Step 990\tAverage score: 26.33\tAverage Q: 1.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ddpg_record = pd.read_csv('/content/drive/MyDrive/강화학습/DDPG_model_record.csv', index_col=0) \n",
        "plt.plot(ddpg_record)\n",
        "plt.title('DDPG')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Moving averaged episode reward')\n",
        "plt.savefig(\"ddpg_seed0.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "rC6wX-2jPVdN",
        "outputId": "245f6b3f-9abd-4106-e0a5-fc38f3da30fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DDPG Metric"
      ],
      "metadata": {
        "id": "sWkTvajZbYbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric_0 = pd.read_csv('/content/drive/MyDrive/강화학습/DDPG_model_record.csv', index_col = 0).rename(columns = {'0':'return'})\n",
        "metric_700 = pd.read_csv('/content/drive/MyDrive/강화학습/DDPG_model_record2_seed700.csv', index_col = 0).rename(columns = {'0':'return'})\n",
        "metric_1000 = pd.read_csv('/content/drive/MyDrive/강화학습/DDPG_model_record3_seed1000.csv', index_col = 0).rename(columns = {'0':'return'})\n",
        "metric_2022 = pd.read_csv('/content/drive/MyDrive/강화학습/DDPG_model_record4_seed2022.csv', index_col = 0).rename(columns = {'0':'return'})"
      ],
      "metadata": {
        "id": "0jtgdUkbUKkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric_0.iloc[-100:].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "IrSWNmUhUfrm",
        "outputId": "32312d33-d90e-4211-9af9-b6e20ccf7766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           return\n",
              "count  100.000000\n",
              "mean    26.543208\n",
              "std      0.188016\n",
              "min     26.160302\n",
              "25%     26.394048\n",
              "50%     26.512036\n",
              "75%     26.697853\n",
              "max     26.943444"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0368002f-f34d-4287-af1c-0154b45b51d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>100.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>26.543208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.188016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>26.160302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>26.394048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>26.512036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>26.697853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>26.943444</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0368002f-f34d-4287-af1c-0154b45b51d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0368002f-f34d-4287-af1c-0154b45b51d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0368002f-f34d-4287-af1c-0154b45b51d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric_700.iloc[-100:].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "RjI1GtCXUfnl",
        "outputId": "054f2a5d-5120-413f-ef3f-0c165f817294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           return\n",
              "count  100.000000\n",
              "mean    34.187719\n",
              "std      3.969014\n",
              "min     19.996534\n",
              "25%     32.737695\n",
              "50%     34.838002\n",
              "75%     36.765918\n",
              "max     41.586996"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1d2ca8d3-eade-40e3-bdc9-00de4364bfe7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>100.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>34.187719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.969014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>19.996534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>32.737695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>34.838002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>36.765918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>41.586996</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d2ca8d3-eade-40e3-bdc9-00de4364bfe7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1d2ca8d3-eade-40e3-bdc9-00de4364bfe7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1d2ca8d3-eade-40e3-bdc9-00de4364bfe7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric_1000.iloc[-100:].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "9nkIs4FcUfks",
        "outputId": "0d428d6b-428b-45c4-e08f-f286c1e27860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           return\n",
              "count  100.000000\n",
              "mean    38.371603\n",
              "std      1.861572\n",
              "min     32.727442\n",
              "25%     37.089846\n",
              "50%     38.382411\n",
              "75%     39.613944\n",
              "max     43.826350"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6a36c17e-d551-4569-a657-eceec2511768\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>100.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>38.371603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.861572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>32.727442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>37.089846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>38.382411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>39.613944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>43.826350</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a36c17e-d551-4569-a657-eceec2511768')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6a36c17e-d551-4569-a657-eceec2511768 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6a36c17e-d551-4569-a657-eceec2511768');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric_2022.iloc[-100:].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "EXUgWlGsVO8s",
        "outputId": "a59c6ff3-a8c1-452c-e1fe-e2b5699291f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           return\n",
              "count  100.000000\n",
              "mean    14.639294\n",
              "std      4.372540\n",
              "min      4.563836\n",
              "25%     11.592961\n",
              "50%     14.729695\n",
              "75%     18.069143\n",
              "max     23.765650"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e58f05bc-54b5-413f-960d-c0d7fee5667a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>100.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>14.639294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.372540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>4.563836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>11.592961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>14.729695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>18.069143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>23.765650</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e58f05bc-54b5-413f-960d-c0d7fee5667a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e58f05bc-54b5-413f-960d-c0d7fee5667a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e58f05bc-54b5-413f-960d-c0d7fee5667a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}